{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Token-Level Explanations: SHAP and LIME Analysis\n",
        "\n",
        "**Project:** HEARTS Adaptation - Gender Bias Detection  \n",
        "**Section:** 3.3 Token Level Explanations (adapted from original HEARTS paper)\n",
        "\n",
        "This notebook implements:\n",
        "1. **SHAP Analysis** - Generate token-level importance values using SHAP\n",
        "2. **LIME Analysis** - Generate token-level importance values using LIME\n",
        "3. **Similarity Metrics** - Calculate Cosine Similarity, Pearson Correlation, and Jensen-Shannon Divergence\n",
        "4. **Explanation Confidence Scores** - Compare SHAP and LIME outputs to assess explanation confidence\n",
        "5. **Visualizations** - Display token importance and explanation confidence\n",
        "\n",
        "**Mathematical Framework:**\n",
        "- SHAP vector: $\\phi_i = (\\phi_{i1}, \\phi_{i2}, ..., \\phi_{iN})$ for each text instance $i$\n",
        "- LIME vector: $\\beta_i = (\\beta_{i1}, \\beta_{i2}, ..., \\beta_{iN})$ for each text instance $i$\n",
        "- Similarity metrics: Cosine Similarity, Pearson Correlation, Jensen-Shannon Divergence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
        "import shap\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up paths\n",
        "current_dir = Path.cwd()\n",
        "if current_dir.name == 'notebooks':\n",
        "    project_root = current_dir.parent\n",
        "else:\n",
        "    project_root = current_dir\n",
        "\n",
        "data_dir = project_root / 'data'\n",
        "models_dir = project_root / 'models'\n",
        "results_dir = project_root / 'results'\n",
        "explainability_dir = project_root / 'explainability'\n",
        "\n",
        "# Create explainability directory\n",
        "os.makedirs(explainability_dir, exist_ok=True)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TOKEN-LEVEL EXPLANATIONS: SHAP AND LIME ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nProject root: {project_root}\")\n",
        "print(f\"Data directory: {data_dir}\")\n",
        "print(f\"Models directory: {models_dir}\")\n",
        "print(f\"Results directory: {results_dir}\")\n",
        "print(f\"Explainability directory: {explainability_dir}\")\n",
        "\n",
        "# Convert Path objects to strings for compatibility\n",
        "data_dir = str(data_dir)\n",
        "models_dir = str(models_dir)\n",
        "results_dir = str(results_dir)\n",
        "explainability_dir = str(explainability_dir)\n",
        "\n",
        "# Check device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Test Data and Model\n",
        "\n",
        "Load the test data and a trained model for explanation analysis. We'll use ALBERT-V2 as the primary model (best performer).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test data (from splits directory created by preprocessing notebook)\n",
        "test_data_path = os.path.join(data_dir, 'splits', 'test.csv')\n",
        "\n",
        "if os.path.exists(test_data_path):\n",
        "    test_data = pd.read_csv(test_data_path)\n",
        "    print(f\"✅ Loaded test data: {len(test_data):,} samples\")\n",
        "    print(f\"   Label distribution:\")\n",
        "    print(test_data['label'].value_counts().sort_index())\n",
        "else:\n",
        "    print(f\"⚠️  Test data not found at: {test_data_path}\")\n",
        "    print(\"   Please run 01_Data_Loading_Preprocessing.ipynb first\")\n",
        "    test_data = None\n",
        "\n",
        "# Load model (default: ALBERT-V2)\n",
        "model_name = 'albert_albert-base-v2'  # Can be changed to 'distilbert_distilbert-base-uncased' or 'google-bert_bert-base-uncased'\n",
        "model_dir = os.path.join(models_dir, 'job_descriptions', model_name)\n",
        "\n",
        "if os.path.exists(model_dir):\n",
        "    print(f\"\\n✅ Model found at: {model_dir}\")\n",
        "    print(f\"   Loading model: {model_name}\")\n",
        "    \n",
        "    # Load model and tokenizer\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "    \n",
        "    # Create pipeline\n",
        "    device_id = 0 if torch.cuda.is_available() else -1\n",
        "    pipe = pipeline(\n",
        "        \"text-classification\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=device_id,\n",
        "        return_all_scores=True\n",
        "    )\n",
        "    print(f\"✅ Model loaded successfully!\")\n",
        "else:\n",
        "    print(f\"\\n⚠️  Model not found at: {model_dir}\")\n",
        "    print(\"   Please train a model first using 02_Model_Training.ipynb\")\n",
        "    pipe = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Test Instances\n",
        "\n",
        "Sample a subset of test instances for explanation analysis. We'll sample both correct and incorrect predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_instances_for_explanation(test_data, model_pipeline, n_samples=50, seed=42):\n",
        "    \"\"\"\n",
        "    Sample test instances for explanation analysis\n",
        "    Includes both correct and incorrect predictions\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    test_data : pd.DataFrame\n",
        "        Test dataset with 'text' and 'label' columns\n",
        "    model_pipeline : pipeline\n",
        "        HuggingFace text classification pipeline\n",
        "    n_samples : int\n",
        "        Number of samples per category (correct/incorrect)\n",
        "    seed : int\n",
        "        Random seed\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    sampled_data : pd.DataFrame\n",
        "        Sampled instances with predictions\n",
        "    \"\"\"\n",
        "    if model_pipeline is None or test_data is None:\n",
        "        print(\"⚠️  Cannot sample - model or test data not loaded\")\n",
        "        return None\n",
        "    \n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # Get predictions\n",
        "    print(\"Getting predictions for all test instances...\")\n",
        "    predictions = model_pipeline(test_data['text'].to_list(), return_all_scores=True)\n",
        "    \n",
        "    # Extract predicted labels and probabilities\n",
        "    pred_labels = []\n",
        "    pred_probs = []\n",
        "    for pred in predictions:\n",
        "        best_pred = max(pred, key=lambda x: x['score'])\n",
        "        label_str = best_pred['label']\n",
        "        if 'LABEL_' in label_str:\n",
        "            label_num = int(label_str.split('_')[-1])\n",
        "        else:\n",
        "            label_num = int(label_str) if label_str.isdigit() else 0\n",
        "        pred_labels.append(label_num)\n",
        "        pred_probs.append(best_pred['score'])\n",
        "    \n",
        "    # Add predictions to dataframe\n",
        "    test_data = test_data.copy()\n",
        "    test_data['predicted_label'] = pred_labels\n",
        "    test_data['predicted_probability'] = pred_probs\n",
        "    test_data['is_correct'] = test_data['predicted_label'] == test_data['label']\n",
        "    \n",
        "    # Sample correct and incorrect predictions\n",
        "    correct_predictions = test_data[test_data['is_correct'] == True]\n",
        "    incorrect_predictions = test_data[test_data['is_correct'] == False]\n",
        "    \n",
        "    print(f\"\\nCorrect predictions: {len(correct_predictions):,}\")\n",
        "    print(f\"Incorrect predictions: {len(incorrect_predictions):,}\")\n",
        "    \n",
        "    # Sample\n",
        "    n_correct = min(n_samples, len(correct_predictions))\n",
        "    n_incorrect = min(n_samples, len(incorrect_predictions))\n",
        "    \n",
        "    sampled_correct = correct_predictions.sample(n=n_correct, random_state=seed) if n_correct > 0 else pd.DataFrame()\n",
        "    sampled_incorrect = incorrect_predictions.sample(n=n_incorrect, random_state=seed) if n_incorrect > 0 else pd.DataFrame()\n",
        "    \n",
        "    sampled_data = pd.concat([sampled_correct, sampled_incorrect], ignore_index=True)\n",
        "    sampled_data = sampled_data.sample(frac=1, random_state=seed).reset_index(drop=True)  # Shuffle\n",
        "    \n",
        "    print(f\"\\n✅ Sampled {len(sampled_data)} instances for explanation analysis\")\n",
        "    print(f\"   Correct predictions: {len(sampled_correct)}\")\n",
        "    print(f\"   Incorrect predictions: {len(sampled_incorrect)}\")\n",
        "    \n",
        "    return sampled_data\n",
        "\n",
        "# Sample instances (adjust n_samples based on computational resources)\n",
        "if test_data is not None and pipe is not None:\n",
        "    sampled_data = sample_instances_for_explanation(test_data, pipe, n_samples=50, seed=42)\n",
        "    \n",
        "    # Save sampled data\n",
        "    sampled_path = os.path.join(explainability_dir, 'sampled_instances.csv')\n",
        "    sampled_data.to_csv(sampled_path, index=False)\n",
        "    print(f\"\\n✅ Sampled data saved to: {sampled_path}\")\n",
        "else:\n",
        "    sampled_data = None\n",
        "    print(\"\\n⚠️  Cannot sample instances - missing model or test data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SHAP Analysis\n",
        "\n",
        "Generate SHAP values for token-level importance. SHAP (SHapley Additive exPlanations) calculates the contribution of each token to the model's prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_shap_values(sampled_data, model_pipeline, class_index=1):\n",
        "    \"\"\"\n",
        "    Compute SHAP values for token-level importance\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    sampled_data : pd.DataFrame\n",
        "        Sampled instances for explanation\n",
        "    model_pipeline : pipeline\n",
        "        HuggingFace text classification pipeline\n",
        "    class_index : int\n",
        "        Class index to explain (0 = Non-Biased, 1 = Biased)\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    shap_results : pd.DataFrame\n",
        "        SHAP values for each token in each instance\n",
        "    \"\"\"\n",
        "    if model_pipeline is None or sampled_data is None:\n",
        "        print(\"⚠️  Cannot compute SHAP - model or data not loaded\")\n",
        "        return None\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "    print(\"COMPUTING SHAP VALUES\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nAnalyzing {len(sampled_data)} instances...\")\n",
        "    print(f\"Class index: {class_index} ({'Biased' if class_index == 1 else 'Non-Biased'})\")\n",
        "    \n",
        "    # Create SHAP masker and explainer\n",
        "    # Using regex tokenizer to match word boundaries\n",
        "    masker = shap.maskers.Text(tokenizer=r'\\b\\w+\\b')\n",
        "    explainer = shap.Explainer(model_pipeline, masker)\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for idx, row in sampled_data.iterrows():\n",
        "        text_input = row['text']\n",
        "        \n",
        "        try:\n",
        "            # Compute SHAP values\n",
        "            shap_values = explainer([text_input])\n",
        "            \n",
        "            # Extract SHAP values for the specified class\n",
        "            # shap_values structure: [instance][token][class]\n",
        "            if hasattr(shap_values, 'values'):\n",
        "                values = shap_values.values[0, :, class_index]  # Get values for class_index\n",
        "            else:\n",
        "                values = shap_values[:, :, class_index].values[0]\n",
        "            \n",
        "            # Tokenize text (matching SHAP tokenization)\n",
        "            tokens = re.findall(r'\\b\\w+\\b', text_input)\n",
        "            \n",
        "            # Ensure we have the same number of tokens and values\n",
        "            min_len = min(len(tokens), len(values))\n",
        "            tokens = tokens[:min_len]\n",
        "            values = values[:min_len]\n",
        "            \n",
        "            # Store results\n",
        "            for token, value in zip(tokens, values):\n",
        "                results.append({\n",
        "                    'sentence_id': idx,\n",
        "                    'token': token,\n",
        "                    'value_shap': float(value),\n",
        "                    'sentence': text_input,\n",
        "                    'predicted_label': row['predicted_label'],\n",
        "                    'actual_label': row['label'],\n",
        "                    'is_correct': row['is_correct'],\n",
        "                    'predicted_probability': row['predicted_probability']\n",
        "                })\n",
        "            \n",
        "            if (idx + 1) % 10 == 0:\n",
        "                print(f\"  Processed {idx + 1}/{len(sampled_data)} instances...\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠️  Error processing instance {idx}: {str(e)[:100]}\")\n",
        "            continue\n",
        "    \n",
        "    shap_results = pd.DataFrame(results)\n",
        "    print(f\"\\n✅ SHAP analysis complete!\")\n",
        "    print(f\"   Total token explanations: {len(shap_results):,}\")\n",
        "    \n",
        "    return shap_results\n",
        "\n",
        "# Compute SHAP values\n",
        "if sampled_data is not None and pipe is not None:\n",
        "    print(\"\\n⚠️  Note: SHAP computation can be slow. This may take several minutes...\")\n",
        "    shap_results = compute_shap_values(sampled_data, pipe, class_index=1)\n",
        "    \n",
        "    if shap_results is not None:\n",
        "        # Save SHAP results\n",
        "        shap_path = os.path.join(explainability_dir, 'shap_results.csv')\n",
        "        shap_results.to_csv(shap_path, index=False)\n",
        "        print(f\"\\n✅ SHAP results saved to: {shap_path}\")\n",
        "else:\n",
        "    shap_results = None\n",
        "    print(\"\\n⚠️  Cannot compute SHAP - missing model or sampled data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LIME Analysis\n",
        "\n",
        "Generate LIME values for token-level importance. LIME (Local Interpretable Model-agnostic Explanations) creates local explanations by perturbing the input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_tokenizer(text):\n",
        "    \"\"\"\n",
        "    Custom tokenizer for LIME (matches SHAP tokenization)\n",
        "    Uses regex to split on word boundaries\n",
        "    \"\"\"\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "    return tokens\n",
        "\n",
        "def compute_lime_values(sampled_data, model_pipeline, class_index=1, num_samples=100):\n",
        "    \"\"\"\n",
        "    Compute LIME values for token-level importance\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    sampled_data : pd.DataFrame\n",
        "        Sampled instances for explanation\n",
        "    model_pipeline : pipeline\n",
        "        HuggingFace text classification pipeline\n",
        "    class_index : int\n",
        "        Class index to explain (0 = Non-Biased, 1 = Biased)\n",
        "    num_samples : int\n",
        "        Number of samples for LIME perturbation\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    lime_results : pd.DataFrame\n",
        "        LIME values for each token in each instance\n",
        "    \"\"\"\n",
        "    if model_pipeline is None or sampled_data is None:\n",
        "        print(\"⚠️  Cannot compute LIME - model or data not loaded\")\n",
        "        return None\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "    print(\"COMPUTING LIME VALUES\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nAnalyzing {len(sampled_data)} instances...\")\n",
        "    print(f\"Class index: {class_index} ({'Biased' if class_index == 1 else 'Non-Biased'})\")\n",
        "    print(f\"LIME samples per instance: {num_samples}\")\n",
        "    \n",
        "    # Define prediction function for LIME\n",
        "    def predict_proba(texts):\n",
        "        \"\"\"Predict probabilities for LIME\"\"\"\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "        preds = model_pipeline(texts, return_all_scores=True)\n",
        "        # Return probabilities for both classes\n",
        "        probabilities = np.array([[pred['score'] for pred in preds_single] for preds_single in preds])\n",
        "        return probabilities\n",
        "    \n",
        "    # Create LIME explainer\n",
        "    explainer = LimeTextExplainer(\n",
        "        class_names=['Non-Biased', 'Biased'],\n",
        "        split_expression=lambda x: custom_tokenizer(x)\n",
        "    )\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for idx, row in sampled_data.iterrows():\n",
        "        text_input = row['text']\n",
        "        tokens = custom_tokenizer(text_input)\n",
        "        \n",
        "        try:\n",
        "            # Generate LIME explanation\n",
        "            exp = explainer.explain_instance(\n",
        "                text_input,\n",
        "                predict_proba,\n",
        "                num_features=len(tokens),\n",
        "                num_samples=num_samples,\n",
        "                labels=[class_index]\n",
        "            )\n",
        "            \n",
        "            # Get explanation as list of (token, value) pairs\n",
        "            explanation_list = exp.as_list(label=class_index)\n",
        "            \n",
        "            # Create dictionary for easy lookup\n",
        "            token_value_dict = {token: value for token, value in explanation_list}\n",
        "            \n",
        "            # Store results (including tokens with zero importance)\n",
        "            for token in tokens:\n",
        "                value = token_value_dict.get(token, 0.0)\n",
        "                results.append({\n",
        "                    'sentence_id': idx,\n",
        "                    'token': token,\n",
        "                    'value_lime': float(value),\n",
        "                    'sentence': text_input,\n",
        "                    'predicted_label': row['predicted_label'],\n",
        "                    'actual_label': row['label'],\n",
        "                    'is_correct': row['is_correct'],\n",
        "                    'predicted_probability': row['predicted_probability']\n",
        "                })\n",
        "            \n",
        "            if (idx + 1) % 10 == 0:\n",
        "                print(f\"  Processed {idx + 1}/{len(sampled_data)} instances...\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠️  Error processing instance {idx}: {str(e)[:100]}\")\n",
        "            continue\n",
        "    \n",
        "    lime_results = pd.DataFrame(results)\n",
        "    print(f\"\\n✅ LIME analysis complete!\")\n",
        "    print(f\"   Total token explanations: {len(lime_results):,}\")\n",
        "    \n",
        "    return lime_results\n",
        "\n",
        "# Compute LIME values\n",
        "if sampled_data is not None and pipe is not None:\n",
        "    print(\"\\n⚠️  Note: LIME computation can be slow. This may take several minutes...\")\n",
        "    lime_results = compute_lime_values(sampled_data, pipe, class_index=1, num_samples=100)\n",
        "    \n",
        "    if lime_results is not None:\n",
        "        # Save LIME results\n",
        "        lime_path = os.path.join(explainability_dir, 'lime_results.csv')\n",
        "        lime_results.to_csv(lime_path, index=False)\n",
        "        print(f\"\\n✅ LIME results saved to: {lime_path}\")\n",
        "else:\n",
        "    lime_results = None\n",
        "    print(\"\\n⚠️  Cannot compute LIME - missing model or sampled data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Similarity Metrics\n",
        "\n",
        "Calculate similarity metrics between SHAP and LIME vectors to assess explanation confidence:\n",
        "1. **Cosine Similarity**: Measures the angle between vectors\n",
        "2. **Pearson Correlation**: Measures linear correlation\n",
        "3. **Jensen-Shannon Divergence**: Measures distributional similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_cosine_similarity(vector1, vector2):\n",
        "    \"\"\"\n",
        "    Compute cosine similarity between two vectors\n",
        "    \n",
        "    CS(φᵢ, βᵢ) = (φᵢ · βᵢ) / (||φᵢ|| ||βᵢ||)\n",
        "    \"\"\"\n",
        "    vector1 = np.array(vector1).flatten()\n",
        "    vector2 = np.array(vector2).flatten()\n",
        "    \n",
        "    # Handle zero vectors\n",
        "    if np.linalg.norm(vector1) == 0 or np.linalg.norm(vector2) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    dot_product = np.dot(vector1, vector2)\n",
        "    norm1 = np.linalg.norm(vector1)\n",
        "    norm2 = np.linalg.norm(vector2)\n",
        "    \n",
        "    return dot_product / (norm1 * norm2)\n",
        "\n",
        "def compute_pearson_correlation(vector1, vector2):\n",
        "    \"\"\"\n",
        "    Compute Pearson correlation coefficient\n",
        "    \n",
        "    PC(φᵢ, βᵢ) = Cov(φᵢ, βᵢ) / (σ_φᵢ σ_βᵢ)\n",
        "    \"\"\"\n",
        "    vector1 = np.array(vector1).flatten()\n",
        "    vector2 = np.array(vector2).flatten()\n",
        "    \n",
        "    if len(vector1) != len(vector2) or len(vector1) < 2:\n",
        "        return 0.0\n",
        "    \n",
        "    correlation, _ = pearsonr(vector1, vector2)\n",
        "    return correlation if not np.isnan(correlation) else 0.0\n",
        "\n",
        "def compute_js_divergence(vector1, vector2):\n",
        "    \"\"\"\n",
        "    Compute Jensen-Shannon Divergence\n",
        "    \n",
        "    First converts vectors to probability distributions, then computes JSD\n",
        "    \"\"\"\n",
        "    vector1 = np.array(vector1).flatten()\n",
        "    vector2 = np.array(vector2).flatten()\n",
        "    \n",
        "    # Convert to probability distributions\n",
        "    # Add |Min| to make all values non-negative\n",
        "    min1 = np.min(vector1)\n",
        "    min2 = np.min(vector2)\n",
        "    \n",
        "    if min1 < 0:\n",
        "        vector1 = vector1 + abs(min1)\n",
        "    if min2 < 0:\n",
        "        vector2 = vector2 + abs(min2)\n",
        "    \n",
        "    # Normalize to probabilities\n",
        "    sum1 = np.sum(vector1)\n",
        "    sum2 = np.sum(vector2)\n",
        "    \n",
        "    if sum1 > 0:\n",
        "        prob1 = vector1 / sum1\n",
        "    else:\n",
        "        prob1 = np.ones_like(vector1) / len(vector1)\n",
        "    \n",
        "    if sum2 > 0:\n",
        "        prob2 = vector2 / sum2\n",
        "    else:\n",
        "        prob2 = np.ones_like(vector2) / len(vector2)\n",
        "    \n",
        "    # Compute Jensen-Shannon Divergence\n",
        "    # JSD(P||Q) = 0.5 * KL(P||M) + 0.5 * KL(Q||M), where M = 0.5 * (P + Q)\n",
        "    M = 0.5 * (prob1 + prob2)\n",
        "    \n",
        "    # Avoid log(0)\n",
        "    epsilon = 1e-10\n",
        "    prob1 = prob1 + epsilon\n",
        "    prob2 = prob2 + epsilon\n",
        "    M = M + epsilon\n",
        "    \n",
        "    # Normalize again after adding epsilon\n",
        "    prob1 = prob1 / np.sum(prob1)\n",
        "    prob2 = prob2 / np.sum(prob2)\n",
        "    M = M / np.sum(M)\n",
        "    \n",
        "    kl_pm = np.sum(prob1 * np.log(prob1 / M))\n",
        "    kl_qm = np.sum(prob2 * np.log(prob2 / M))\n",
        "    \n",
        "    jsd = 0.5 * kl_pm + 0.5 * kl_qm\n",
        "    \n",
        "    return np.sqrt(jsd)  # Return square root as in the paper\n",
        "\n",
        "# Compute similarity metrics\n",
        "if shap_results is not None and lime_results is not None:\n",
        "    print(\"=\" * 70)\n",
        "    print(\"COMPUTING SIMILARITY METRICS\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Merge SHAP and LIME results\n",
        "    merge_cols = ['sentence_id', 'token', 'sentence', 'predicted_label', 'actual_label', 'is_correct']\n",
        "    merged_df = pd.merge(\n",
        "        shap_results[merge_cols + ['value_shap']],\n",
        "        lime_results[merge_cols + ['value_lime']],\n",
        "        on=merge_cols,\n",
        "        how='inner',\n",
        "        suffixes=('_shap', '_lime')\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nMerged {len(merged_df):,} token-level explanations\")\n",
        "    \n",
        "    # Compute similarity metrics per sentence\n",
        "    sentence_similarities = []\n",
        "    \n",
        "    for sentence_id in merged_df['sentence_id'].unique():\n",
        "        sentence_data = merged_df[merged_df['sentence_id'] == sentence_id]\n",
        "        \n",
        "        shap_vector = sentence_data['value_shap'].values\n",
        "        lime_vector = sentence_data['value_lime'].values\n",
        "        \n",
        "        # Compute metrics\n",
        "        cosine_sim = compute_cosine_similarity(shap_vector, lime_vector)\n",
        "        pearson_corr = compute_pearson_correlation(shap_vector, lime_vector)\n",
        "        js_div = compute_js_divergence(shap_vector, lime_vector)\n",
        "        \n",
        "        # Get sentence metadata\n",
        "        row = sentence_data.iloc[0]\n",
        "        \n",
        "        sentence_similarities.append({\n",
        "            'sentence_id': sentence_id,\n",
        "            'sentence': row['sentence'],\n",
        "            'predicted_label': row['predicted_label'],\n",
        "            'actual_label': row['actual_label'],\n",
        "            'is_correct': row['is_correct'],\n",
        "            'cosine_similarity': cosine_sim,\n",
        "            'pearson_correlation': pearson_corr,\n",
        "            'jensen_shannon_divergence': js_div,\n",
        "            'num_tokens': len(sentence_data)\n",
        "        })\n",
        "    \n",
        "    similarity_df = pd.DataFrame(sentence_similarities)\n",
        "    \n",
        "    print(f\"\\n✅ Computed similarity metrics for {len(similarity_df)} sentences\")\n",
        "    print(f\"\\nSimilarity Statistics:\")\n",
        "    print(f\"  Cosine Similarity: {similarity_df['cosine_similarity'].mean():.4f} ± {similarity_df['cosine_similarity'].std():.4f}\")\n",
        "    print(f\"  Pearson Correlation: {similarity_df['pearson_correlation'].mean():.4f} ± {similarity_df['pearson_correlation'].std():.4f}\")\n",
        "    print(f\"  Jensen-Shannon Divergence: {similarity_df['jensen_shannon_divergence'].mean():.4f} ± {similarity_df['jensen_shannon_divergence'].std():.4f}\")\n",
        "    \n",
        "    # Save results\n",
        "    similarity_path = os.path.join(explainability_dir, 'sentence_similarity_metrics.csv')\n",
        "    similarity_df.to_csv(similarity_path, index=False)\n",
        "    print(f\"\\n✅ Similarity metrics saved to: {similarity_path}\")\n",
        "    \n",
        "else:\n",
        "    similarity_df = None\n",
        "    print(\"\\n⚠️  Cannot compute similarity metrics - missing SHAP or LIME results\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization: Token Importance and Explanation Confidence\n",
        "\n",
        "Visualize token-level importance and explanation confidence scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_token_importance(shap_results, lime_results, sentence_id, top_n=10):\n",
        "    \"\"\"\n",
        "    Visualize token importance for a specific sentence\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    shap_results : pd.DataFrame\n",
        "        SHAP results\n",
        "    lime_results : pd.DataFrame\n",
        "        LIME results\n",
        "    sentence_id : int\n",
        "        Sentence ID to visualize\n",
        "    top_n : int\n",
        "        Number of top tokens to display\n",
        "    \"\"\"\n",
        "    if shap_results is None or lime_results is None:\n",
        "        print(\"⚠️  Cannot visualize - missing SHAP or LIME results\")\n",
        "        return\n",
        "    \n",
        "    # Get data for this sentence\n",
        "    shap_sent = shap_results[shap_results['sentence_id'] == sentence_id].copy()\n",
        "    lime_sent = lime_results[lime_results['sentence_id'] == sentence_id].copy()\n",
        "    \n",
        "    if len(shap_sent) == 0 or len(lime_sent) == 0:\n",
        "        print(f\"⚠️  No data found for sentence_id {sentence_id}\")\n",
        "        return\n",
        "    \n",
        "    # Merge\n",
        "    merged = pd.merge(shap_sent[['token', 'value_shap']], \n",
        "                     lime_sent[['token', 'value_lime']], \n",
        "                     on='token', how='inner')\n",
        "    \n",
        "    # Get top tokens by absolute SHAP value\n",
        "    merged['abs_shap'] = merged['value_shap'].abs()\n",
        "    top_tokens = merged.nlargest(top_n, 'abs_shap')\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # Plot 1: SHAP values\n",
        "    colors_shap = ['red' if x < 0 else 'blue' for x in top_tokens['value_shap']]\n",
        "    ax1.barh(range(len(top_tokens)), top_tokens['value_shap'], color=colors_shap, alpha=0.7)\n",
        "    ax1.set_yticks(range(len(top_tokens)))\n",
        "    ax1.set_yticklabels(top_tokens['token'], fontsize=10)\n",
        "    ax1.set_xlabel('SHAP Value', fontsize=12)\n",
        "    ax1.set_title(f'Top {top_n} Tokens by SHAP Importance\\n(Sentence ID: {sentence_id})', fontsize=14)\n",
        "    ax1.axvline(x=0, color='black', linestyle='--', linewidth=0.5)\n",
        "    ax1.grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    # Plot 2: LIME values\n",
        "    colors_lime = ['red' if x < 0 else 'blue' for x in top_tokens['value_lime']]\n",
        "    ax2.barh(range(len(top_tokens)), top_tokens['value_lime'], color=colors_lime, alpha=0.7)\n",
        "    ax2.set_yticks(range(len(top_tokens)))\n",
        "    ax2.set_yticklabels(top_tokens['token'], fontsize=10)\n",
        "    ax2.set_xlabel('LIME Value', fontsize=12)\n",
        "    ax2.set_title(f'Top {top_n} Tokens by LIME Importance\\n(Sentence ID: {sentence_id})', fontsize=14)\n",
        "    ax2.axvline(x=0, color='black', linestyle='--', linewidth=0.5)\n",
        "    ax2.grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save figure\n",
        "    fig_path = os.path.join(explainability_dir, f'token_importance_sentence_{sentence_id}.png')\n",
        "    plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"✅ Saved visualization to: {fig_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    # Print sentence and metrics\n",
        "    sentence_text = shap_sent.iloc[0]['sentence']\n",
        "    print(f\"\\nSentence: {sentence_text[:200]}...\")\n",
        "    print(f\"Predicted: {shap_sent.iloc[0]['predicted_label']}, Actual: {shap_sent.iloc[0]['actual_label']}\")\n",
        "    print(f\"\\nTop {top_n} Important Tokens:\")\n",
        "    print(top_tokens[['token', 'value_shap', 'value_lime']].to_string(index=False))\n",
        "\n",
        "# Visualize a few example sentences\n",
        "if shap_results is not None and lime_results is not None and similarity_df is not None:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"VISUALIZING TOKEN IMPORTANCE\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Visualize a few examples (correct and incorrect predictions)\n",
        "    if len(similarity_df) > 0:\n",
        "        # Example 1: High confidence correct prediction\n",
        "        correct_high_conf = similarity_df[\n",
        "            (similarity_df['is_correct'] == True) & \n",
        "            (similarity_df['cosine_similarity'] > similarity_df['cosine_similarity'].median())\n",
        "        ]\n",
        "        if len(correct_high_conf) > 0:\n",
        "            example_id = correct_high_conf.iloc[0]['sentence_id']\n",
        "            print(f\"\\nExample 1: High-confidence correct prediction (Sentence ID: {example_id})\")\n",
        "            visualize_token_importance(shap_results, lime_results, example_id, top_n=10)\n",
        "        \n",
        "        # Example 2: Incorrect prediction\n",
        "        incorrect = similarity_df[similarity_df['is_correct'] == False]\n",
        "        if len(incorrect) > 0:\n",
        "            example_id = incorrect.iloc[0]['sentence_id']\n",
        "            print(f\"\\nExample 2: Incorrect prediction (Sentence ID: {example_id})\")\n",
        "            visualize_token_importance(shap_results, lime_results, example_id, top_n=10)\n",
        "else:\n",
        "    print(\"\\n⚠️  Cannot visualize - missing results\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explanation Confidence Analysis\n",
        "\n",
        "Analyze the relationship between explanation confidence (similarity between SHAP and LIME) and prediction correctness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_explanation_confidence(similarity_df):\n",
        "    \"\"\"\n",
        "    Analyze explanation confidence scores\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    similarity_df : pd.DataFrame\n",
        "        DataFrame with similarity metrics\n",
        "    \"\"\"\n",
        "    if similarity_df is None or len(similarity_df) == 0:\n",
        "        print(\"⚠️  Cannot analyze - missing similarity data\")\n",
        "        return\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "    print(\"EXPLANATION CONFIDENCE ANALYSIS\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Compare correct vs incorrect predictions\n",
        "    correct = similarity_df[similarity_df['is_correct'] == True]\n",
        "    incorrect = similarity_df[similarity_df['is_correct'] == False]\n",
        "    \n",
        "    print(f\"\\nCorrect Predictions: {len(correct)}\")\n",
        "    print(f\"  Cosine Similarity: {correct['cosine_similarity'].mean():.4f} ± {correct['cosine_similarity'].std():.4f}\")\n",
        "    print(f\"  Pearson Correlation: {correct['pearson_correlation'].mean():.4f} ± {correct['pearson_correlation'].std():.4f}\")\n",
        "    print(f\"  Jensen-Shannon Divergence: {correct['jensen_shannon_divergence'].mean():.4f} ± {correct['jensen_shannon_divergence'].std():.4f}\")\n",
        "    \n",
        "    print(f\"\\nIncorrect Predictions: {len(incorrect)}\")\n",
        "    print(f\"  Cosine Similarity: {incorrect['cosine_similarity'].mean():.4f} ± {incorrect['cosine_similarity'].std():.4f}\")\n",
        "    print(f\"  Pearson Correlation: {incorrect['pearson_correlation'].mean():.4f} ± {incorrect['pearson_correlation'].std():.4f}\")\n",
        "    print(f\"  Jensen-Shannon Divergence: {incorrect['jensen_shannon_divergence'].mean():.4f} ± {incorrect['jensen_shannon_divergence'].std():.4f}\")\n",
        "    \n",
        "    # Create visualizations\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    \n",
        "    metrics = ['cosine_similarity', 'pearson_correlation', 'jensen_shannon_divergence']\n",
        "    metric_names = ['Cosine Similarity', 'Pearson Correlation', 'Jensen-Shannon Divergence']\n",
        "    \n",
        "    for ax, metric, name in zip(axes, metrics, metric_names):\n",
        "        correct_vals = correct[metric].values\n",
        "        incorrect_vals = incorrect[metric].values\n",
        "        \n",
        "        ax.hist(correct_vals, alpha=0.6, label='Correct', bins=20, color='green')\n",
        "        ax.hist(incorrect_vals, alpha=0.6, label='Incorrect', bins=20, color='red')\n",
        "        ax.set_xlabel(name, fontsize=11)\n",
        "        ax.set_ylabel('Frequency', fontsize=11)\n",
        "        ax.set_title(f'{name} Distribution', fontsize=12)\n",
        "        ax.legend()\n",
        "        ax.grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save figure\n",
        "    fig_path = os.path.join(explainability_dir, 'explanation_confidence_analysis.png')\n",
        "    plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\n✅ Saved confidence analysis to: {fig_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    # Summary statistics\n",
        "    summary_stats = similarity_df.groupby('is_correct').agg({\n",
        "        'cosine_similarity': ['mean', 'std'],\n",
        "        'pearson_correlation': ['mean', 'std'],\n",
        "        'jensen_shannon_divergence': ['mean', 'std']\n",
        "    }).round(4)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"SUMMARY STATISTICS\")\n",
        "    print(\"=\" * 70)\n",
        "    print(summary_stats)\n",
        "    \n",
        "    # Save summary\n",
        "    summary_path = os.path.join(explainability_dir, 'explanation_confidence_summary.csv')\n",
        "    summary_stats.to_csv(summary_path)\n",
        "    print(f\"\\n✅ Summary statistics saved to: {summary_path}\")\n",
        "\n",
        "# Analyze explanation confidence\n",
        "if similarity_df is not None:\n",
        "    analyze_explanation_confidence(similarity_df)\n",
        "else:\n",
        "    print(\"\\n⚠️  Cannot analyze confidence - missing similarity data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Results\n",
        "\n",
        "Generate a summary report of the token-level explanation analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary report\n",
        "if similarity_df is not None and shap_results is not None and lime_results is not None:\n",
        "    print(\"=\" * 70)\n",
        "    print(\"TOKEN-LEVEL EXPLANATION ANALYSIS SUMMARY\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    print(f\"\\nDataset: Gender Bias Detection in Job Descriptions\")\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Total instances analyzed: {len(similarity_df):,}\")\n",
        "    print(f\"Total token explanations: {len(shap_results):,}\")\n",
        "    \n",
        "    print(f\"\\nOverall Similarity Metrics:\")\n",
        "    print(f\"  Cosine Similarity: {similarity_df['cosine_similarity'].mean():.4f} ± {similarity_df['cosine_similarity'].std():.4f}\")\n",
        "    print(f\"  Pearson Correlation: {similarity_df['pearson_correlation'].mean():.4f} ± {similarity_df['pearson_correlation'].std():.4f}\")\n",
        "    print(f\"  Jensen-Shannon Divergence: {similarity_df['jensen_shannon_divergence'].mean():.4f} ± {similarity_df['jensen_shannon_divergence'].std():.4f}\")\n",
        "    \n",
        "    # Prediction accuracy\n",
        "    accuracy = similarity_df['is_correct'].mean() * 100\n",
        "    print(f\"\\nPrediction Accuracy: {accuracy:.2f}%\")\n",
        "    \n",
        "    # Explanation confidence by prediction correctness\n",
        "    correct_conf = similarity_df[similarity_df['is_correct'] == True]['cosine_similarity'].mean()\n",
        "    incorrect_conf = similarity_df[similarity_df['is_correct'] == False]['cosine_similarity'].mean()\n",
        "    \n",
        "    print(f\"\\nExplanation Confidence (Cosine Similarity):\")\n",
        "    print(f\"  Correct predictions: {correct_conf:.4f}\")\n",
        "    print(f\"  Incorrect predictions: {incorrect_conf:.4f}\")\n",
        "    print(f\"  Difference: {correct_conf - incorrect_conf:.4f}\")\n",
        "    \n",
        "    print(f\"\\n✅ All results saved to: {explainability_dir}\")\n",
        "    print(f\"   - SHAP results: shap_results.csv\")\n",
        "    print(f\"   - LIME results: lime_results.csv\")\n",
        "    print(f\"   - Similarity metrics: sentence_similarity_metrics.csv\")\n",
        "    print(f\"   - Visualizations: token_importance_sentence_*.png\")\n",
        "    print(f\"   - Confidence analysis: explanation_confidence_analysis.png\")\n",
        "    \n",
        "else:\n",
        "    print(\"\\n⚠️  Cannot generate summary - missing analysis results\")\n",
        "    print(\"   Please run the SHAP and LIME analysis cells above first\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
