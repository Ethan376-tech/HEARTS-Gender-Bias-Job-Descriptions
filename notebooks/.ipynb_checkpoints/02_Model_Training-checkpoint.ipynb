{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training for Gender Bias Detection in Job Descriptions\n",
    "\n",
    "**Project:** HEARTS Adaptation - Gender Bias Detection  \n",
    "**SDG Alignment:** SDG 5 (Gender Equality) & SDG 8 (Decent Work and Economic Growth)  \n",
    "**Models:** ALBERT-V2, DistilBERT, BERT  \n",
    "**Task:** Binary classification (Biased vs. Non-Biased job descriptions)\n",
    "\n",
    "This notebook handles:\n",
    "1. Loading preprocessed data from `01_Data_Loading_Preprocessing.ipynb` (uses pre-defined train/val/test splits: 64%/16%/20%)\n",
    "2. Fine-tuning transformer models (ALBERT-V2, DistilBERT, BERT)\n",
    "3. Model training with validation (using pre-defined splits - no re-splitting)\n",
    "4. Saving trained models to `models/job_descriptions/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "CUDA Version: 11.8\n",
      "Project root: D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\n",
      "Data directory: D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\data\n",
      "Models directory: D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\n",
      "Notebooks directory: D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, balanced_accuracy_score, confusion_matrix, roc_curve, auc, precision_recall_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, TrainerCallback\n",
    "from codecarbon import EmissionsTracker\n",
    "import torch\n",
    "\n",
    "# Set up paths - get project root (parent of notebooks directory)\n",
    "# In Jupyter notebooks, we use Path.cwd() to get current working directory\n",
    "current_dir = Path.cwd()\n",
    "# If we're in the notebooks directory, go up one level\n",
    "if current_dir.name == 'notebooks':\n",
    "    project_root = current_dir.parent\n",
    "else:\n",
    "    # If we're in the project root, use it directly\n",
    "    project_root = current_dir\n",
    "\n",
    "data_dir = project_root / 'data'\n",
    "models_dir = project_root / 'models'\n",
    "results_dir = project_root / 'results'\n",
    "notebooks_dir = project_root / 'notebooks'\n",
    "\n",
    "# Enable progress bar and set up logging\n",
    "os.environ[\"HUGGINGFACE_TRAINER_ENABLE_PROGRESS_BAR\"] = \"1\"\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.INFO)\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Models directory: {models_dir}\")\n",
    "print(f\"Notebooks directory: {notebooks_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data\n",
    "\n",
    "Load the preprocessed train/test data from the previous notebook:\n",
    "\n",
    "# Load data\n",
    "train_data, val_data, test_data = load_preprocessed_data(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loaded train split: 14065 examples\n",
      "   Loaded val split: 3517 examples\n",
      "   Loaded test data: 4396 examples\n",
      "\n",
      "Train label distribution:\n",
      "label\n",
      "0    7220\n",
      "1    6845\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Val label distribution:\n",
      "label\n",
      "0    1805\n",
      "1    1712\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test label distribution:\n",
      "label\n",
      "0    2256\n",
      "1    2140\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "def load_preprocessed_data(data_dir=None):\n",
    "    \"\"\"\n",
    "    Load preprocessed train and test data from 01_Data_Loading_Preprocessing.ipynb\n",
    "    \n",
    "    The preprocessing notebook creates train/val/test splits in data/splits/ directory.\n",
    "    This function combines train.csv and val.csv for training (the training function\n",
    "    will do its own 80/20 split internally), and uses test.csv for final evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dir : str or Path, optional\n",
    "        Directory containing preprocessed data files. If None, uses default data_dir.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    train_data : pd.DataFrame\n",
    "        Training data with 'text' and 'label' columns\n",
    "    val_data : pd.DataFrame\n",
    "        Validation data with 'text' and 'label' columns\n",
    "    test_data : pd.DataFrame\n",
    "        Test data with 'text' and 'label' columns\n",
    "    \"\"\"\n",
    "    if data_dir is None:\n",
    "        data_dir = project_root / 'data'\n",
    "    \n",
    "    # Load from splits directory\n",
    "    if isinstance(data_dir, Path):\n",
    "        splits_dir = data_dir / 'splits'\n",
    "        train_path = splits_dir / 'train.csv'\n",
    "        val_path = splits_dir / 'val.csv'\n",
    "        test_path = splits_dir / 'test.csv'\n",
    "    else:\n",
    "        splits_dir = os.path.join(data_dir, 'splits')\n",
    "        train_path = os.path.join(splits_dir, 'train.csv')\n",
    "        val_path = os.path.join(splits_dir, 'val.csv')\n",
    "        test_path = os.path.join(splits_dir, 'test.csv')\n",
    "    \n",
    "    # Check if files exist\n",
    "    missing_files = []\n",
    "    if not os.path.exists(str(train_path)):\n",
    "        missing_files.append(str(train_path))\n",
    "    if not os.path.exists(str(val_path)):\n",
    "        missing_files.append(str(val_path))\n",
    "    if not os.path.exists(str(test_path)):\n",
    "        missing_files.append(str(test_path))\n",
    "    \n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Preprocessed data not found. Please run 01_Data_Loading_Preprocessing.ipynb first.\\n\"\n",
    "            f\"Expected files:\\n  - {train_path}\\n  - {val_path}\\n  - {test_path}\\n\"\n",
    "            f\"Missing files: {missing_files}\"\n",
    "        )\n",
    "    \n",
    "    # Load train, val, and test separately (use pre-defined splits from preprocessing)\n",
    "    train_data = pd.read_csv(str(train_path))\n",
    "    val_data = pd.read_csv(str(val_path))\n",
    "    test_data = pd.read_csv(str(test_path))\n",
    "    \n",
    "    print(f\"   Loaded train split: {len(train_data)} examples\")\n",
    "    print(f\"   Loaded val split: {len(val_data)} examples\")\n",
    "    print(f\"   Loaded test data: {len(test_data)} examples\")\n",
    "    print(f\"\\nTrain label distribution:\")\n",
    "    print(train_data['label'].value_counts().sort_index())\n",
    "    print(f\"\\nVal label distribution:\")\n",
    "    print(val_data['label'].value_counts().sort_index())\n",
    "    print(f\"\\nTest label distribution:\")\n",
    "    print(test_data['label'].value_counts().sort_index())\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Load data\n",
    "train_data, val_data, test_data = load_preprocessed_data(data_dir)\n",
    "    # Early stopping callback: stops when validation loss increases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Function\n",
    "\n",
    "Adapted from HEARTS project. Trains transformer models for binary classification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, val_data, model_path, batch_size=8, epoch=8, learning_rate=2e-5,\n",
    "                gradient_accumulation_steps=4, model_output_base_dir=None, dataset_name='job_descriptions',\n",
    "                seed=42, save_model=True):\n",
    "    \"\"\"\n",
    "    Fine-tune a transformer model for binary classification\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data : pd.DataFrame\n",
    "        Training data with 'text' and 'label' columns\n",
    "    val_data : pd.DataFrame\n",
    "        Validation data with 'text' and 'label' columns\n",
    "    model_path : str\n",
    "        HuggingFace model path (e.g., 'albert/albert-base-v2')\n",
    "    batch_size : int\n",
    "        Batch size for training (default: 8)\n",
    "    epoch : int\n",
    "        Number of training epochs (default: 8)\n",
    "    learning_rate : float\n",
    "        Learning rate for training (default: 2e-5)\n",
    "    gradient_accumulation_steps : int\n",
    "        Number of gradient accumulation steps (default: 4)\n",
    "    model_output_base_dir : str or Path\n",
    "        Base directory for saving models\n",
    "    dataset_name : str\n",
    "        Dataset identifier for model naming (default: 'job_descriptions')\n",
    "    seed : int\n",
    "    save_model : bool\n",
    "        Whether to save the trained model (default: True)\n",
    "        Random seed for reproducibility (default: 42)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model_output_dir : str or Path\n",
    "        Path to saved model directory\n",
    "    \"\"\"\n",
    "    # Set random seeds for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Get number of unique labels\n",
    "    num_labels = len(train_data['label'].unique())\n",
    "    \n",
    "    # Set up output directory\n",
    "    if model_output_base_dir is None:\n",
    "        model_output_base_dir = models_dir\n",
    "    \n",
    "    if isinstance(model_output_base_dir, Path):\n",
    "        model_output_dir = model_output_base_dir / dataset_name / model_path.replace('/', '_')\n",
    "    else:\n",
    "        model_output_dir = os.path.join(model_output_base_dir, dataset_name, model_path.replace('/', '_'))\n",
    "    \n",
    "    os.makedirs(str(model_output_dir), exist_ok=True)\n",
    "    \n",
    "    # Start emissions tracking\n",
    "    # Start emissions tracking (with allow_multiple_runs to handle concurrent instances)\n",
    "    tracker = None\n",
    "    emissions = None\n",
    "    try:\n",
    "        tracker = EmissionsTracker(allow_multiple_runs=True)\n",
    "        tracker.start()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not start emissions tracking: {e}\")\n",
    "        print(\"Continuing training without emissions tracking...\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Loading Model: {model_path}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path, \n",
    "        num_labels=num_labels, \n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Handle special token cases (e.g., GPT models)\n",
    "    if model_path.startswith(\"gpt\"):\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"], \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        )\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    print(\"Tokenizing training data...\")\n",
    "    tokenized_train = Dataset.from_pandas(train_data).map(\n",
    "        tokenize_function, \n",
    "        batched=True\n",
    "    ).map(lambda examples: {'labels': examples['label']})\n",
    "    \n",
    "    print(\"Tokenizing validation data...\")\n",
    "    tokenized_val = Dataset.from_pandas(val_data).map(\n",
    "        tokenize_function, \n",
    "        batched=True\n",
    "    ).map(lambda examples: {'labels': examples['label']})\n",
    "    \n",
    "    print(\"   Tokenization complete\\n\")\n",
    "    \n",
    "    # Metrics computation function\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            labels, predictions, average='macro'\n",
    "        )\n",
    "        balanced_acc = balanced_accuracy_score(labels, predictions)\n",
    "        \n",
    "        # Print detailed classification report\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"CLASSIFICATION REPORT\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(classification_report(\n",
    "            labels, \n",
    "            predictions, \n",
    "            target_names=['Non-Biased', 'Biased'], \n",
    "            digits=4\n",
    "        ))\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        return {\n",
    "            \"precision\": precision, \n",
    "            \"recall\": recall, \n",
    "            \"f1\": f1, \n",
    "            \"balanced_accuracy\": balanced_acc\n",
    "        }\n",
    "    \n",
    "    # Set up training arguments\n",
    "    output_dir_str = str(model_output_dir) if isinstance(model_output_dir, Path) else model_output_dir\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir_str,\n",
    "        num_train_epochs=epoch,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        save_total_limit=1,\n",
    "        logging_dir=os.path.join(output_dir_str, 'logs'),\n",
    "        logging_steps=100,\n",
    "        report_to=\"none\",\n",
    "        seed=seed,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        lr_scheduler_type=\"constant\",\n",
    "        disable_tqdm=False,\n",
    "    )\n",
    "    \n",
    "    # Create Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    # Print training configuration\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"TRAINING CONFIGURATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Model: {model_path}\")\n",
    "    print(f\"Number of Epochs: {epoch}\")\n",
    "    print(f\"Learning Rate: {learning_rate:.2e}\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(f\"Gradient Accumulation Steps: {gradient_accumulation_steps}\")\n",
    "    print(f\"Effective Batch Size: {batch_size * gradient_accumulation_steps}\")\n",
    "    print(f\"Training Samples: {len(tokenized_train):,}\")\n",
    "    print(f\"Validation Samples: {len(tokenized_val):,}\")\n",
    "    print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "    print(f\"Mixed Precision (FP16): {torch.cuda.is_available()}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Start training\n",
    "    import time\n",
    "    print(f\"   Starting training...\\n\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time_seconds = end_time - start_time\n",
    "    training_time_minutes = training_time_seconds / 60\n",
    "    training_time_hours = training_time_minutes / 60\n",
    "    \n",
    "    # Save the final model (if save_model is True)\n",
    "    if save_model:\n",
    "        print(f\"\\n   Saving model...\")\n",
    "        trainer.save_model(output_dir_str)\n",
    "        tokenizer.save_pretrained(output_dir_str)\n",
    "    else:\n",
    "        print(f\"\\n   Model saving skipped (save_model=False)\")\n",
    "    \n",
    "    # Stop emissions tracking and extract emissions value\n",
    "    if tracker is not None:\n",
    "        try:\n",
    "            emissions_result = tracker.stop()\n",
    "            # Extract emissions value (handle both object and numeric return types)\n",
    "            if hasattr(emissions_result, 'emissions'):\n",
    "                emissions_kg = emissions_result.emissions\n",
    "            elif hasattr(emissions_result, 'total_emissions'):\n",
    "                emissions_kg = emissions_result.total_emissions\n",
    "            else:\n",
    "                emissions_kg = float(emissions_result) if emissions_result is not None else None\n",
    "            # Convert to grams and print\n",
    "            if emissions_kg is not None:\n",
    "                emissions_g = emissions_kg * 1000\n",
    "                print(f\"Carbon dioxide emission: {emissions_g:.6f} g\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not stop emissions tracking: {e}\")\n",
    "    return model_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model paths defined:\n",
      "  ALBERT: albert/albert-base-v2\n",
      "  DISTILBERT: distilbert/distilbert-base-uncased\n",
      "  BERT: google-bert/bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# Define model paths\n",
    "MODELS = {\n",
    "    'albert': 'albert/albert-base-v2',\n",
    "    'distilbert': 'distilbert/distilbert-base-uncased',\n",
    "    'bert': 'google-bert/bert-base-uncased'\n",
    "}\n",
    "\n",
    "print(\"Model paths defined:\")\n",
    "for name, path in MODELS.items():\n",
    "    print(f\"  {name.upper()}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# albert-v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration for albert-v2\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    'batch_size': 8,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'epochs': 8,\n",
    "    'learning_rate': 2e-5,\n",
    "    'seed': 42,\n",
    "    'save_model': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 17:56:01] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 17:56:01] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 17:56:01] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 17:56:01] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 17:56:01] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 17:56:01] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 17:56:03] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 17:56:03] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 17:56:03]   Platform system: Windows-10-10.0.26100-SP0\n",
      "[codecarbon INFO @ 17:56:03]   Python version: 3.10.19\n",
      "[codecarbon INFO @ 17:56:03]   CodeCarbon version: 2.8.0\n",
      "[codecarbon INFO @ 17:56:03]   Available RAM : 15.870 GB\n",
      "[codecarbon INFO @ 17:56:03]   CPU count: 16\n",
      "[codecarbon INFO @ 17:56:03]   CPU model: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 17:56:03]   GPU count: 1\n",
      "[codecarbon INFO @ 17:56:03]   GPU model: 1 x NVIDIA GeForce RTX 2060\n",
      "[codecarbon INFO @ 17:56:06] Saving emissions data to file D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\notebooks\\emissions.csv\n",
      "loading configuration file config.json from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Loading Model: albert/albert-base-v2\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert/albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\model.safetensors\n",
      "Some weights of the model checkpoint at albert/albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert/albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading file spiece.model from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\spiece.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--albert--albert-base-v2\\snapshots\\8e2f239c5f8a2c0f253781ca60135db913e5c80c\\config.json\n",
      "Model config AlbertConfig {\n",
      "  \"_name_or_path\": \"albert/albert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"AlbertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ac7815bdf44b37ac7895f3713f02b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14065 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c458b9878a45a099f256b8c4b13369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14065 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing validation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1835d2c075364f6dae307e14dd22ce6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3517 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520851c4759b47dd89341cd51404b758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3517 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\hearts\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tokenization complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\b'h'w'l\\AppData\\Local\\Temp\\ipykernel_23460\\1808897401.py:165: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Using auto half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 14,065\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 3,512\n",
      "  Number of trainable parameters = 11,685,122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING CONFIGURATION\n",
      "============================================================\n",
      "Model: albert/albert-base-v2\n",
      "Number of Epochs: 8\n",
      "Learning Rate: 2.00e-05\n",
      "Batch Size: 8\n",
      "Gradient Accumulation Steps: 4\n",
      "Effective Batch Size: 32\n",
      "Training Samples: 14,065\n",
      "Validation Samples: 3,517\n",
      "Device: cuda\n",
      "Mixed Precision (FP16): True\n",
      "============================================================\n",
      "\n",
      "   Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 17:56:21] Energy consumed for RAM : 0.000025 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 17:56:21] Energy consumed for all GPUs : 0.000056 kWh. Total GPU Power : 13.489401271747873 W\n",
      "[codecarbon INFO @ 17:56:21] Energy consumed for all CPUs : 0.000094 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:56:21] 0.000175 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3512' max='3512' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3512/3512 1:19:09, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.692200</td>\n",
       "      <td>0.674836</td>\n",
       "      <td>0.536110</td>\n",
       "      <td>0.504916</td>\n",
       "      <td>0.374250</td>\n",
       "      <td>0.504916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.660300</td>\n",
       "      <td>0.649336</td>\n",
       "      <td>0.604643</td>\n",
       "      <td>0.604705</td>\n",
       "      <td>0.604467</td>\n",
       "      <td>0.604705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.610800</td>\n",
       "      <td>0.601484</td>\n",
       "      <td>0.660228</td>\n",
       "      <td>0.658988</td>\n",
       "      <td>0.657296</td>\n",
       "      <td>0.658988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.514800</td>\n",
       "      <td>0.476875</td>\n",
       "      <td>0.773456</td>\n",
       "      <td>0.771974</td>\n",
       "      <td>0.772194</td>\n",
       "      <td>0.771974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.411800</td>\n",
       "      <td>0.390064</td>\n",
       "      <td>0.832910</td>\n",
       "      <td>0.828855</td>\n",
       "      <td>0.826798</td>\n",
       "      <td>0.828855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.322100</td>\n",
       "      <td>0.347993</td>\n",
       "      <td>0.848928</td>\n",
       "      <td>0.843276</td>\n",
       "      <td>0.843799</td>\n",
       "      <td>0.843276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.349121</td>\n",
       "      <td>0.859139</td>\n",
       "      <td>0.859315</td>\n",
       "      <td>0.859196</td>\n",
       "      <td>0.859315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.386209</td>\n",
       "      <td>0.861380</td>\n",
       "      <td>0.858468</td>\n",
       "      <td>0.856819</td>\n",
       "      <td>0.858468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 17:56:36] Energy consumed for RAM : 0.000050 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 17:56:36] Energy consumed for all GPUs : 0.000384 kWh. Total GPU Power : 78.654245168861 W\n",
      "[codecarbon INFO @ 17:56:36] Energy consumed for all CPUs : 0.000188 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:56:36] 0.000621 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:56:51] Energy consumed for RAM : 0.000074 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 17:56:51] Energy consumed for all GPUs : 0.000714 kWh. Total GPU Power : 79.10979954952386 W\n",
      "[codecarbon INFO @ 17:56:51] Energy consumed for all CPUs : 0.000281 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:56:51] 0.001069 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:57:06] Energy consumed for RAM : 0.000099 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 17:57:06] Energy consumed for all GPUs : 0.001045 kWh. Total GPU Power : 79.48161855834155 W\n",
      "[codecarbon INFO @ 17:57:06] Energy consumed for all CPUs : 0.000375 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:57:06] 0.001519 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:57:21] Energy consumed for RAM : 0.000124 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 17:57:21] Energy consumed for all GPUs : 0.001374 kWh. Total GPU Power : 78.9584478890803 W\n",
      "[codecarbon INFO @ 17:57:21] Energy consumed for all CPUs : 0.000469 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:57:21] 0.001967 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:57:36] Energy consumed for RAM : 0.000149 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 17:57:36] Energy consumed for all GPUs : 0.001705 kWh. Total GPU Power : 79.44057024215302 W\n",
      "[codecarbon INFO @ 17:57:36] Energy consumed for all CPUs : 0.000563 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:57:36] 0.002417 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:57:51] Energy consumed for RAM : 0.000174 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 17:57:51] Energy consumed for all GPUs : 0.002035 kWh. Total GPU Power : 79.04910967652515 W\n",
      "[codecarbon INFO @ 17:57:51] Energy consumed for all CPUs : 0.000657 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:57:51] 0.002865 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:58:06] Energy consumed for RAM : 0.000198 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 17:58:06] Energy consumed for all GPUs : 0.002366 kWh. Total GPU Power : 79.53826938075977 W\n",
      "[codecarbon INFO @ 17:58:06] Energy consumed for all CPUs : 0.000750 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:58:06] 0.003315 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:58:06] 0.006559 g.CO2eq/s mean an estimation of 206.85535628076772 kg.CO2eq/year\n",
      "[codecarbon INFO @ 17:58:21] Energy consumed for RAM : 0.000223 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 17:58:21] Energy consumed for all GPUs : 0.002696 kWh. Total GPU Power : 79.0518872126575 W\n",
      "[codecarbon INFO @ 17:58:21] Energy consumed for all CPUs : 0.000844 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:58:21] 0.003763 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:58:36] Energy consumed for RAM : 0.000248 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 17:58:36] Energy consumed for all GPUs : 0.003027 kWh. Total GPU Power : 79.60186306212904 W\n",
      "[codecarbon INFO @ 17:58:36] Energy consumed for all CPUs : 0.000938 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:58:36] 0.004213 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:58:51] Energy consumed for RAM : 0.000273 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 17:58:51] Energy consumed for all GPUs : 0.003357 kWh. Total GPU Power : 79.02462814596575 W\n",
      "[codecarbon INFO @ 17:58:51] Energy consumed for all CPUs : 0.001032 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:58:51] 0.004661 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:59:06] Energy consumed for RAM : 0.000298 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 17:59:06] Energy consumed for all GPUs : 0.003688 kWh. Total GPU Power : 79.52993589831081 W\n",
      "[codecarbon INFO @ 17:59:06] Energy consumed for all CPUs : 0.001126 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:59:06] 0.005111 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:59:21] Energy consumed for RAM : 0.000322 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 17:59:21] Energy consumed for all GPUs : 0.004018 kWh. Total GPU Power : 78.98025699173168 W\n",
      "[codecarbon INFO @ 17:59:21] Energy consumed for all CPUs : 0.001219 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:59:21] 0.005559 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:59:36] Energy consumed for RAM : 0.000347 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 17:59:36] Energy consumed for all GPUs : 0.004348 kWh. Total GPU Power : 79.32527334530572 W\n",
      "[codecarbon INFO @ 17:59:36] Energy consumed for all CPUs : 0.001313 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:59:36] 0.006009 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:59:51] Energy consumed for RAM : 0.000372 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 17:59:51] Energy consumed for all GPUs : 0.004678 kWh. Total GPU Power : 79.05183688822767 W\n",
      "[codecarbon INFO @ 17:59:51] Energy consumed for all CPUs : 0.001407 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 17:59:51] 0.006457 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:00:06] Energy consumed for RAM : 0.000397 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:00:06] Energy consumed for all GPUs : 0.005007 kWh. Total GPU Power : 79.09020551156443 W\n",
      "[codecarbon INFO @ 18:00:06] Energy consumed for all CPUs : 0.001501 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:00:06] 0.006905 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:00:06] 0.007104 g.CO2eq/s mean an estimation of 224.02288342098893 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:00:21] Energy consumed for RAM : 0.000422 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:00:21] Energy consumed for all GPUs : 0.005339 kWh. Total GPU Power : 79.60498023208017 W\n",
      "[codecarbon INFO @ 18:00:21] Energy consumed for all CPUs : 0.001595 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:00:21] 0.007355 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:00:36] Energy consumed for RAM : 0.000446 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:00:36] Energy consumed for all GPUs : 0.005669 kWh. Total GPU Power : 79.05262819136826 W\n",
      "[codecarbon INFO @ 18:00:36] Energy consumed for all CPUs : 0.001688 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:00:36] 0.007803 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:00:51] Energy consumed for RAM : 0.000471 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:00:51] Energy consumed for all GPUs : 0.006000 kWh. Total GPU Power : 79.51740940778267 W\n",
      "[codecarbon INFO @ 18:00:51] Energy consumed for all CPUs : 0.001782 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:00:51] 0.008254 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:01:06] Energy consumed for RAM : 0.000496 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:01:06] Energy consumed for all GPUs : 0.006330 kWh. Total GPU Power : 79.04261965836727 W\n",
      "[codecarbon INFO @ 18:01:06] Energy consumed for all CPUs : 0.001876 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:01:06] 0.008702 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:01:21] Energy consumed for RAM : 0.000521 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:01:21] Energy consumed for all GPUs : 0.006661 kWh. Total GPU Power : 79.5548423435944 W\n",
      "[codecarbon INFO @ 18:01:21] Energy consumed for all CPUs : 0.001970 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:01:21] 0.009152 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:01:36] Energy consumed for RAM : 0.000546 kWh. RAM Power : 5.951219558715821 W\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:01:36] Energy consumed for all GPUs : 0.006991 kWh. Total GPU Power : 78.99395428158392 W\n",
      "[codecarbon INFO @ 18:01:36] Energy consumed for all CPUs : 0.002064 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:01:36] 0.009600 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:01:51] Energy consumed for RAM : 0.000570 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:01:51] Energy consumed for all GPUs : 0.007322 kWh. Total GPU Power : 79.63671229551906 W\n",
      "[codecarbon INFO @ 18:01:51] Energy consumed for all CPUs : 0.002157 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:01:51] 0.010050 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:02:06] Energy consumed for RAM : 0.000595 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:02:06] Energy consumed for all GPUs : 0.007652 kWh. Total GPU Power : 78.99741934453452 W\n",
      "[codecarbon INFO @ 18:02:06] Energy consumed for all CPUs : 0.002251 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:02:06] 0.010498 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:02:06] 0.007110 g.CO2eq/s mean an estimation of 224.21297046545416 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:02:21] Energy consumed for RAM : 0.000620 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:02:21] Energy consumed for all GPUs : 0.007983 kWh. Total GPU Power : 79.51962172998712 W\n",
      "[codecarbon INFO @ 18:02:21] Energy consumed for all CPUs : 0.002345 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:02:21] 0.010948 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:02:36] Energy consumed for RAM : 0.000645 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:02:36] Energy consumed for all GPUs : 0.008313 kWh. Total GPU Power : 79.08182730885127 W\n",
      "[codecarbon INFO @ 18:02:36] Energy consumed for all CPUs : 0.002439 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:02:36] 0.011396 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:02:51] Energy consumed for RAM : 0.000670 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:02:51] Energy consumed for all GPUs : 0.008642 kWh. Total GPU Power : 79.07106537453913 W\n",
      "[codecarbon INFO @ 18:02:51] Energy consumed for all CPUs : 0.002532 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:02:51] 0.011844 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:03:06] Energy consumed for RAM : 0.000694 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:03:06] Energy consumed for all GPUs : 0.008973 kWh. Total GPU Power : 79.40669297346574 W\n",
      "[codecarbon INFO @ 18:03:06] Energy consumed for all CPUs : 0.002626 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:03:06] 0.012294 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:03:21] Energy consumed for RAM : 0.000719 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:03:21] Energy consumed for all GPUs : 0.009301 kWh. Total GPU Power : 78.68186323658996 W\n",
      "[codecarbon INFO @ 18:03:21] Energy consumed for all CPUs : 0.002720 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:03:21] 0.012740 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:03:36] Energy consumed for RAM : 0.000744 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:03:36] Energy consumed for all GPUs : 0.009632 kWh. Total GPU Power : 79.37800523732558 W\n",
      "[codecarbon INFO @ 18:03:36] Energy consumed for all CPUs : 0.002814 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:03:36] 0.013190 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:03:51] Energy consumed for RAM : 0.000769 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:03:51] Energy consumed for all GPUs : 0.009962 kWh. Total GPU Power : 78.98262166970744 W\n",
      "[codecarbon INFO @ 18:03:51] Energy consumed for all CPUs : 0.002908 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:03:51] 0.013638 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:04:06] Energy consumed for RAM : 0.000794 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:04:06] Energy consumed for all GPUs : 0.010293 kWh. Total GPU Power : 79.65428364199786 W\n",
      "[codecarbon INFO @ 18:04:06] Energy consumed for all CPUs : 0.003001 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:04:06] 0.014088 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:04:06] 0.007105 g.CO2eq/s mean an estimation of 224.05426675890905 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:04:21] Energy consumed for RAM : 0.000818 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:04:21] Energy consumed for all GPUs : 0.010622 kWh. Total GPU Power : 78.91581646211993 W\n",
      "[codecarbon INFO @ 18:04:21] Energy consumed for all CPUs : 0.003095 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:04:21] 0.014536 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:04:36] Energy consumed for RAM : 0.000843 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:04:36] Energy consumed for all GPUs : 0.010954 kWh. Total GPU Power : 79.49539913087861 W\n",
      "[codecarbon INFO @ 18:04:36] Energy consumed for all CPUs : 0.003189 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:04:36] 0.014986 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:04:51] Energy consumed for RAM : 0.000868 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:04:51] Energy consumed for all GPUs : 0.011283 kWh. Total GPU Power : 78.9692508207667 W\n",
      "[codecarbon INFO @ 18:04:51] Energy consumed for all CPUs : 0.003283 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:04:51] 0.015433 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:05:06] Energy consumed for RAM : 0.000893 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:05:06] Energy consumed for all GPUs : 0.011612 kWh. Total GPU Power : 79.10857556742968 W\n",
      "[codecarbon INFO @ 18:05:06] Energy consumed for all CPUs : 0.003377 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:05:06] 0.015882 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:05:21] Energy consumed for RAM : 0.000918 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:05:21] Energy consumed for all GPUs : 0.011944 kWh. Total GPU Power : 79.50281794845476 W\n",
      "[codecarbon INFO @ 18:05:21] Energy consumed for all CPUs : 0.003470 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:05:21] 0.016332 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3517\n",
      "  Batch size = 8\n",
      "[codecarbon INFO @ 18:05:36] Energy consumed for RAM : 0.000942 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:05:36] Energy consumed for all GPUs : 0.012274 kWh. Total GPU Power : 79.101735312481 W\n",
      "[codecarbon INFO @ 18:05:36] Energy consumed for all CPUs : 0.003564 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:05:36] 0.016780 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:05:51] Energy consumed for RAM : 0.000967 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:05:51] Energy consumed for all GPUs : 0.012605 kWh. Total GPU Power : 79.55118103096248 W\n",
      "[codecarbon INFO @ 18:05:51] Energy consumed for all CPUs : 0.003658 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:05:51] 0.017230 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:06:06] Energy consumed for RAM : 0.000992 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:06:06] Energy consumed for all GPUs : 0.012935 kWh. Total GPU Power : 78.99139992443888 W\n",
      "[codecarbon INFO @ 18:06:06] Energy consumed for all CPUs : 0.003752 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:06:06] 0.017679 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:06:06] 0.007103 g.CO2eq/s mean an estimation of 224.01400374995848 kg.CO2eq/year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Biased     0.5158    0.9695    0.6733      1805\n",
      "      Biased     0.5565    0.0403    0.0752      1712\n",
      "\n",
      "    accuracy                         0.5172      3517\n",
      "   macro avg     0.5361    0.5049    0.3742      3517\n",
      "weighted avg     0.5356    0.5172    0.3822      3517\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-439\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-439\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-439\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-439\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-439\\special_tokens_map.json\n",
      "[codecarbon INFO @ 18:06:21] Energy consumed for RAM : 0.001017 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:06:21] Energy consumed for all GPUs : 0.013264 kWh. Total GPU Power : 79.05936395157866 W\n",
      "[codecarbon INFO @ 18:06:21] Energy consumed for all CPUs : 0.003846 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:06:21] 0.018127 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:06:36] Energy consumed for RAM : 0.001042 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:06:36] Energy consumed for all GPUs : 0.013594 kWh. Total GPU Power : 79.19594921612442 W\n",
      "[codecarbon INFO @ 18:06:36] Energy consumed for all CPUs : 0.003939 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:06:36] 0.018575 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:06:51] Energy consumed for RAM : 0.001066 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:06:51] Energy consumed for all GPUs : 0.013924 kWh. Total GPU Power : 79.14721038046785 W\n",
      "[codecarbon INFO @ 18:06:51] Energy consumed for all CPUs : 0.004033 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:06:51] 0.019023 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:07:06] Energy consumed for RAM : 0.001091 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:07:06] Energy consumed for all GPUs : 0.014256 kWh. Total GPU Power : 79.6900670079483 W\n",
      "[codecarbon INFO @ 18:07:06] Energy consumed for all CPUs : 0.004127 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:07:06] 0.019474 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:07:21] Energy consumed for RAM : 0.001116 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:07:21] Energy consumed for all GPUs : 0.014586 kWh. Total GPU Power : 79.08487578803644 W\n",
      "[codecarbon INFO @ 18:07:21] Energy consumed for all CPUs : 0.004221 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:07:21] 0.019922 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:07:36] Energy consumed for RAM : 0.001141 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:07:36] Energy consumed for all GPUs : 0.014916 kWh. Total GPU Power : 79.17742525798204 W\n",
      "[codecarbon INFO @ 18:07:36] Energy consumed for all CPUs : 0.004315 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:07:36] 0.020371 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:07:51] Energy consumed for RAM : 0.001166 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:07:51] Energy consumed for all GPUs : 0.015248 kWh. Total GPU Power : 79.62129113707282 W\n",
      "[codecarbon INFO @ 18:07:51] Energy consumed for all CPUs : 0.004408 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:07:51] 0.020822 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:08:06] Energy consumed for RAM : 0.001190 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:08:06] Energy consumed for all GPUs : 0.015578 kWh. Total GPU Power : 79.14077417042023 W\n",
      "[codecarbon INFO @ 18:08:06] Energy consumed for all CPUs : 0.004502 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:08:06] 0.021270 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:08:06] 0.007108 g.CO2eq/s mean an estimation of 224.14602431737896 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:08:21] Energy consumed for RAM : 0.001215 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:08:21] Energy consumed for all GPUs : 0.015910 kWh. Total GPU Power : 79.72297048351315 W\n",
      "[codecarbon INFO @ 18:08:21] Energy consumed for all CPUs : 0.004596 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:08:21] 0.021721 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:08:36] Energy consumed for RAM : 0.001240 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:08:36] Energy consumed for all GPUs : 0.016240 kWh. Total GPU Power : 79.1895928323617 W\n",
      "[codecarbon INFO @ 18:08:36] Energy consumed for all CPUs : 0.004690 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:08:36] 0.022170 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:08:51] Energy consumed for RAM : 0.001265 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:08:51] Energy consumed for all GPUs : 0.016572 kWh. Total GPU Power : 79.54793962949071 W\n",
      "[codecarbon INFO @ 18:08:51] Energy consumed for all CPUs : 0.004784 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:08:51] 0.022620 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:09:06] Energy consumed for RAM : 0.001289 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:09:06] Energy consumed for all GPUs : 0.016902 kWh. Total GPU Power : 79.23159176637787 W\n",
      "[codecarbon INFO @ 18:09:06] Energy consumed for all CPUs : 0.004877 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:09:06] 0.023069 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:09:21] Energy consumed for RAM : 0.001314 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:09:21] Energy consumed for all GPUs : 0.017232 kWh. Total GPU Power : 79.17405177929795 W\n",
      "[codecarbon INFO @ 18:09:21] Energy consumed for all CPUs : 0.004971 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:09:21] 0.023517 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:09:36] Energy consumed for RAM : 0.001339 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:09:36] Energy consumed for all GPUs : 0.017564 kWh. Total GPU Power : 79.5994510131239 W\n",
      "[codecarbon INFO @ 18:09:36] Energy consumed for all CPUs : 0.005065 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:09:36] 0.023968 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:09:51] Energy consumed for RAM : 0.001364 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:09:51] Energy consumed for all GPUs : 0.017894 kWh. Total GPU Power : 79.12436381249456 W\n",
      "[codecarbon INFO @ 18:09:51] Energy consumed for all CPUs : 0.005159 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:09:51] 0.024416 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:06] Energy consumed for RAM : 0.001389 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:10:06] Energy consumed for all GPUs : 0.018226 kWh. Total GPU Power : 79.78274910793962 W\n",
      "[codecarbon INFO @ 18:10:06] Energy consumed for all CPUs : 0.005252 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:10:06] 0.024867 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:06] 0.007118 g.CO2eq/s mean an estimation of 224.46947042967784 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:10:21] Energy consumed for RAM : 0.001413 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:10:21] Energy consumed for all GPUs : 0.018556 kWh. Total GPU Power : 79.10154587833959 W\n",
      "[codecarbon INFO @ 18:10:21] Energy consumed for all CPUs : 0.005346 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:10:21] 0.025315 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:36] Energy consumed for RAM : 0.001438 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:10:36] Energy consumed for all GPUs : 0.018885 kWh. Total GPU Power : 79.0422136390067 W\n",
      "[codecarbon INFO @ 18:10:36] Energy consumed for all CPUs : 0.005440 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:10:36] 0.025763 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:10:51] Energy consumed for RAM : 0.001463 kWh. RAM Power : 5.951219558715821 W\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:10:51] Energy consumed for all GPUs : 0.019217 kWh. Total GPU Power : 79.59663251509895 W\n",
      "[codecarbon INFO @ 18:10:51] Energy consumed for all CPUs : 0.005534 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:10:51] 0.026214 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:06] Energy consumed for RAM : 0.001488 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:11:06] Energy consumed for all GPUs : 0.019546 kWh. Total GPU Power : 79.08726139991033 W\n",
      "[codecarbon INFO @ 18:11:06] Energy consumed for all CPUs : 0.005628 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:11:06] 0.026662 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:21] Energy consumed for RAM : 0.001513 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:11:21] Energy consumed for all GPUs : 0.019878 kWh. Total GPU Power : 79.62096377464205 W\n",
      "[codecarbon INFO @ 18:11:21] Energy consumed for all CPUs : 0.005722 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:11:21] 0.027113 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:36] Energy consumed for RAM : 0.001537 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:11:36] Energy consumed for all GPUs : 0.020209 kWh. Total GPU Power : 79.18386951277962 W\n",
      "[codecarbon INFO @ 18:11:36] Energy consumed for all CPUs : 0.005815 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:11:36] 0.027561 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:11:51] Energy consumed for RAM : 0.001562 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:11:51] Energy consumed for all GPUs : 0.020541 kWh. Total GPU Power : 79.80429728235413 W\n",
      "[codecarbon INFO @ 18:11:51] Energy consumed for all CPUs : 0.005909 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:11:51] 0.028012 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:12:06] Energy consumed for RAM : 0.001587 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:12:06] Energy consumed for all GPUs : 0.020872 kWh. Total GPU Power : 79.32417144684533 W\n",
      "[codecarbon INFO @ 18:12:06] Energy consumed for all CPUs : 0.006003 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:12:06] 0.028461 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:12:06] 0.007113 g.CO2eq/s mean an estimation of 224.3045965098974 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:12:21] Energy consumed for RAM : 0.001612 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:12:21] Energy consumed for all GPUs : 0.021201 kWh. Total GPU Power : 79.14501447482289 W\n",
      "[codecarbon INFO @ 18:12:21] Energy consumed for all CPUs : 0.006097 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:12:21] 0.028910 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:12:36] Energy consumed for RAM : 0.001637 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:12:36] Energy consumed for all GPUs : 0.021534 kWh. Total GPU Power : 79.68233226359905 W\n",
      "[codecarbon INFO @ 18:12:36] Energy consumed for all CPUs : 0.006190 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:12:36] 0.029361 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:12:51] Energy consumed for RAM : 0.001661 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:12:51] Energy consumed for all GPUs : 0.021864 kWh. Total GPU Power : 79.22230228674542 W\n",
      "[codecarbon INFO @ 18:12:51] Energy consumed for all CPUs : 0.006284 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:12:51] 0.029810 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:13:06] Energy consumed for RAM : 0.001686 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:13:06] Energy consumed for all GPUs : 0.022196 kWh. Total GPU Power : 79.63245350356974 W\n",
      "[codecarbon INFO @ 18:13:06] Energy consumed for all CPUs : 0.006378 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:13:06] 0.030260 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:13:21] Energy consumed for RAM : 0.001711 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:13:21] Energy consumed for all GPUs : 0.022526 kWh. Total GPU Power : 79.17977656000258 W\n",
      "[codecarbon INFO @ 18:13:21] Energy consumed for all CPUs : 0.006472 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:13:21] 0.030709 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:13:36] Energy consumed for RAM : 0.001736 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:13:36] Energy consumed for all GPUs : 0.022855 kWh. Total GPU Power : 79.05991307803642 W\n",
      "[codecarbon INFO @ 18:13:36] Energy consumed for all CPUs : 0.006566 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:13:36] 0.031157 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:13:51] Energy consumed for RAM : 0.001761 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:13:51] Energy consumed for all GPUs : 0.023187 kWh. Total GPU Power : 79.63750064532783 W\n",
      "[codecarbon INFO @ 18:13:51] Energy consumed for all CPUs : 0.006659 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:13:51] 0.031607 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:14:06] Energy consumed for RAM : 0.001785 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:14:06] Energy consumed for all GPUs : 0.023517 kWh. Total GPU Power : 79.05587998653338 W\n",
      "[codecarbon INFO @ 18:14:06] Energy consumed for all CPUs : 0.006753 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:14:06] 0.032055 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:14:06] 0.007112 g.CO2eq/s mean an estimation of 224.27795058799276 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:14:21] Energy consumed for RAM : 0.001810 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:14:21] Energy consumed for all GPUs : 0.023849 kWh. Total GPU Power : 79.67651016460694 W\n",
      "[codecarbon INFO @ 18:14:21] Energy consumed for all CPUs : 0.006847 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:14:21] 0.032506 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:14:36] Energy consumed for RAM : 0.001835 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:14:36] Energy consumed for all GPUs : 0.024179 kWh. Total GPU Power : 79.07890294764243 W\n",
      "[codecarbon INFO @ 18:14:36] Energy consumed for all CPUs : 0.006941 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:14:36] 0.032955 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:14:51] Energy consumed for RAM : 0.001860 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:14:51] Energy consumed for all GPUs : 0.024510 kWh. Total GPU Power : 79.56718362575363 W\n",
      "[codecarbon INFO @ 18:14:51] Energy consumed for all CPUs : 0.007035 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:14:51] 0.033405 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:15:06] Energy consumed for RAM : 0.001885 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:15:06] Energy consumed for all GPUs : 0.024840 kWh. Total GPU Power : 79.11294735297251 W\n",
      "[codecarbon INFO @ 18:15:06] Energy consumed for all CPUs : 0.007128 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:15:06] 0.033853 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3517\n",
      "  Batch size = 8\n",
      "[codecarbon INFO @ 18:15:21] Energy consumed for RAM : 0.001909 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:15:21] Energy consumed for all GPUs : 0.025170 kWh. Total GPU Power : 79.10690889889541 W\n",
      "[codecarbon INFO @ 18:15:21] Energy consumed for all CPUs : 0.007222 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:15:21] 0.034301 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:15:36] Energy consumed for RAM : 0.001934 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:15:36] Energy consumed for all GPUs : 0.025502 kWh. Total GPU Power : 79.57756037976829 W\n",
      "[codecarbon INFO @ 18:15:36] Energy consumed for all CPUs : 0.007316 kWh. Total CPU Power : 22.5 W\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:15:36] 0.034752 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:15:51] Energy consumed for RAM : 0.001959 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:15:51] Energy consumed for all GPUs : 0.025831 kWh. Total GPU Power : 79.05407047610021 W\n",
      "[codecarbon INFO @ 18:15:51] Energy consumed for all CPUs : 0.007410 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:15:51] 0.035200 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:16:06] Energy consumed for RAM : 0.001984 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:16:06] Energy consumed for all GPUs : 0.026160 kWh. Total GPU Power : 79.10792037037184 W\n",
      "[codecarbon INFO @ 18:16:06] Energy consumed for all CPUs : 0.007504 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:16:06] 0.035648 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:16:06] 0.007109 g.CO2eq/s mean an estimation of 224.17632533101798 kg.CO2eq/year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Biased     0.6190    0.5967    0.6076      1805\n",
      "      Biased     0.5903    0.6127    0.6013      1712\n",
      "\n",
      "    accuracy                         0.6045      3517\n",
      "   macro avg     0.6046    0.6047    0.6045      3517\n",
      "weighted avg     0.6050    0.6045    0.6046      3517\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-878\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-878\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-878\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-878\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-878\\special_tokens_map.json\n",
      "Deleting older checkpoint [D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-439] due to args.save_total_limit\n",
      "[codecarbon INFO @ 18:16:21] Energy consumed for RAM : 0.002009 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:16:21] Energy consumed for all GPUs : 0.026491 kWh. Total GPU Power : 79.42649092176154 W\n",
      "[codecarbon INFO @ 18:16:21] Energy consumed for all CPUs : 0.007597 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:16:21] 0.036097 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:16:36] Energy consumed for RAM : 0.002033 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:16:36] Energy consumed for all GPUs : 0.026821 kWh. Total GPU Power : 79.18264152308498 W\n",
      "[codecarbon INFO @ 18:16:36] Energy consumed for all CPUs : 0.007691 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:16:36] 0.036546 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:16:51] Energy consumed for RAM : 0.002058 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:16:51] Energy consumed for all GPUs : 0.027151 kWh. Total GPU Power : 79.10359431460381 W\n",
      "[codecarbon INFO @ 18:16:51] Energy consumed for all CPUs : 0.007785 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:16:51] 0.036994 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:17:06] Energy consumed for RAM : 0.002083 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:17:06] Energy consumed for all GPUs : 0.027483 kWh. Total GPU Power : 79.71907248728904 W\n",
      "[codecarbon INFO @ 18:17:06] Energy consumed for all CPUs : 0.007879 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:17:06] 0.037445 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:17:21] Energy consumed for RAM : 0.002108 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:17:21] Energy consumed for all GPUs : 0.027813 kWh. Total GPU Power : 79.14569476936659 W\n",
      "[codecarbon INFO @ 18:17:21] Energy consumed for all CPUs : 0.007973 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:17:21] 0.037894 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:17:36] Energy consumed for RAM : 0.002133 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:17:36] Energy consumed for all GPUs : 0.028146 kWh. Total GPU Power : 79.70808388019529 W\n",
      "[codecarbon INFO @ 18:17:36] Energy consumed for all CPUs : 0.008066 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:17:36] 0.038345 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:17:51] Energy consumed for RAM : 0.002157 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:17:51] Energy consumed for all GPUs : 0.028476 kWh. Total GPU Power : 79.23080049155548 W\n",
      "[codecarbon INFO @ 18:17:51] Energy consumed for all CPUs : 0.008160 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:17:51] 0.038793 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:18:06] Energy consumed for RAM : 0.002182 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:18:06] Energy consumed for all GPUs : 0.028805 kWh. Total GPU Power : 79.07465748831696 W\n",
      "[codecarbon INFO @ 18:18:06] Energy consumed for all CPUs : 0.008254 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:18:06] 0.039241 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:18:06] 0.007111 g.CO2eq/s mean an estimation of 224.2600813438431 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:18:21] Energy consumed for RAM : 0.002207 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:18:21] Energy consumed for all GPUs : 0.029138 kWh. Total GPU Power : 79.73185052619586 W\n",
      "[codecarbon INFO @ 18:18:21] Energy consumed for all CPUs : 0.008348 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:18:21] 0.039692 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:18:36] Energy consumed for RAM : 0.002232 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:18:36] Energy consumed for all GPUs : 0.029468 kWh. Total GPU Power : 79.1911633547714 W\n",
      "[codecarbon INFO @ 18:18:36] Energy consumed for all CPUs : 0.008441 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:18:36] 0.040141 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:18:51] Energy consumed for RAM : 0.002257 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:18:51] Energy consumed for all GPUs : 0.029800 kWh. Total GPU Power : 79.60905656375091 W\n",
      "[codecarbon INFO @ 18:18:51] Energy consumed for all CPUs : 0.008535 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:18:51] 0.040591 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:19:06] Energy consumed for RAM : 0.002281 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:19:06] Energy consumed for all GPUs : 0.030129 kWh. Total GPU Power : 79.10805459513318 W\n",
      "[codecarbon INFO @ 18:19:06] Energy consumed for all CPUs : 0.008629 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:19:06] 0.041040 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:19:21] Energy consumed for RAM : 0.002306 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:19:21] Energy consumed for all GPUs : 0.030461 kWh. Total GPU Power : 79.64077736173658 W\n",
      "[codecarbon INFO @ 18:19:21] Energy consumed for all CPUs : 0.008723 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:19:21] 0.041490 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:19:36] Energy consumed for RAM : 0.002331 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:19:36] Energy consumed for all GPUs : 0.030791 kWh. Total GPU Power : 79.0770526230108 W\n",
      "[codecarbon INFO @ 18:19:36] Energy consumed for all CPUs : 0.008817 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:19:36] 0.041939 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:19:51] Energy consumed for RAM : 0.002356 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:19:51] Energy consumed for all GPUs : 0.031123 kWh. Total GPU Power : 79.64829832414654 W\n",
      "[codecarbon INFO @ 18:19:51] Energy consumed for all CPUs : 0.008910 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:19:51] 0.042389 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:20:06] Energy consumed for RAM : 0.002381 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:20:06] Energy consumed for all GPUs : 0.031452 kWh. Total GPU Power : 79.02120316786652 W\n",
      "[codecarbon INFO @ 18:20:06] Energy consumed for all CPUs : 0.009004 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:20:07] 0.042837 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:20:07] 0.007115 g.CO2eq/s mean an estimation of 224.38052527852068 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:20:22] Energy consumed for RAM : 0.002405 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:20:22] Energy consumed for all GPUs : 0.031784 kWh. Total GPU Power : 79.61365120838275 W\n",
      "[codecarbon INFO @ 18:20:22] Energy consumed for all CPUs : 0.009098 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:20:22] 0.043288 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:20:37] Energy consumed for RAM : 0.002430 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:20:37] Energy consumed for all GPUs : 0.032114 kWh. Total GPU Power : 79.15259660321675 W\n",
      "[codecarbon INFO @ 18:20:37] Energy consumed for all CPUs : 0.009192 kWh. Total CPU Power : 22.5 W\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:20:37] 0.043736 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:20:52] Energy consumed for RAM : 0.002455 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:20:52] Energy consumed for all GPUs : 0.032446 kWh. Total GPU Power : 79.61877865916436 W\n",
      "[codecarbon INFO @ 18:20:52] Energy consumed for all CPUs : 0.009286 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:20:52] 0.044187 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:21:07] Energy consumed for RAM : 0.002480 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:21:07] Energy consumed for all GPUs : 0.032776 kWh. Total GPU Power : 79.25973763690557 W\n",
      "[codecarbon INFO @ 18:21:07] Energy consumed for all CPUs : 0.009379 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:21:07] 0.044635 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:21:22] Energy consumed for RAM : 0.002505 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:21:22] Energy consumed for all GPUs : 0.033106 kWh. Total GPU Power : 79.18276130757825 W\n",
      "[codecarbon INFO @ 18:21:22] Energy consumed for all CPUs : 0.009473 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:21:22] 0.045084 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:21:37] Energy consumed for RAM : 0.002529 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:21:37] Energy consumed for all GPUs : 0.033439 kWh. Total GPU Power : 79.74390914575459 W\n",
      "[codecarbon INFO @ 18:21:37] Energy consumed for all CPUs : 0.009567 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:21:37] 0.045535 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:21:52] Energy consumed for RAM : 0.002554 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:21:52] Energy consumed for all GPUs : 0.033769 kWh. Total GPU Power : 79.15745360211766 W\n",
      "[codecarbon INFO @ 18:21:52] Energy consumed for all CPUs : 0.009661 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:21:52] 0.045984 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:22:07] Energy consumed for RAM : 0.002579 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:22:07] Energy consumed for all GPUs : 0.034099 kWh. Total GPU Power : 79.18633400063538 W\n",
      "[codecarbon INFO @ 18:22:07] Energy consumed for all CPUs : 0.009755 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:22:07] 0.046432 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:22:07] 0.007114 g.CO2eq/s mean an estimation of 224.34423468326713 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:22:22] Energy consumed for RAM : 0.002604 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:22:22] Energy consumed for all GPUs : 0.034430 kWh. Total GPU Power : 79.59580617170276 W\n",
      "[codecarbon INFO @ 18:22:22] Energy consumed for all CPUs : 0.009848 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:22:22] 0.046882 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:22:37] Energy consumed for RAM : 0.002628 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:22:37] Energy consumed for all GPUs : 0.034760 kWh. Total GPU Power : 79.04201194613017 W\n",
      "[codecarbon INFO @ 18:22:37] Energy consumed for all CPUs : 0.009942 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:22:37] 0.047330 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:22:52] Energy consumed for RAM : 0.002653 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:22:52] Energy consumed for all GPUs : 0.035092 kWh. Total GPU Power : 79.62869840078552 W\n",
      "[codecarbon INFO @ 18:22:52] Energy consumed for all CPUs : 0.010036 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:22:52] 0.047781 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:23:07] Energy consumed for RAM : 0.002678 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:23:07] Energy consumed for all GPUs : 0.035421 kWh. Total GPU Power : 79.0227641485665 W\n",
      "[codecarbon INFO @ 18:23:07] Energy consumed for all CPUs : 0.010130 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:23:07] 0.048229 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:23:22] Energy consumed for RAM : 0.002703 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:23:22] Energy consumed for all GPUs : 0.035753 kWh. Total GPU Power : 79.68126878957573 W\n",
      "[codecarbon INFO @ 18:23:22] Energy consumed for all CPUs : 0.010224 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:23:22] 0.048680 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:23:37] Energy consumed for RAM : 0.002728 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:23:37] Energy consumed for all GPUs : 0.036083 kWh. Total GPU Power : 79.1530375283867 W\n",
      "[codecarbon INFO @ 18:23:37] Energy consumed for all CPUs : 0.010317 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:23:37] 0.049128 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:23:52] Energy consumed for RAM : 0.002752 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:23:52] Energy consumed for all GPUs : 0.036413 kWh. Total GPU Power : 79.18300370377162 W\n",
      "[codecarbon INFO @ 18:23:52] Energy consumed for all CPUs : 0.010411 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:23:52] 0.049577 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:24:07] Energy consumed for RAM : 0.002777 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:24:07] Energy consumed for all GPUs : 0.036746 kWh. Total GPU Power : 79.7169651171882 W\n",
      "[codecarbon INFO @ 18:24:07] Energy consumed for all CPUs : 0.010505 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:24:07] 0.050028 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:24:07] 0.007115 g.CO2eq/s mean an estimation of 224.37845199090305 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:24:22] Energy consumed for RAM : 0.002802 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:24:22] Energy consumed for all GPUs : 0.037075 kWh. Total GPU Power : 79.12976826307742 W\n",
      "[codecarbon INFO @ 18:24:22] Energy consumed for all CPUs : 0.010599 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:24:22] 0.050476 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:24:37] Energy consumed for RAM : 0.002827 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:24:37] Energy consumed for all GPUs : 0.037407 kWh. Total GPU Power : 79.65188752162346 W\n",
      "[codecarbon INFO @ 18:24:37] Energy consumed for all CPUs : 0.010693 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:24:37] 0.050927 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:24:52] Energy consumed for RAM : 0.002852 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:24:52] Energy consumed for all GPUs : 0.037737 kWh. Total GPU Power : 79.2195121909511 W\n",
      "[codecarbon INFO @ 18:24:52] Energy consumed for all CPUs : 0.010786 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:24:52] 0.051375 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:25:07] Energy consumed for RAM : 0.002876 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:25:07] Energy consumed for all GPUs : 0.038067 kWh. Total GPU Power : 79.14703190220834 W\n",
      "[codecarbon INFO @ 18:25:07] Energy consumed for all CPUs : 0.010880 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:25:07] 0.051824 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3517\n",
      "  Batch size = 8\n",
      "[codecarbon INFO @ 18:25:22] Energy consumed for RAM : 0.002901 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:25:22] Energy consumed for all GPUs : 0.038399 kWh. Total GPU Power : 79.55084267497452 W\n",
      "[codecarbon INFO @ 18:25:22] Energy consumed for all CPUs : 0.010974 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:25:22] 0.052274 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:25:37] Energy consumed for RAM : 0.002926 kWh. RAM Power : 5.951219558715821 W\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:25:37] Energy consumed for all GPUs : 0.038728 kWh. Total GPU Power : 79.01286695965442 W\n",
      "[codecarbon INFO @ 18:25:37] Energy consumed for all CPUs : 0.011068 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:25:37] 0.052722 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:25:52] Energy consumed for RAM : 0.002951 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:25:52] Energy consumed for all GPUs : 0.039058 kWh. Total GPU Power : 79.04591106638937 W\n",
      "[codecarbon INFO @ 18:25:52] Energy consumed for all CPUs : 0.011162 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:25:52] 0.053170 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Biased     0.6882    0.6089    0.6461      1805\n",
      "      Biased     0.6323    0.7091    0.6685      1712\n",
      "\n",
      "    accuracy                         0.6577      3517\n",
      "   macro avg     0.6602    0.6590    0.6573      3517\n",
      "weighted avg     0.6610    0.6577    0.6570      3517\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-1317\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-1317\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-1317\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-1317\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-1317\\special_tokens_map.json\n",
      "Deleting older checkpoint [D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-878] due to args.save_total_limit\n",
      "[codecarbon INFO @ 18:26:07] Energy consumed for RAM : 0.002976 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:26:07] Energy consumed for all GPUs : 0.039386 kWh. Total GPU Power : 78.88351208561072 W\n",
      "[codecarbon INFO @ 18:26:07] Energy consumed for all CPUs : 0.011255 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:26:07] 0.053617 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:26:07] 0.007104 g.CO2eq/s mean an estimation of 224.01978119246277 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:26:22] Energy consumed for RAM : 0.003000 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:26:22] Energy consumed for all GPUs : 0.039716 kWh. Total GPU Power : 79.10594649470768 W\n",
      "[codecarbon INFO @ 18:26:22] Energy consumed for all CPUs : 0.011349 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:26:22] 0.054066 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:26:37] Energy consumed for RAM : 0.003025 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:26:37] Energy consumed for all GPUs : 0.040046 kWh. Total GPU Power : 79.12863172118855 W\n",
      "[codecarbon INFO @ 18:26:37] Energy consumed for all CPUs : 0.011443 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:26:37] 0.054514 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:26:52] Energy consumed for RAM : 0.003050 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:26:52] Energy consumed for all GPUs : 0.040378 kWh. Total GPU Power : 79.77438356570406 W\n",
      "[codecarbon INFO @ 18:26:52] Energy consumed for all CPUs : 0.011537 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:26:52] 0.054965 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:27:07] Energy consumed for RAM : 0.003075 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:27:07] Energy consumed for all GPUs : 0.040708 kWh. Total GPU Power : 79.11030332470447 W\n",
      "[codecarbon INFO @ 18:27:07] Energy consumed for all CPUs : 0.011630 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:27:07] 0.055413 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:27:22] Energy consumed for RAM : 0.003100 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:27:22] Energy consumed for all GPUs : 0.041040 kWh. Total GPU Power : 79.66335860585468 W\n",
      "[codecarbon INFO @ 18:27:22] Energy consumed for all CPUs : 0.011724 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:27:22] 0.055864 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:27:37] Energy consumed for RAM : 0.003124 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:27:37] Energy consumed for all GPUs : 0.041370 kWh. Total GPU Power : 79.12030959759397 W\n",
      "[codecarbon INFO @ 18:27:37] Energy consumed for all CPUs : 0.011818 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:27:37] 0.056312 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:27:52] Energy consumed for RAM : 0.003149 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:27:52] Energy consumed for all GPUs : 0.041702 kWh. Total GPU Power : 79.6427675576319 W\n",
      "[codecarbon INFO @ 18:27:52] Energy consumed for all CPUs : 0.011912 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:27:52] 0.056763 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:28:07] Energy consumed for RAM : 0.003174 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:28:07] Energy consumed for all GPUs : 0.042032 kWh. Total GPU Power : 79.0846678014726 W\n",
      "[codecarbon INFO @ 18:28:07] Energy consumed for all CPUs : 0.012006 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:28:07] 0.057211 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:28:07] 0.007112 g.CO2eq/s mean an estimation of 224.2746926065021 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:28:22] Energy consumed for RAM : 0.003199 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:28:22] Energy consumed for all GPUs : 0.042364 kWh. Total GPU Power : 79.63615588247274 W\n",
      "[codecarbon INFO @ 18:28:22] Energy consumed for all CPUs : 0.012099 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:28:22] 0.057662 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:28:37] Energy consumed for RAM : 0.003224 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:28:37] Energy consumed for all GPUs : 0.042694 kWh. Total GPU Power : 79.17042038331962 W\n",
      "[codecarbon INFO @ 18:28:37] Energy consumed for all CPUs : 0.012193 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:28:37] 0.058110 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:28:52] Energy consumed for RAM : 0.003248 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:28:52] Energy consumed for all GPUs : 0.043023 kWh. Total GPU Power : 79.13613144560483 W\n",
      "[codecarbon INFO @ 18:28:52] Energy consumed for all CPUs : 0.012287 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:28:52] 0.058559 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:29:07] Energy consumed for RAM : 0.003273 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:29:07] Energy consumed for all GPUs : 0.043355 kWh. Total GPU Power : 79.66188019976285 W\n",
      "[codecarbon INFO @ 18:29:07] Energy consumed for all CPUs : 0.012381 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:29:07] 0.059009 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:29:22] Energy consumed for RAM : 0.003298 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:29:22] Energy consumed for all GPUs : 0.043685 kWh. Total GPU Power : 79.07475138730729 W\n",
      "[codecarbon INFO @ 18:29:22] Energy consumed for all CPUs : 0.012475 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:29:22] 0.059458 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:29:37] Energy consumed for RAM : 0.003323 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:29:37] Energy consumed for all GPUs : 0.044017 kWh. Total GPU Power : 79.5948833799707 W\n",
      "[codecarbon INFO @ 18:29:37] Energy consumed for all CPUs : 0.012568 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:29:37] 0.059908 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:29:52] Energy consumed for RAM : 0.003348 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:29:52] Energy consumed for all GPUs : 0.044347 kWh. Total GPU Power : 79.22332430574396 W\n",
      "[codecarbon INFO @ 18:29:52] Energy consumed for all CPUs : 0.012662 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:29:52] 0.060356 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:30:07] Energy consumed for RAM : 0.003372 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:30:07] Energy consumed for all GPUs : 0.044676 kWh. Total GPU Power : 79.09962091349442 W\n",
      "[codecarbon INFO @ 18:30:07] Energy consumed for all CPUs : 0.012756 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:30:07] 0.060805 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:30:07] 0.007111 g.CO2eq/s mean an estimation of 224.24553243333497 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:30:22] Energy consumed for RAM : 0.003397 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:30:22] Energy consumed for all GPUs : 0.045008 kWh. Total GPU Power : 79.62586632437862 W\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:30:22] Energy consumed for all CPUs : 0.012850 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:30:22] 0.061255 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:30:37] Energy consumed for RAM : 0.003422 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:30:37] Energy consumed for all GPUs : 0.045339 kWh. Total GPU Power : 79.20628453506549 W\n",
      "[codecarbon INFO @ 18:30:37] Energy consumed for all CPUs : 0.012944 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:30:37] 0.061704 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:30:52] Energy consumed for RAM : 0.003447 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:30:52] Energy consumed for all GPUs : 0.045670 kWh. Total GPU Power : 79.63806595743742 W\n",
      "[codecarbon INFO @ 18:30:52] Energy consumed for all CPUs : 0.013037 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:30:52] 0.062155 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:31:07] Energy consumed for RAM : 0.003472 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:31:07] Energy consumed for all GPUs : 0.046001 kWh. Total GPU Power : 79.24641955960033 W\n",
      "[codecarbon INFO @ 18:31:07] Energy consumed for all CPUs : 0.013131 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:31:07] 0.062604 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:31:22] Energy consumed for RAM : 0.003496 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:31:22] Energy consumed for all GPUs : 0.046332 kWh. Total GPU Power : 79.55394047876433 W\n",
      "[codecarbon INFO @ 18:31:22] Energy consumed for all CPUs : 0.013225 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:31:22] 0.063054 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:31:37] Energy consumed for RAM : 0.003521 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:31:37] Energy consumed for all GPUs : 0.046662 kWh. Total GPU Power : 79.20020646245484 W\n",
      "[codecarbon INFO @ 18:31:37] Energy consumed for all CPUs : 0.013319 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:31:37] 0.063502 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:31:52] Energy consumed for RAM : 0.003546 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:31:52] Energy consumed for all GPUs : 0.046995 kWh. Total GPU Power : 79.68107577314716 W\n",
      "[codecarbon INFO @ 18:31:52] Energy consumed for all CPUs : 0.013413 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:31:52] 0.063953 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:32:07] Energy consumed for RAM : 0.003571 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:32:07] Energy consumed for all GPUs : 0.047325 kWh. Total GPU Power : 79.20750233397115 W\n",
      "[codecarbon INFO @ 18:32:07] Energy consumed for all CPUs : 0.013506 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:32:07] 0.064402 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:32:07] 0.007117 g.CO2eq/s mean an estimation of 224.45498646689438 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:32:22] Energy consumed for RAM : 0.003596 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:32:22] Energy consumed for all GPUs : 0.047655 kWh. Total GPU Power : 79.1090957560483 W\n",
      "[codecarbon INFO @ 18:32:22] Energy consumed for all CPUs : 0.013600 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:32:22] 0.064850 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:32:37] Energy consumed for RAM : 0.003620 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:32:37] Energy consumed for all GPUs : 0.047986 kWh. Total GPU Power : 79.62299776700311 W\n",
      "[codecarbon INFO @ 18:32:37] Energy consumed for all CPUs : 0.013694 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:32:37] 0.065301 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:32:52] Energy consumed for RAM : 0.003645 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:32:52] Energy consumed for all GPUs : 0.048316 kWh. Total GPU Power : 79.10918220900628 W\n",
      "[codecarbon INFO @ 18:32:52] Energy consumed for all CPUs : 0.013788 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:32:52] 0.065749 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:33:07] Energy consumed for RAM : 0.003670 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:33:07] Energy consumed for all GPUs : 0.048648 kWh. Total GPU Power : 79.62576490663895 W\n",
      "[codecarbon INFO @ 18:33:07] Energy consumed for all CPUs : 0.013881 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:33:07] 0.066200 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:33:22] Energy consumed for RAM : 0.003695 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:33:22] Energy consumed for all GPUs : 0.048978 kWh. Total GPU Power : 79.15111128798618 W\n",
      "[codecarbon INFO @ 18:33:22] Energy consumed for all CPUs : 0.013975 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:33:22] 0.066648 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:33:37] Energy consumed for RAM : 0.003720 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:33:37] Energy consumed for all GPUs : 0.049308 kWh. Total GPU Power : 79.14529647814092 W\n",
      "[codecarbon INFO @ 18:33:37] Energy consumed for all CPUs : 0.014069 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:33:37] 0.067096 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:33:52] Energy consumed for RAM : 0.003744 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:33:52] Energy consumed for all GPUs : 0.049639 kWh. Total GPU Power : 79.62877219260011 W\n",
      "[codecarbon INFO @ 18:33:52] Energy consumed for all CPUs : 0.014163 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:33:52] 0.067547 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:34:07] Energy consumed for RAM : 0.003769 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:34:07] Energy consumed for all GPUs : 0.049969 kWh. Total GPU Power : 79.21132647329908 W\n",
      "[codecarbon INFO @ 18:34:07] Energy consumed for all CPUs : 0.014257 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:34:07] 0.067995 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:34:07] 0.007112 g.CO2eq/s mean an estimation of 224.27593699294863 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:34:22] Energy consumed for RAM : 0.003794 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:34:22] Energy consumed for all GPUs : 0.050301 kWh. Total GPU Power : 79.54747546642263 W\n",
      "[codecarbon INFO @ 18:34:22] Energy consumed for all CPUs : 0.014350 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:34:22] 0.068446 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:34:37] Energy consumed for RAM : 0.003819 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:34:37] Energy consumed for all GPUs : 0.050631 kWh. Total GPU Power : 79.12054603241891 W\n",
      "[codecarbon INFO @ 18:34:37] Energy consumed for all CPUs : 0.014444 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:34:37] 0.068894 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:34:52] Energy consumed for RAM : 0.003844 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:34:52] Energy consumed for all GPUs : 0.050963 kWh. Total GPU Power : 79.48824364950694 W\n",
      "[codecarbon INFO @ 18:34:52] Energy consumed for all CPUs : 0.014538 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:34:52] 0.069344 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3517\n",
      "  Batch size = 8\n",
      "[codecarbon INFO @ 18:35:07] Energy consumed for RAM : 0.003868 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:35:07] Energy consumed for all GPUs : 0.051292 kWh. Total GPU Power : 79.23459285948285 W\n",
      "[codecarbon INFO @ 18:35:07] Energy consumed for all CPUs : 0.014632 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:35:07] 0.069793 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:35:22] Energy consumed for RAM : 0.003893 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:35:22] Energy consumed for all GPUs : 0.051622 kWh. Total GPU Power : 78.98917647127668 W\n",
      "[codecarbon INFO @ 18:35:22] Energy consumed for all CPUs : 0.014726 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:35:22] 0.070240 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:35:37] Energy consumed for RAM : 0.003918 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:35:37] Energy consumed for all GPUs : 0.051951 kWh. Total GPU Power : 79.00494964336336 W\n",
      "[codecarbon INFO @ 18:35:37] Energy consumed for all CPUs : 0.014819 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:35:37] 0.070688 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:35:52] Energy consumed for RAM : 0.003943 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:35:52] Energy consumed for all GPUs : 0.052283 kWh. Total GPU Power : 79.60459688143322 W\n",
      "[codecarbon INFO @ 18:35:52] Energy consumed for all CPUs : 0.014913 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:35:52] 0.071139 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Biased     0.7653    0.8039    0.7841      1805\n",
      "      Biased     0.7816    0.7401    0.7603      1712\n",
      "\n",
      "    accuracy                         0.7728      3517\n",
      "   macro avg     0.7735    0.7720    0.7722      3517\n",
      "weighted avg     0.7732    0.7728    0.7725      3517\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-1757\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-1757\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-1757\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-1757\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-1757\\special_tokens_map.json\n",
      "Deleting older checkpoint [D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-1317] due to args.save_total_limit\n",
      "[codecarbon INFO @ 18:36:07] Energy consumed for RAM : 0.003968 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:36:07] Energy consumed for all GPUs : 0.052610 kWh. Total GPU Power : 78.49556635514901 W\n",
      "[codecarbon INFO @ 18:36:07] Energy consumed for all CPUs : 0.015007 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:36:07] 0.071585 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:36:07] 0.007102 g.CO2eq/s mean an estimation of 223.97727218171585 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:36:22] Energy consumed for RAM : 0.003992 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:36:22] Energy consumed for all GPUs : 0.052942 kWh. Total GPU Power : 79.57341672127099 W\n",
      "[codecarbon INFO @ 18:36:22] Energy consumed for all CPUs : 0.015101 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:36:22] 0.072035 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:36:37] Energy consumed for RAM : 0.004017 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:36:37] Energy consumed for all GPUs : 0.053271 kWh. Total GPU Power : 79.01160805516506 W\n",
      "[codecarbon INFO @ 18:36:37] Energy consumed for all CPUs : 0.015195 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:36:37] 0.072483 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:36:52] Energy consumed for RAM : 0.004042 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:36:52] Energy consumed for all GPUs : 0.053601 kWh. Total GPU Power : 79.06701836155628 W\n",
      "[codecarbon INFO @ 18:36:52] Energy consumed for all CPUs : 0.015288 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:36:52] 0.072931 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:37:07] Energy consumed for RAM : 0.004067 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:37:07] Energy consumed for all GPUs : 0.053933 kWh. Total GPU Power : 79.58295406899384 W\n",
      "[codecarbon INFO @ 18:37:07] Energy consumed for all CPUs : 0.015382 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:37:07] 0.073382 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:37:22] Energy consumed for RAM : 0.004092 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:37:22] Energy consumed for all GPUs : 0.054264 kWh. Total GPU Power : 79.48798400120009 W\n",
      "[codecarbon INFO @ 18:37:22] Energy consumed for all CPUs : 0.015476 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:37:22] 0.073832 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:37:37] Energy consumed for RAM : 0.004116 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:37:37] Energy consumed for all GPUs : 0.054594 kWh. Total GPU Power : 79.17871945173033 W\n",
      "[codecarbon INFO @ 18:37:37] Energy consumed for all CPUs : 0.015570 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:37:37] 0.074280 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:37:52] Energy consumed for RAM : 0.004141 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:37:52] Energy consumed for all GPUs : 0.054923 kWh. Total GPU Power : 78.94740597999743 W\n",
      "[codecarbon INFO @ 18:37:52] Energy consumed for all CPUs : 0.015664 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:37:52] 0.074728 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:38:07] Energy consumed for RAM : 0.004166 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:38:07] Energy consumed for all GPUs : 0.055255 kWh. Total GPU Power : 79.55935448779334 W\n",
      "[codecarbon INFO @ 18:38:07] Energy consumed for all CPUs : 0.015757 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:38:07] 0.075178 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:38:07] 0.007110 g.CO2eq/s mean an estimation of 224.2139650760727 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:38:22] Energy consumed for RAM : 0.004191 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:38:22] Energy consumed for all GPUs : 0.055584 kWh. Total GPU Power : 79.06078177587516 W\n",
      "[codecarbon INFO @ 18:38:22] Energy consumed for all CPUs : 0.015851 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:38:22] 0.075626 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:38:37] Energy consumed for RAM : 0.004216 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:38:37] Energy consumed for all GPUs : 0.055916 kWh. Total GPU Power : 79.57981838942634 W\n",
      "[codecarbon INFO @ 18:38:37] Energy consumed for all CPUs : 0.015945 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:38:37] 0.076077 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:38:52] Energy consumed for RAM : 0.004240 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:38:52] Energy consumed for all GPUs : 0.056246 kWh. Total GPU Power : 78.987439378716 W\n",
      "[codecarbon INFO @ 18:38:52] Energy consumed for all CPUs : 0.016039 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:38:52] 0.076525 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:39:07] Energy consumed for RAM : 0.004265 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:39:07] Energy consumed for all GPUs : 0.056577 kWh. Total GPU Power : 79.50051388554412 W\n",
      "[codecarbon INFO @ 18:39:07] Energy consumed for all CPUs : 0.016133 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:39:07] 0.076975 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:39:22] Energy consumed for RAM : 0.004290 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:39:22] Energy consumed for all GPUs : 0.056906 kWh. Total GPU Power : 79.01548986364152 W\n",
      "[codecarbon INFO @ 18:39:22] Energy consumed for all CPUs : 0.016226 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:39:22] 0.077423 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:39:37] Energy consumed for RAM : 0.004315 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:39:37] Energy consumed for all GPUs : 0.057236 kWh. Total GPU Power : 79.128488965331 W\n",
      "[codecarbon INFO @ 18:39:37] Energy consumed for all CPUs : 0.016320 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:39:37] 0.077871 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:39:52] Energy consumed for RAM : 0.004340 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:39:52] Energy consumed for all GPUs : 0.057568 kWh. Total GPU Power : 79.58849265312843 W\n",
      "[codecarbon INFO @ 18:39:52] Energy consumed for all CPUs : 0.016414 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:39:52] 0.078321 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:40:07] Energy consumed for RAM : 0.004364 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:40:07] Energy consumed for all GPUs : 0.057897 kWh. Total GPU Power : 79.04133640793341 W\n",
      "[codecarbon INFO @ 18:40:07] Energy consumed for all CPUs : 0.016508 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:40:07] 0.078769 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:40:07] 0.007106 g.CO2eq/s mean an estimation of 224.08973173146674 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:40:22] Energy consumed for RAM : 0.004389 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:40:22] Energy consumed for all GPUs : 0.058229 kWh. Total GPU Power : 79.56995571347265 W\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:40:22] Energy consumed for all CPUs : 0.016602 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:40:22] 0.079219 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:40:37] Energy consumed for RAM : 0.004414 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:40:37] Energy consumed for all GPUs : 0.058558 kWh. Total GPU Power : 79.04375736530665 W\n",
      "[codecarbon INFO @ 18:40:37] Energy consumed for all CPUs : 0.016695 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:40:37] 0.079668 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:40:52] Energy consumed for RAM : 0.004439 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:40:52] Energy consumed for all GPUs : 0.058888 kWh. Total GPU Power : 79.02824996039568 W\n",
      "[codecarbon INFO @ 18:40:52] Energy consumed for all CPUs : 0.016789 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:40:52] 0.080116 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:41:07] Energy consumed for RAM : 0.004464 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:41:07] Energy consumed for all GPUs : 0.059219 kWh. Total GPU Power : 79.630450425194 W\n",
      "[codecarbon INFO @ 18:41:07] Energy consumed for all CPUs : 0.016883 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:41:07] 0.080566 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:41:22] Energy consumed for RAM : 0.004488 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:41:22] Energy consumed for all GPUs : 0.059549 kWh. Total GPU Power : 79.01215504146481 W\n",
      "[codecarbon INFO @ 18:41:22] Energy consumed for all CPUs : 0.016977 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:41:22] 0.081014 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:41:37] Energy consumed for RAM : 0.004513 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:41:37] Energy consumed for all GPUs : 0.059880 kWh. Total GPU Power : 79.57835011705968 W\n",
      "[codecarbon INFO @ 18:41:37] Energy consumed for all CPUs : 0.017071 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:41:37] 0.081464 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:41:52] Energy consumed for RAM : 0.004538 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:41:52] Energy consumed for all GPUs : 0.060210 kWh. Total GPU Power : 79.1187614540934 W\n",
      "[codecarbon INFO @ 18:41:52] Energy consumed for all CPUs : 0.017164 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:41:52] 0.081912 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:42:07] Energy consumed for RAM : 0.004563 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:42:07] Energy consumed for all GPUs : 0.060539 kWh. Total GPU Power : 79.02210544823762 W\n",
      "[codecarbon INFO @ 18:42:07] Energy consumed for all CPUs : 0.017258 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:42:07] 0.082360 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:42:07] 0.007107 g.CO2eq/s mean an estimation of 224.11514610574136 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:42:22] Energy consumed for RAM : 0.004588 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:42:22] Energy consumed for all GPUs : 0.060871 kWh. Total GPU Power : 79.54916226689545 W\n",
      "[codecarbon INFO @ 18:42:22] Energy consumed for all CPUs : 0.017352 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:42:22] 0.082810 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:42:37] Energy consumed for RAM : 0.004612 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:42:37] Energy consumed for all GPUs : 0.061200 kWh. Total GPU Power : 79.0232933632289 W\n",
      "[codecarbon INFO @ 18:42:37] Energy consumed for all CPUs : 0.017446 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:42:37] 0.083258 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:42:52] Energy consumed for RAM : 0.004637 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:42:52] Energy consumed for all GPUs : 0.061532 kWh. Total GPU Power : 79.51153562228832 W\n",
      "[codecarbon INFO @ 18:42:52] Energy consumed for all CPUs : 0.017539 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:42:52] 0.083708 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:43:07] Energy consumed for RAM : 0.004662 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:43:07] Energy consumed for all GPUs : 0.061861 kWh. Total GPU Power : 79.00361389923813 W\n",
      "[codecarbon INFO @ 18:43:07] Energy consumed for all CPUs : 0.017633 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:43:07] 0.084157 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:43:22] Energy consumed for RAM : 0.004687 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:43:22] Energy consumed for all GPUs : 0.062193 kWh. Total GPU Power : 79.69038473262063 W\n",
      "[codecarbon INFO @ 18:43:22] Energy consumed for all CPUs : 0.017727 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:43:22] 0.084607 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:43:37] Energy consumed for RAM : 0.004712 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:43:37] Energy consumed for all GPUs : 0.062523 kWh. Total GPU Power : 79.0912040722555 W\n",
      "[codecarbon INFO @ 18:43:37] Energy consumed for all CPUs : 0.017821 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:43:37] 0.085055 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:43:52] Energy consumed for RAM : 0.004736 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:43:52] Energy consumed for all GPUs : 0.062855 kWh. Total GPU Power : 79.618780541824 W\n",
      "[codecarbon INFO @ 18:43:52] Energy consumed for all CPUs : 0.017915 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:43:52] 0.085506 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:44:07] Energy consumed for RAM : 0.004761 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:44:07] Energy consumed for all GPUs : 0.063185 kWh. Total GPU Power : 79.14603945699709 W\n",
      "[codecarbon INFO @ 18:44:07] Energy consumed for all CPUs : 0.018008 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:44:07] 0.085954 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:44:07] 0.007112 g.CO2eq/s mean an estimation of 224.28374076775694 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:44:22] Energy consumed for RAM : 0.004786 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:44:22] Energy consumed for all GPUs : 0.063514 kWh. Total GPU Power : 79.01664561361277 W\n",
      "[codecarbon INFO @ 18:44:22] Energy consumed for all CPUs : 0.018102 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:44:22] 0.086403 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:44:37] Energy consumed for RAM : 0.004811 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:44:37] Energy consumed for all GPUs : 0.063846 kWh. Total GPU Power : 79.56957012621332 W\n",
      "[codecarbon INFO @ 18:44:37] Energy consumed for all CPUs : 0.018196 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:44:37] 0.086853 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:44:52] Energy consumed for RAM : 0.004836 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:44:52] Energy consumed for all GPUs : 0.064176 kWh. Total GPU Power : 79.10207990787076 W\n",
      "[codecarbon INFO @ 18:44:52] Energy consumed for all CPUs : 0.018290 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:44:52] 0.087301 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3517\n",
      "  Batch size = 8\n",
      "[codecarbon INFO @ 18:45:07] Energy consumed for RAM : 0.004860 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:45:07] Energy consumed for all GPUs : 0.064507 kWh. Total GPU Power : 79.47961565318731 W\n",
      "[codecarbon INFO @ 18:45:07] Energy consumed for all CPUs : 0.018384 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:45:07] 0.087751 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:45:22] Energy consumed for RAM : 0.004885 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:45:22] Energy consumed for all GPUs : 0.064836 kWh. Total GPU Power : 79.03680164326458 W\n",
      "[codecarbon INFO @ 18:45:22] Energy consumed for all CPUs : 0.018477 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:45:22] 0.088199 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:45:37] Energy consumed for RAM : 0.004910 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:45:37] Energy consumed for all GPUs : 0.065166 kWh. Total GPU Power : 78.92944026095202 W\n",
      "[codecarbon INFO @ 18:45:37] Energy consumed for all CPUs : 0.018571 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:45:37] 0.088647 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Biased     0.8839    0.7634    0.8193      1805\n",
      "      Biased     0.7819    0.8943    0.8343      1712\n",
      "\n",
      "    accuracy                         0.8271      3517\n",
      "   macro avg     0.8329    0.8289    0.8268      3517\n",
      "weighted avg     0.8343    0.8271    0.8266      3517\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-2196\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-2196\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-2196\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-2196\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-2196\\special_tokens_map.json\n",
      "Deleting older checkpoint [D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-1757] due to args.save_total_limit\n",
      "[codecarbon INFO @ 18:45:52] Energy consumed for RAM : 0.004935 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:45:52] Energy consumed for all GPUs : 0.065496 kWh. Total GPU Power : 79.2787935013975 W\n",
      "[codecarbon INFO @ 18:45:52] Energy consumed for all CPUs : 0.018665 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:45:52] 0.089096 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:46:07] Energy consumed for RAM : 0.004960 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:46:07] Energy consumed for all GPUs : 0.065826 kWh. Total GPU Power : 79.03813877579816 W\n",
      "[codecarbon INFO @ 18:46:07] Energy consumed for all CPUs : 0.018759 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:46:07] 0.089544 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:46:07] 0.007102 g.CO2eq/s mean an estimation of 223.9658766310006 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:46:22] Energy consumed for RAM : 0.004984 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:46:22] Energy consumed for all GPUs : 0.066158 kWh. Total GPU Power : 79.72265021733217 W\n",
      "[codecarbon INFO @ 18:46:22] Energy consumed for all CPUs : 0.018853 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:46:22] 0.089995 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:46:37] Energy consumed for RAM : 0.005009 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:46:37] Energy consumed for all GPUs : 0.066488 kWh. Total GPU Power : 79.2928322046212 W\n",
      "[codecarbon INFO @ 18:46:37] Energy consumed for all CPUs : 0.018946 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:46:37] 0.090444 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:46:52] Energy consumed for RAM : 0.005034 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:46:52] Energy consumed for all GPUs : 0.066818 kWh. Total GPU Power : 79.1651106874693 W\n",
      "[codecarbon INFO @ 18:46:52] Energy consumed for all CPUs : 0.019040 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:46:52] 0.090892 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:47:07] Energy consumed for RAM : 0.005059 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:47:07] Energy consumed for all GPUs : 0.067150 kWh. Total GPU Power : 79.65746879885829 W\n",
      "[codecarbon INFO @ 18:47:07] Energy consumed for all CPUs : 0.019134 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:47:07] 0.091343 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:47:22] Energy consumed for RAM : 0.005084 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:47:22] Energy consumed for all GPUs : 0.067480 kWh. Total GPU Power : 79.15965707107492 W\n",
      "[codecarbon INFO @ 18:47:22] Energy consumed for all CPUs : 0.019228 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:47:22] 0.091792 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:47:37] Energy consumed for RAM : 0.005108 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:47:37] Energy consumed for all GPUs : 0.067810 kWh. Total GPU Power : 79.14235804837686 W\n",
      "[codecarbon INFO @ 18:47:37] Energy consumed for all CPUs : 0.019322 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:47:37] 0.092240 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:47:52] Energy consumed for RAM : 0.005133 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:47:52] Energy consumed for all GPUs : 0.068143 kWh. Total GPU Power : 79.84454915514277 W\n",
      "[codecarbon INFO @ 18:47:52] Energy consumed for all CPUs : 0.019415 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:47:52] 0.092691 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:48:07] Energy consumed for RAM : 0.005158 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:48:07] Energy consumed for all GPUs : 0.068472 kWh. Total GPU Power : 79.11742511678236 W\n",
      "[codecarbon INFO @ 18:48:07] Energy consumed for all CPUs : 0.019509 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:48:07] 0.093140 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:48:07] 0.007115 g.CO2eq/s mean an estimation of 224.3827052896619 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:48:22] Energy consumed for RAM : 0.005183 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:48:22] Energy consumed for all GPUs : 0.068804 kWh. Total GPU Power : 79.62902252141586 W\n",
      "[codecarbon INFO @ 18:48:22] Energy consumed for all CPUs : 0.019603 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:48:22] 0.093590 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:48:37] Energy consumed for RAM : 0.005208 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:48:37] Energy consumed for all GPUs : 0.069134 kWh. Total GPU Power : 79.15426091949534 W\n",
      "[codecarbon INFO @ 18:48:37] Energy consumed for all CPUs : 0.019697 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:48:37] 0.094039 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:48:52] Energy consumed for RAM : 0.005232 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:48:52] Energy consumed for all GPUs : 0.069463 kWh. Total GPU Power : 79.02978076073258 W\n",
      "[codecarbon INFO @ 18:48:52] Energy consumed for all CPUs : 0.019791 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:48:52] 0.094486 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:49:07] Energy consumed for RAM : 0.005257 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:49:07] Energy consumed for all GPUs : 0.069795 kWh. Total GPU Power : 79.4922890497433 W\n",
      "[codecarbon INFO @ 18:49:07] Energy consumed for all CPUs : 0.019884 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:49:07] 0.094936 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:49:22] Energy consumed for RAM : 0.005282 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:49:22] Energy consumed for all GPUs : 0.070125 kWh. Total GPU Power : 79.17060959618792 W\n",
      "[codecarbon INFO @ 18:49:22] Energy consumed for all CPUs : 0.019978 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:49:22] 0.095385 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:49:37] Energy consumed for RAM : 0.005307 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:49:38] Energy consumed for all GPUs : 0.070457 kWh. Total GPU Power : 79.60310648951722 W\n",
      "[codecarbon INFO @ 18:49:38] Energy consumed for all CPUs : 0.020072 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:49:38] 0.095836 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:49:53] Energy consumed for RAM : 0.005331 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:49:53] Energy consumed for all GPUs : 0.070787 kWh. Total GPU Power : 79.29338196356895 W\n",
      "[codecarbon INFO @ 18:49:53] Energy consumed for all CPUs : 0.020166 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:49:53] 0.096284 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:50:08] Energy consumed for RAM : 0.005356 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:50:08] Energy consumed for all GPUs : 0.071116 kWh. Total GPU Power : 79.03071629880696 W\n",
      "[codecarbon INFO @ 18:50:08] Energy consumed for all CPUs : 0.020260 kWh. Total CPU Power : 22.5 W\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 18:50:08] 0.096732 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:50:08] 0.007110 g.CO2eq/s mean an estimation of 224.20801159948175 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:50:23] Energy consumed for RAM : 0.005381 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:50:23] Energy consumed for all GPUs : 0.071449 kWh. Total GPU Power : 79.69776165855899 W\n",
      "[codecarbon INFO @ 18:50:23] Energy consumed for all CPUs : 0.020353 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:50:23] 0.097183 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:50:38] Energy consumed for RAM : 0.005406 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:50:38] Energy consumed for all GPUs : 0.071779 kWh. Total GPU Power : 79.09825974197159 W\n",
      "[codecarbon INFO @ 18:50:38] Energy consumed for all CPUs : 0.020447 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:50:38] 0.097632 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:50:53] Energy consumed for RAM : 0.005431 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:50:53] Energy consumed for all GPUs : 0.072108 kWh. Total GPU Power : 79.08021106797328 W\n",
      "[codecarbon INFO @ 18:50:53] Energy consumed for all CPUs : 0.020541 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:50:53] 0.098080 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:51:08] Energy consumed for RAM : 0.005455 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:51:08] Energy consumed for all GPUs : 0.072440 kWh. Total GPU Power : 79.64543273838741 W\n",
      "[codecarbon INFO @ 18:51:08] Energy consumed for all CPUs : 0.020635 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:51:08] 0.098530 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:51:23] Energy consumed for RAM : 0.005480 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:51:23] Energy consumed for all GPUs : 0.072770 kWh. Total GPU Power : 79.11799237755666 W\n",
      "[codecarbon INFO @ 18:51:23] Energy consumed for all CPUs : 0.020729 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:51:23] 0.098979 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:51:38] Energy consumed for RAM : 0.005505 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:51:38] Energy consumed for all GPUs : 0.073102 kWh. Total GPU Power : 79.57574105180532 W\n",
      "[codecarbon INFO @ 18:51:38] Energy consumed for all CPUs : 0.020822 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:51:38] 0.099429 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:51:53] Energy consumed for RAM : 0.005530 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:51:53] Energy consumed for all GPUs : 0.073432 kWh. Total GPU Power : 79.20178049011254 W\n",
      "[codecarbon INFO @ 18:51:53] Energy consumed for all CPUs : 0.020916 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:51:53] 0.099878 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:52:08] Energy consumed for RAM : 0.005555 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:52:08] Energy consumed for all GPUs : 0.073761 kWh. Total GPU Power : 79.12897237968295 W\n",
      "[codecarbon INFO @ 18:52:08] Energy consumed for all CPUs : 0.021010 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:52:08] 0.100326 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:52:08] 0.007111 g.CO2eq/s mean an estimation of 224.2568695620875 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:52:23] Energy consumed for RAM : 0.005579 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:52:23] Energy consumed for all GPUs : 0.074091 kWh. Total GPU Power : 79.19889301842444 W\n",
      "[codecarbon INFO @ 18:52:23] Energy consumed for all CPUs : 0.021104 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:52:23] 0.100775 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:52:38] Energy consumed for RAM : 0.005604 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:52:38] Energy consumed for all GPUs : 0.074423 kWh. Total GPU Power : 79.5431241031922 W\n",
      "[codecarbon INFO @ 18:52:38] Energy consumed for all CPUs : 0.021197 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:52:38] 0.101225 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:52:53] Energy consumed for RAM : 0.005629 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:52:53] Energy consumed for all GPUs : 0.074753 kWh. Total GPU Power : 79.1492665724194 W\n",
      "[codecarbon INFO @ 18:52:53] Energy consumed for all CPUs : 0.021291 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:52:53] 0.101673 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:53:08] Energy consumed for RAM : 0.005654 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:53:08] Energy consumed for all GPUs : 0.075085 kWh. Total GPU Power : 79.7446264765526 W\n",
      "[codecarbon INFO @ 18:53:08] Energy consumed for all CPUs : 0.021385 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:53:08] 0.102124 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:53:23] Energy consumed for RAM : 0.005679 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:53:23] Energy consumed for all GPUs : 0.075415 kWh. Total GPU Power : 79.24119734336718 W\n",
      "[codecarbon INFO @ 18:53:23] Energy consumed for all CPUs : 0.021479 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:53:23] 0.102573 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:53:38] Energy consumed for RAM : 0.005703 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:53:38] Energy consumed for all GPUs : 0.075745 kWh. Total GPU Power : 79.04938134586699 W\n",
      "[codecarbon INFO @ 18:53:38] Energy consumed for all CPUs : 0.021573 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:53:38] 0.103021 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:53:53] Energy consumed for RAM : 0.005728 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:53:53] Energy consumed for all GPUs : 0.076077 kWh. Total GPU Power : 79.62745789200869 W\n",
      "[codecarbon INFO @ 18:53:53] Energy consumed for all CPUs : 0.021666 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:53:53] 0.103471 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:54:08] Energy consumed for RAM : 0.005753 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:54:08] Energy consumed for all GPUs : 0.076407 kWh. Total GPU Power : 79.25717963593517 W\n",
      "[codecarbon INFO @ 18:54:08] Energy consumed for all CPUs : 0.021760 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:54:08] 0.103921 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:54:08] 0.007113 g.CO2eq/s mean an estimation of 224.32362738713132 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:54:23] Energy consumed for RAM : 0.005778 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:54:23] Energy consumed for all GPUs : 0.076737 kWh. Total GPU Power : 79.16141297870148 W\n",
      "[codecarbon INFO @ 18:54:23] Energy consumed for all CPUs : 0.021854 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:54:23] 0.104369 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:54:38] Energy consumed for RAM : 0.005803 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:54:38] Energy consumed for all GPUs : 0.077069 kWh. Total GPU Power : 79.6608318446059 W\n",
      "[codecarbon INFO @ 18:54:38] Energy consumed for all CPUs : 0.021948 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:54:38] 0.104820 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:54:53] Energy consumed for RAM : 0.005827 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:54:53] Energy consumed for all GPUs : 0.077399 kWh. Total GPU Power : 79.2414931776783 W\n",
      "[codecarbon INFO @ 18:54:53] Energy consumed for all CPUs : 0.022042 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:54:53] 0.105268 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batch size = 8\n",
      "[codecarbon INFO @ 18:55:08] Energy consumed for RAM : 0.005852 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:55:08] Energy consumed for all GPUs : 0.077729 kWh. Total GPU Power : 79.1970620791566 W\n",
      "[codecarbon INFO @ 18:55:08] Energy consumed for all CPUs : 0.022135 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:55:08] 0.105717 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:55:23] Energy consumed for RAM : 0.005877 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:55:23] Energy consumed for all GPUs : 0.078059 kWh. Total GPU Power : 79.10621752844713 W\n",
      "[codecarbon INFO @ 18:55:23] Energy consumed for all CPUs : 0.022229 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:55:23] 0.106165 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:55:38] Energy consumed for RAM : 0.005902 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:55:38] Energy consumed for all GPUs : 0.078391 kWh. Total GPU Power : 79.65023376571449 W\n",
      "[codecarbon INFO @ 18:55:38] Energy consumed for all CPUs : 0.022323 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:55:38] 0.106615 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Biased     0.8168    0.8992    0.8560      1805\n",
      "      Biased     0.8810    0.7874    0.8316      1712\n",
      "\n",
      "    accuracy                         0.8448      3517\n",
      "   macro avg     0.8489    0.8433    0.8438      3517\n",
      "weighted avg     0.8481    0.8448    0.8441      3517\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-2635\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-2635\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-2635\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-2635\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-2635\\special_tokens_map.json\n",
      "Deleting older checkpoint [D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-2196] due to args.save_total_limit\n",
      "[codecarbon INFO @ 18:55:53] Energy consumed for RAM : 0.005927 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:55:53] Energy consumed for all GPUs : 0.078719 kWh. Total GPU Power : 78.71345881333265 W\n",
      "[codecarbon INFO @ 18:55:53] Energy consumed for all CPUs : 0.022417 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:55:53] 0.107062 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:56:08] Energy consumed for RAM : 0.005951 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:56:08] Energy consumed for all GPUs : 0.079051 kWh. Total GPU Power : 79.69596796365776 W\n",
      "[codecarbon INFO @ 18:56:08] Energy consumed for all CPUs : 0.022510 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:56:08] 0.107513 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:56:08] 0.007110 g.CO2eq/s mean an estimation of 224.21521502368898 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:56:23] Energy consumed for RAM : 0.005976 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:56:23] Energy consumed for all GPUs : 0.079381 kWh. Total GPU Power : 79.10288474745539 W\n",
      "[codecarbon INFO @ 18:56:23] Energy consumed for all CPUs : 0.022604 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:56:23] 0.107961 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:56:38] Energy consumed for RAM : 0.006001 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:56:38] Energy consumed for all GPUs : 0.079711 kWh. Total GPU Power : 79.12851315828931 W\n",
      "[codecarbon INFO @ 18:56:38] Energy consumed for all CPUs : 0.022698 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:56:38] 0.108410 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:56:53] Energy consumed for RAM : 0.006026 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:56:53] Energy consumed for all GPUs : 0.080042 kWh. Total GPU Power : 79.5817733386914 W\n",
      "[codecarbon INFO @ 18:56:53] Energy consumed for all CPUs : 0.022792 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:56:53] 0.108860 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:57:08] Energy consumed for RAM : 0.006051 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:57:08] Energy consumed for all GPUs : 0.080373 kWh. Total GPU Power : 79.11919141063046 W\n",
      "[codecarbon INFO @ 18:57:08] Energy consumed for all CPUs : 0.022886 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:57:08] 0.109309 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:57:23] Energy consumed for RAM : 0.006075 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:57:23] Energy consumed for all GPUs : 0.080705 kWh. Total GPU Power : 79.76521491838272 W\n",
      "[codecarbon INFO @ 18:57:23] Energy consumed for all CPUs : 0.022979 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:57:23] 0.109760 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:57:38] Energy consumed for RAM : 0.006100 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:57:38] Energy consumed for all GPUs : 0.081035 kWh. Total GPU Power : 79.23144893206222 W\n",
      "[codecarbon INFO @ 18:57:38] Energy consumed for all CPUs : 0.023073 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:57:38] 0.110208 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:57:53] Energy consumed for RAM : 0.006125 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:57:53] Energy consumed for all GPUs : 0.081364 kWh. Total GPU Power : 79.07069839709119 W\n",
      "[codecarbon INFO @ 18:57:53] Energy consumed for all CPUs : 0.023167 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:57:53] 0.110656 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:58:08] Energy consumed for RAM : 0.006150 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:58:08] Energy consumed for all GPUs : 0.081697 kWh. Total GPU Power : 79.69430997241537 W\n",
      "[codecarbon INFO @ 18:58:08] Energy consumed for all CPUs : 0.023261 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:58:08] 0.111107 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:58:08] 0.007112 g.CO2eq/s mean an estimation of 224.29116728796635 kg.CO2eq/year\n",
      "[codecarbon INFO @ 18:58:23] Energy consumed for RAM : 0.006175 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:58:23] Energy consumed for all GPUs : 0.082026 kWh. Total GPU Power : 79.03184230628428 W\n",
      "[codecarbon INFO @ 18:58:23] Energy consumed for all CPUs : 0.023355 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:58:23] 0.111555 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:58:38] Energy consumed for RAM : 0.006199 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:58:38] Energy consumed for all GPUs : 0.082355 kWh. Total GPU Power : 78.95886759530036 W\n",
      "[codecarbon INFO @ 18:58:38] Energy consumed for all CPUs : 0.023448 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:58:38] 0.112003 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:58:53] Energy consumed for RAM : 0.006224 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:58:53] Energy consumed for all GPUs : 0.082686 kWh. Total GPU Power : 79.45608377105253 W\n",
      "[codecarbon INFO @ 18:58:53] Energy consumed for all CPUs : 0.023542 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:58:53] 0.112452 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:59:08] Energy consumed for RAM : 0.006249 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:59:08] Energy consumed for all GPUs : 0.083015 kWh. Total GPU Power : 79.0002778717509 W\n",
      "[codecarbon INFO @ 18:59:08] Energy consumed for all CPUs : 0.023636 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:59:08] 0.112900 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:59:23] Energy consumed for RAM : 0.006274 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:59:23] Energy consumed for all GPUs : 0.083345 kWh. Total GPU Power : 79.03045192064259 W\n",
      "[codecarbon INFO @ 18:59:23] Energy consumed for all CPUs : 0.023730 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:59:23] 0.113348 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:59:38] Energy consumed for RAM : 0.006299 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:59:38] Energy consumed for all GPUs : 0.083677 kWh. Total GPU Power : 79.664010035826 W\n",
      "[codecarbon INFO @ 18:59:38] Energy consumed for all CPUs : 0.023823 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:59:38] 0.113799 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 18:59:53] Energy consumed for RAM : 0.006323 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 18:59:53] Energy consumed for all GPUs : 0.084007 kWh. Total GPU Power : 79.21996296380468 W\n",
      "[codecarbon INFO @ 18:59:53] Energy consumed for all CPUs : 0.023917 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 18:59:53] 0.114247 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:00:08] Energy consumed for RAM : 0.006348 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:00:08] Energy consumed for all GPUs : 0.084336 kWh. Total GPU Power : 79.06971523042526 W\n",
      "[codecarbon INFO @ 19:00:08] Energy consumed for all CPUs : 0.024011 kWh. Total CPU Power : 22.5 W\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 19:00:08] 0.114696 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:00:08] 0.007102 g.CO2eq/s mean an estimation of 223.95714947041628 kg.CO2eq/year\n",
      "[codecarbon INFO @ 19:00:23] Energy consumed for RAM : 0.006373 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:00:23] Energy consumed for all GPUs : 0.084669 kWh. Total GPU Power : 79.77605177721999 W\n",
      "[codecarbon INFO @ 19:00:23] Energy consumed for all CPUs : 0.024105 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:00:23] 0.115147 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:00:38] Energy consumed for RAM : 0.006398 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:00:38] Energy consumed for all GPUs : 0.084999 kWh. Total GPU Power : 79.21157623649448 W\n",
      "[codecarbon INFO @ 19:00:38] Energy consumed for all CPUs : 0.024199 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:00:38] 0.115595 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:00:53] Energy consumed for RAM : 0.006423 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:00:53] Energy consumed for all GPUs : 0.085331 kWh. Total GPU Power : 79.59193965102648 W\n",
      "[codecarbon INFO @ 19:00:53] Energy consumed for all CPUs : 0.024292 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:00:53] 0.116046 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:01:08] Energy consumed for RAM : 0.006447 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:01:08] Energy consumed for all GPUs : 0.085660 kWh. Total GPU Power : 79.09936706283467 W\n",
      "[codecarbon INFO @ 19:01:08] Energy consumed for all CPUs : 0.024386 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:01:08] 0.116494 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:01:23] Energy consumed for RAM : 0.006472 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:01:23] Energy consumed for all GPUs : 0.085991 kWh. Total GPU Power : 79.2088736174469 W\n",
      "[codecarbon INFO @ 19:01:23] Energy consumed for all CPUs : 0.024480 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:01:23] 0.116943 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:01:38] Energy consumed for RAM : 0.006497 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:01:38] Energy consumed for all GPUs : 0.086323 kWh. Total GPU Power : 79.6786860886384 W\n",
      "[codecarbon INFO @ 19:01:38] Energy consumed for all CPUs : 0.024574 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:01:38] 0.117393 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:01:53] Energy consumed for RAM : 0.006522 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:01:53] Energy consumed for all GPUs : 0.086652 kWh. Total GPU Power : 79.04215913209194 W\n",
      "[codecarbon INFO @ 19:01:53] Energy consumed for all CPUs : 0.024668 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:01:53] 0.117842 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:02:08] Energy consumed for RAM : 0.006547 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:02:08] Energy consumed for all GPUs : 0.086982 kWh. Total GPU Power : 79.11070524934397 W\n",
      "[codecarbon INFO @ 19:02:08] Energy consumed for all CPUs : 0.024761 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:02:08] 0.118290 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:02:08] 0.007112 g.CO2eq/s mean an estimation of 224.29766375374453 kg.CO2eq/year\n",
      "[codecarbon INFO @ 19:02:23] Energy consumed for RAM : 0.006571 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:02:23] Energy consumed for all GPUs : 0.087314 kWh. Total GPU Power : 79.63443651230422 W\n",
      "[codecarbon INFO @ 19:02:23] Energy consumed for all CPUs : 0.024855 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:02:23] 0.118740 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:02:38] Energy consumed for RAM : 0.006596 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:02:38] Energy consumed for all GPUs : 0.087644 kWh. Total GPU Power : 79.26266138972603 W\n",
      "[codecarbon INFO @ 19:02:38] Energy consumed for all CPUs : 0.024949 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:02:38] 0.119189 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:02:53] Energy consumed for RAM : 0.006621 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:02:53] Energy consumed for all GPUs : 0.087976 kWh. Total GPU Power : 79.55015111700098 W\n",
      "[codecarbon INFO @ 19:02:53] Energy consumed for all CPUs : 0.025043 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:02:53] 0.119640 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:03:08] Energy consumed for RAM : 0.006646 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:03:08] Energy consumed for all GPUs : 0.088306 kWh. Total GPU Power : 79.25844272773259 W\n",
      "[codecarbon INFO @ 19:03:08] Energy consumed for all CPUs : 0.025137 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:03:08] 0.120088 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:03:23] Energy consumed for RAM : 0.006671 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:03:23] Energy consumed for all GPUs : 0.088636 kWh. Total GPU Power : 79.16730879644328 W\n",
      "[codecarbon INFO @ 19:03:23] Energy consumed for all CPUs : 0.025230 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:03:23] 0.120537 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:03:38] Energy consumed for RAM : 0.006695 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:03:38] Energy consumed for all GPUs : 0.088968 kWh. Total GPU Power : 79.73159384998397 W\n",
      "[codecarbon INFO @ 19:03:38] Energy consumed for all CPUs : 0.025324 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:03:38] 0.120988 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:03:53] Energy consumed for RAM : 0.006720 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:03:53] Energy consumed for all GPUs : 0.089298 kWh. Total GPU Power : 79.13786777997278 W\n",
      "[codecarbon INFO @ 19:03:53] Energy consumed for all CPUs : 0.025418 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:03:53] 0.121436 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:04:08] Energy consumed for RAM : 0.006745 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:04:08] Energy consumed for all GPUs : 0.089627 kWh. Total GPU Power : 79.06524787738576 W\n",
      "[codecarbon INFO @ 19:04:08] Energy consumed for all CPUs : 0.025512 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:04:08] 0.121884 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:04:08] 0.007113 g.CO2eq/s mean an estimation of 224.30250927282637 kg.CO2eq/year\n",
      "[codecarbon INFO @ 19:04:23] Energy consumed for RAM : 0.006770 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:04:23] Energy consumed for all GPUs : 0.089959 kWh. Total GPU Power : 79.65785865375233 W\n",
      "[codecarbon INFO @ 19:04:23] Energy consumed for all CPUs : 0.025605 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:04:23] 0.122334 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:04:38] Energy consumed for RAM : 0.006794 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:04:38] Energy consumed for all GPUs : 0.090289 kWh. Total GPU Power : 79.08999155629925 W\n",
      "[codecarbon INFO @ 19:04:38] Energy consumed for all CPUs : 0.025699 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:04:38] 0.122782 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3517\n",
      "  Batch size = 8\n",
      "[codecarbon INFO @ 19:04:53] Energy consumed for RAM : 0.006819 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:04:53] Energy consumed for all GPUs : 0.090618 kWh. Total GPU Power : 78.89681120544249 W\n",
      "[codecarbon INFO @ 19:04:53] Energy consumed for all CPUs : 0.025793 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:04:53] 0.123230 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 19:05:08] Energy consumed for RAM : 0.006844 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:05:08] Energy consumed for all GPUs : 0.090948 kWh. Total GPU Power : 79.24345690037894 W\n",
      "[codecarbon INFO @ 19:05:08] Energy consumed for all CPUs : 0.025887 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:05:08] 0.123679 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:05:23] Energy consumed for RAM : 0.006869 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:05:23] Energy consumed for all GPUs : 0.091277 kWh. Total GPU Power : 78.8805393114745 W\n",
      "[codecarbon INFO @ 19:05:23] Energy consumed for all CPUs : 0.025981 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:05:23] 0.124127 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:05:38] Energy consumed for RAM : 0.006894 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:05:38] Energy consumed for all GPUs : 0.091605 kWh. Total GPU Power : 78.72684806082152 W\n",
      "[codecarbon INFO @ 19:05:38] Energy consumed for all CPUs : 0.026074 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:05:38] 0.124573 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Biased     0.8672    0.8571    0.8621      1805\n",
      "      Biased     0.8511    0.8616    0.8563      1712\n",
      "\n",
      "    accuracy                         0.8593      3517\n",
      "   macro avg     0.8591    0.8593    0.8592      3517\n",
      "weighted avg     0.8594    0.8593    0.8593      3517\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-3074\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-3074\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-3074\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-3074\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-3074\\special_tokens_map.json\n",
      "Deleting older checkpoint [D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-3074] due to args.save_total_limit\n",
      "[codecarbon INFO @ 19:05:53] Energy consumed for RAM : 0.006918 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:05:53] Energy consumed for all GPUs : 0.091935 kWh. Total GPU Power : 79.05080967336409 W\n",
      "[codecarbon INFO @ 19:05:53] Energy consumed for all CPUs : 0.026168 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:05:53] 0.125021 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:06:08] Energy consumed for RAM : 0.006943 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:06:08] Energy consumed for all GPUs : 0.092264 kWh. Total GPU Power : 79.10778864640915 W\n",
      "[codecarbon INFO @ 19:06:08] Energy consumed for all CPUs : 0.026262 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:06:08] 0.125469 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:06:08] 0.007095 g.CO2eq/s mean an estimation of 223.75288006926635 kg.CO2eq/year\n",
      "[codecarbon INFO @ 19:06:23] Energy consumed for RAM : 0.006968 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:06:23] Energy consumed for all GPUs : 0.092593 kWh. Total GPU Power : 78.99788080908007 W\n",
      "[codecarbon INFO @ 19:06:23] Energy consumed for all CPUs : 0.026356 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:06:23] 0.125917 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:06:38] Energy consumed for RAM : 0.006993 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:06:38] Energy consumed for all GPUs : 0.092926 kWh. Total GPU Power : 79.73976692083777 W\n",
      "[codecarbon INFO @ 19:06:38] Energy consumed for all CPUs : 0.026450 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:06:38] 0.126368 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:06:53] Energy consumed for RAM : 0.007018 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:06:53] Energy consumed for all GPUs : 0.093256 kWh. Total GPU Power : 79.20330706200885 W\n",
      "[codecarbon INFO @ 19:06:53] Energy consumed for all CPUs : 0.026543 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:06:53] 0.126817 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:07:08] Energy consumed for RAM : 0.007042 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:07:08] Energy consumed for all GPUs : 0.093586 kWh. Total GPU Power : 79.15801204567745 W\n",
      "[codecarbon INFO @ 19:07:08] Energy consumed for all CPUs : 0.026637 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:07:08] 0.127265 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:07:23] Energy consumed for RAM : 0.007067 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:07:23] Energy consumed for all GPUs : 0.093918 kWh. Total GPU Power : 79.6319416051097 W\n",
      "[codecarbon INFO @ 19:07:23] Energy consumed for all CPUs : 0.026731 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:07:23] 0.127716 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:07:38] Energy consumed for RAM : 0.007092 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:07:38] Energy consumed for all GPUs : 0.094247 kWh. Total GPU Power : 79.17210034787179 W\n",
      "[codecarbon INFO @ 19:07:38] Energy consumed for all CPUs : 0.026825 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:07:38] 0.128164 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:07:53] Energy consumed for RAM : 0.007117 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:07:53] Energy consumed for all GPUs : 0.094577 kWh. Total GPU Power : 79.13786605802623 W\n",
      "[codecarbon INFO @ 19:07:53] Energy consumed for all CPUs : 0.026918 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:07:53] 0.128613 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:08:08] Energy consumed for RAM : 0.007142 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:08:08] Energy consumed for all GPUs : 0.094909 kWh. Total GPU Power : 79.70594086034222 W\n",
      "[codecarbon INFO @ 19:08:08] Energy consumed for all CPUs : 0.027012 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:08:08] 0.129063 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:08:08] 0.007113 g.CO2eq/s mean an estimation of 224.30511869003675 kg.CO2eq/year\n",
      "[codecarbon INFO @ 19:08:23] Energy consumed for RAM : 0.007166 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:08:23] Energy consumed for all GPUs : 0.095240 kWh. Total GPU Power : 79.29001122694183 W\n",
      "[codecarbon INFO @ 19:08:23] Energy consumed for all CPUs : 0.027106 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:08:23] 0.129512 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:08:38] Energy consumed for RAM : 0.007191 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:08:38] Energy consumed for all GPUs : 0.095570 kWh. Total GPU Power : 79.11284488077783 W\n",
      "[codecarbon INFO @ 19:08:38] Energy consumed for all CPUs : 0.027200 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:08:38] 0.129960 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:08:53] Energy consumed for RAM : 0.007216 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:08:53] Energy consumed for all GPUs : 0.095902 kWh. Total GPU Power : 79.8150376971808 W\n",
      "[codecarbon INFO @ 19:08:53] Energy consumed for all CPUs : 0.027293 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:08:53] 0.130411 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:09:08] Energy consumed for RAM : 0.007241 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:09:08] Energy consumed for all GPUs : 0.096231 kWh. Total GPU Power : 79.08469987078885 W\n",
      "[codecarbon INFO @ 19:09:08] Energy consumed for all CPUs : 0.027387 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:09:08] 0.130859 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:09:23] Energy consumed for RAM : 0.007266 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:09:23] Energy consumed for all GPUs : 0.096561 kWh. Total GPU Power : 79.19416599012622 W\n",
      "[codecarbon INFO @ 19:09:23] Energy consumed for all CPUs : 0.027481 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:09:23] 0.131308 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:09:38] Energy consumed for RAM : 0.007290 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:09:38] Energy consumed for all GPUs : 0.096891 kWh. Total GPU Power : 79.14501241781026 W\n",
      "[codecarbon INFO @ 19:09:38] Energy consumed for all CPUs : 0.027575 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:09:38] 0.131756 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:09:53] Energy consumed for RAM : 0.007315 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:09:53] Energy consumed for all GPUs : 0.097224 kWh. Total GPU Power : 79.75741322985995 W\n",
      "[codecarbon INFO @ 19:09:53] Energy consumed for all CPUs : 0.027669 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:09:53] 0.132207 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:10:08] Energy consumed for RAM : 0.007340 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:10:08] Energy consumed for all GPUs : 0.097553 kWh. Total GPU Power : 79.02384024500911 W\n",
      "[codecarbon INFO @ 19:10:08] Energy consumed for all CPUs : 0.027762 kWh. Total CPU Power : 22.5 W\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 19:10:08] 0.132655 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:10:08] 0.007110 g.CO2eq/s mean an estimation of 224.22236493665895 kg.CO2eq/year\n",
      "[codecarbon INFO @ 19:10:23] Energy consumed for RAM : 0.007365 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:10:23] Energy consumed for all GPUs : 0.097885 kWh. Total GPU Power : 79.61743259047702 W\n",
      "[codecarbon INFO @ 19:10:23] Energy consumed for all CPUs : 0.027856 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:10:23] 0.133106 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:10:38] Energy consumed for RAM : 0.007390 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:10:38] Energy consumed for all GPUs : 0.098216 kWh. Total GPU Power : 79.32728208042415 W\n",
      "[codecarbon INFO @ 19:10:38] Energy consumed for all CPUs : 0.027950 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:10:38] 0.133555 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:10:53] Energy consumed for RAM : 0.007414 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:10:53] Energy consumed for all GPUs : 0.098545 kWh. Total GPU Power : 79.13951903195975 W\n",
      "[codecarbon INFO @ 19:10:53] Energy consumed for all CPUs : 0.028044 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:10:53] 0.134004 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:11:08] Energy consumed for RAM : 0.007439 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:11:08] Energy consumed for all GPUs : 0.098875 kWh. Total GPU Power : 79.20480165126936 W\n",
      "[codecarbon INFO @ 19:11:08] Energy consumed for all CPUs : 0.028138 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:11:08] 0.134452 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:11:23] Energy consumed for RAM : 0.007464 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:11:23] Energy consumed for all GPUs : 0.099207 kWh. Total GPU Power : 79.58078739762011 W\n",
      "[codecarbon INFO @ 19:11:23] Energy consumed for all CPUs : 0.028231 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:11:23] 0.134902 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:11:38] Energy consumed for RAM : 0.007489 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:11:38] Energy consumed for all GPUs : 0.099537 kWh. Total GPU Power : 79.20956158231736 W\n",
      "[codecarbon INFO @ 19:11:38] Energy consumed for all CPUs : 0.028325 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:11:38] 0.135351 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:11:53] Energy consumed for RAM : 0.007513 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:11:53] Energy consumed for all GPUs : 0.099867 kWh. Total GPU Power : 79.1700252226639 W\n",
      "[codecarbon INFO @ 19:11:53] Energy consumed for all CPUs : 0.028419 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:11:53] 0.135799 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:12:08] Energy consumed for RAM : 0.007538 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:12:08] Energy consumed for all GPUs : 0.100199 kWh. Total GPU Power : 79.76012352929523 W\n",
      "[codecarbon INFO @ 19:12:08] Energy consumed for all CPUs : 0.028513 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:12:08] 0.136250 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:12:08] 0.007115 g.CO2eq/s mean an estimation of 224.3746407194039 kg.CO2eq/year\n",
      "[codecarbon INFO @ 19:12:23] Energy consumed for RAM : 0.007563 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:12:23] Energy consumed for all GPUs : 0.100529 kWh. Total GPU Power : 79.13404027144009 W\n",
      "[codecarbon INFO @ 19:12:23] Energy consumed for all CPUs : 0.028606 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:12:23] 0.136698 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:12:38] Energy consumed for RAM : 0.007588 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:12:38] Energy consumed for all GPUs : 0.100859 kWh. Total GPU Power : 79.21305407343166 W\n",
      "[codecarbon INFO @ 19:12:38] Energy consumed for all CPUs : 0.028700 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:12:38] 0.137147 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:12:53] Energy consumed for RAM : 0.007613 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:12:53] Energy consumed for all GPUs : 0.101191 kWh. Total GPU Power : 79.49845965743043 W\n",
      "[codecarbon INFO @ 19:12:53] Energy consumed for all CPUs : 0.028794 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:12:53] 0.137597 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:13:08] Energy consumed for RAM : 0.007637 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:13:08] Energy consumed for all GPUs : 0.101521 kWh. Total GPU Power : 79.19938657478919 W\n",
      "[codecarbon INFO @ 19:13:08] Energy consumed for all CPUs : 0.028888 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:13:08] 0.138046 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:13:23] Energy consumed for RAM : 0.007662 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:13:23] Energy consumed for all GPUs : 0.101851 kWh. Total GPU Power : 79.26904894735522 W\n",
      "[codecarbon INFO @ 19:13:23] Energy consumed for all CPUs : 0.028982 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:13:23] 0.138495 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:13:38] Energy consumed for RAM : 0.007687 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:13:38] Energy consumed for all GPUs : 0.102181 kWh. Total GPU Power : 79.22879104325429 W\n",
      "[codecarbon INFO @ 19:13:38] Energy consumed for all CPUs : 0.029075 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:13:38] 0.138943 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:13:53] Energy consumed for RAM : 0.007712 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:13:53] Energy consumed for all GPUs : 0.102513 kWh. Total GPU Power : 79.65685257579682 W\n",
      "[codecarbon INFO @ 19:13:53] Energy consumed for all CPUs : 0.029169 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:13:53] 0.139394 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:14:08] Energy consumed for RAM : 0.007737 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:14:08] Energy consumed for all GPUs : 0.102843 kWh. Total GPU Power : 79.16010233899812 W\n",
      "[codecarbon INFO @ 19:14:08] Energy consumed for all CPUs : 0.029263 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:14:08] 0.139843 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:14:08] 0.007109 g.CO2eq/s mean an estimation of 224.19771763745086 kg.CO2eq/year\n",
      "[codecarbon INFO @ 19:14:23] Energy consumed for RAM : 0.007761 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:14:23] Energy consumed for all GPUs : 0.103175 kWh. Total GPU Power : 79.55770546322103 W\n",
      "[codecarbon INFO @ 19:14:23] Energy consumed for all CPUs : 0.029357 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:14:23] 0.140293 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:14:38] Energy consumed for RAM : 0.007786 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:14:38] Energy consumed for all GPUs : 0.103505 kWh. Total GPU Power : 79.23899320862503 W\n",
      "[codecarbon INFO @ 19:14:38] Energy consumed for all CPUs : 0.029450 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:14:38] 0.140741 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-3512\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-3512\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-3512\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-3512\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-3512\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-3512] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3517\n",
      "  Batch size = 8\n",
      "[codecarbon INFO @ 19:14:53] Energy consumed for RAM : 0.007811 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:14:53] Energy consumed for all GPUs : 0.103833 kWh. Total GPU Power : 78.70989488034986 W\n",
      "[codecarbon INFO @ 19:14:53] Energy consumed for all CPUs : 0.029544 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:14:53] 0.141188 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:15:08] Energy consumed for RAM : 0.007836 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:15:08] Energy consumed for all GPUs : 0.104162 kWh. Total GPU Power : 78.89404555693847 W\n",
      "[codecarbon INFO @ 19:15:08] Energy consumed for all CPUs : 0.029638 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:15:08] 0.141636 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 19:15:23] Energy consumed for RAM : 0.007861 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:15:23] Energy consumed for all GPUs : 0.104493 kWh. Total GPU Power : 79.44686040323475 W\n",
      "[codecarbon INFO @ 19:15:23] Energy consumed for all CPUs : 0.029732 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:15:23] 0.142086 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Biased     0.9084    0.8022    0.8520      1805\n",
      "      Biased     0.8144    0.9147    0.8616      1712\n",
      "\n",
      "    accuracy                         0.8570      3517\n",
      "   macro avg     0.8614    0.8585    0.8568      3517\n",
      "weighted avg     0.8626    0.8570    0.8567      3517\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-3512\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-3512\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-3512\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-3512\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-3512\\special_tokens_map.json\n",
      "Deleting older checkpoint [D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-3512] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-2635 (score: 0.3479927182197571).\n",
      "Deleting older checkpoint [D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\checkpoint-3951] due to args.save_total_limit\n",
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\special_tokens_map.json\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\\special_tokens_map.json\n",
      "[codecarbon INFO @ 19:15:30] Energy consumed for RAM : 0.007872 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 19:15:30] Energy consumed for all GPUs : 0.104645 kWh. Total GPU Power : 77.66858157660282 W\n",
      "[codecarbon INFO @ 19:15:30] Energy consumed for all CPUs : 0.029776 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 19:15:30] 0.142293 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Saving model...\n",
      "Carbon dioxide emission: 33.807206 g\n"
     ]
    }
   ],
   "source": [
    "albert_model_dir = train_model(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    model_path='albert/albert-base-v2',\n",
    "    batch_size=TRAINING_CONFIG['batch_size'],\n",
    "    epoch=TRAINING_CONFIG['epochs'],\n",
    "    learning_rate=TRAINING_CONFIG['learning_rate'],\n",
    "    gradient_accumulation_steps=TRAINING_CONFIG['gradient_accumulation_steps'],\n",
    "    dataset_name='job_descriptions',\n",
    "    seed=TRAINING_CONFIG['seed'],\n",
    "    save_model=TRAINING_CONFIG['save_model'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration for DistilBERT\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    'batch_size': 8,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'epochs':4 ,\n",
    "    'learning_rate': 2e-5,\n",
    "    'seed': 42,\n",
    "    'save_model': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 15:06:31] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 15:06:31] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 15:06:31] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 15:06:31] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 15:06:31] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 15:06:31] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 15:06:32] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 15:06:32] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 15:06:33]   Platform system: Windows-10-10.0.26100-SP0\n",
      "[codecarbon INFO @ 15:06:33]   Python version: 3.10.19\n",
      "[codecarbon INFO @ 15:06:33]   CodeCarbon version: 2.8.0\n",
      "[codecarbon INFO @ 15:06:33]   Available RAM : 15.870 GB\n",
      "[codecarbon INFO @ 15:06:33]   CPU count: 16\n",
      "[codecarbon INFO @ 15:06:33]   CPU model: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 15:06:33]   GPU count: 1\n",
      "[codecarbon INFO @ 15:06:33]   GPU model: 1 x NVIDIA GeForce RTX 2060\n",
      "[codecarbon INFO @ 15:06:36] Saving emissions data to file D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\notebooks\\emissions.csv\n",
      "loading configuration file config.json from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert/distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Loading Model: distilbert/distilbert-base-uncased\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert/distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert/distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased\\snapshots\\12040accade4e8a0f71eabdb258fecc2e7e948be\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert/distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa033b4af734657881684c6bd75b05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14065 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5742d5a212054ceab792784d3a6d4eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14065 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing validation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67a42879d9d44b6a20daff53bae0649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3517 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b796904de57d4ea6b29f368030cbb318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3517 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\hearts\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tokenization complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\b'h'w'l\\AppData\\Local\\Temp\\ipykernel_46708\\1808897401.py:165: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Using auto half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 14,065\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1,756\n",
      "  Number of trainable parameters = 66,955,010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING CONFIGURATION\n",
      "============================================================\n",
      "Model: distilbert/distilbert-base-uncased\n",
      "Number of Epochs: 4\n",
      "Learning Rate: 2.00e-05\n",
      "Batch Size: 8\n",
      "Gradient Accumulation Steps: 4\n",
      "Effective Batch Size: 32\n",
      "Training Samples: 14,065\n",
      "Validation Samples: 3,517\n",
      "Device: cuda\n",
      "Mixed Precision (FP16): True\n",
      "============================================================\n",
      "\n",
      "   Starting training...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1756' max='1756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1756/1756 14:59, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.659400</td>\n",
       "      <td>0.643067</td>\n",
       "      <td>0.638658</td>\n",
       "      <td>0.637404</td>\n",
       "      <td>0.637193</td>\n",
       "      <td>0.637404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.609600</td>\n",
       "      <td>0.629803</td>\n",
       "      <td>0.651153</td>\n",
       "      <td>0.648214</td>\n",
       "      <td>0.647489</td>\n",
       "      <td>0.648214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.536100</td>\n",
       "      <td>0.558736</td>\n",
       "      <td>0.701463</td>\n",
       "      <td>0.699946</td>\n",
       "      <td>0.698295</td>\n",
       "      <td>0.699946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.442600</td>\n",
       "      <td>0.524413</td>\n",
       "      <td>0.739982</td>\n",
       "      <td>0.739742</td>\n",
       "      <td>0.739822</td>\n",
       "      <td>0.739742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 15:06:51] Energy consumed for RAM : 0.000025 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:06:51] Energy consumed for all GPUs : 0.000086 kWh. Total GPU Power : 20.63653425944073 W\n",
      "[codecarbon INFO @ 15:06:51] Energy consumed for all CPUs : 0.000094 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:06:51] 0.000205 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:07:06] Energy consumed for RAM : 0.000050 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:07:06] Energy consumed for all GPUs : 0.000415 kWh. Total GPU Power : 78.8207109417539 W\n",
      "[codecarbon INFO @ 15:07:06] Energy consumed for all CPUs : 0.000188 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:07:06] 0.000652 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:07:21] Energy consumed for RAM : 0.000074 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:07:21] Energy consumed for all GPUs : 0.000744 kWh. Total GPU Power : 78.9180483814446 W\n",
      "[codecarbon INFO @ 15:07:21] Energy consumed for all CPUs : 0.000281 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:07:21] 0.001099 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:07:36] Energy consumed for RAM : 0.000099 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:07:36] Energy consumed for all GPUs : 0.001075 kWh. Total GPU Power : 79.43998214118923 W\n",
      "[codecarbon INFO @ 15:07:36] Energy consumed for all CPUs : 0.000375 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:07:36] 0.001549 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:07:51] Energy consumed for RAM : 0.000124 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:07:51] Energy consumed for all GPUs : 0.001404 kWh. Total GPU Power : 79.00115003508519 W\n",
      "[codecarbon INFO @ 15:07:51] Energy consumed for all CPUs : 0.000469 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:07:51] 0.001997 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:08:06] Energy consumed for RAM : 0.000149 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:08:06] Energy consumed for all GPUs : 0.001733 kWh. Total GPU Power : 78.91413783210142 W\n",
      "[codecarbon INFO @ 15:08:06] Energy consumed for all CPUs : 0.000563 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:08:06] 0.002445 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:08:21] Energy consumed for RAM : 0.000174 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:08:21] Energy consumed for all GPUs : 0.002062 kWh. Total GPU Power : 78.9360625312697 W\n",
      "[codecarbon INFO @ 15:08:21] Energy consumed for all CPUs : 0.000657 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:08:21] 0.002892 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:08:36] Energy consumed for RAM : 0.000198 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:08:36] Energy consumed for all GPUs : 0.002394 kWh. Total GPU Power : 79.49414874687479 W\n",
      "[codecarbon INFO @ 15:08:36] Energy consumed for all CPUs : 0.000751 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:08:36] 0.003343 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:08:36] 0.006613 g.CO2eq/s mean an estimation of 208.53992469490225 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:08:51] Energy consumed for RAM : 0.000223 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:08:51] Energy consumed for all GPUs : 0.002723 kWh. Total GPU Power : 79.0589010609044 W\n",
      "[codecarbon INFO @ 15:08:51] Energy consumed for all CPUs : 0.000844 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:08:51] 0.003790 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:09:06] Energy consumed for RAM : 0.000248 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:09:06] Energy consumed for all GPUs : 0.003053 kWh. Total GPU Power : 79.04677997695339 W\n",
      "[codecarbon INFO @ 15:09:06] Energy consumed for all CPUs : 0.000938 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:09:06] 0.004239 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:09:21] Energy consumed for RAM : 0.000273 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:09:21] Energy consumed for all GPUs : 0.003381 kWh. Total GPU Power : 78.891589559749 W\n",
      "[codecarbon INFO @ 15:09:21] Energy consumed for all CPUs : 0.001032 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:09:21] 0.004686 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:09:36] Energy consumed for RAM : 0.000298 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:09:36] Energy consumed for all GPUs : 0.003710 kWh. Total GPU Power : 78.99570584765338 W\n",
      "[codecarbon INFO @ 15:09:36] Energy consumed for all CPUs : 0.001126 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:09:36] 0.005134 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:09:51] Energy consumed for RAM : 0.000322 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:09:51] Energy consumed for all GPUs : 0.004040 kWh. Total GPU Power : 79.06215431635465 W\n",
      "[codecarbon INFO @ 15:09:51] Energy consumed for all CPUs : 0.001219 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:09:51] 0.005582 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:10:06] Energy consumed for RAM : 0.000347 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:10:06] Energy consumed for all GPUs : 0.004371 kWh. Total GPU Power : 79.51355449275493 W\n",
      "[codecarbon INFO @ 15:10:06] Energy consumed for all CPUs : 0.001313 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:10:06] 0.006032 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3517\n",
      "  Batch size = 8\n",
      "[codecarbon INFO @ 15:10:21] Energy consumed for RAM : 0.000372 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:10:21] Energy consumed for all GPUs : 0.004700 kWh. Total GPU Power : 78.8199139074098 W\n",
      "[codecarbon INFO @ 15:10:21] Energy consumed for all CPUs : 0.001407 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:10:21] 0.006479 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Biased     0.6382    0.6831    0.6599      1805\n",
      "      Biased     0.6391    0.5917    0.6145      1712\n",
      "\n",
      "    accuracy                         0.6386      3517\n",
      "   macro avg     0.6387    0.6374    0.6372      3517\n",
      "weighted avg     0.6386    0.6386    0.6378      3517\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-439\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-439\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-439\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-439\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-439\\special_tokens_map.json\n",
      "[codecarbon INFO @ 15:10:36] Energy consumed for RAM : 0.000397 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:10:36] Energy consumed for all GPUs : 0.005020 kWh. Total GPU Power : 76.89091912544752 W\n",
      "[codecarbon INFO @ 15:10:36] Energy consumed for all CPUs : 0.001501 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:10:36] 0.006918 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:10:36] 0.007076 g.CO2eq/s mean an estimation of 223.1425646967988 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:10:51] Energy consumed for RAM : 0.000422 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:10:51] Energy consumed for all GPUs : 0.005350 kWh. Total GPU Power : 79.03614486838272 W\n",
      "[codecarbon INFO @ 15:10:51] Energy consumed for all CPUs : 0.001595 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:10:51] 0.007366 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:11:06] Energy consumed for RAM : 0.000446 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:11:06] Energy consumed for all GPUs : 0.005679 kWh. Total GPU Power : 79.00990076983194 W\n",
      "[codecarbon INFO @ 15:11:06] Energy consumed for all CPUs : 0.001688 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:11:06] 0.007814 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:11:21] Energy consumed for RAM : 0.000471 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:11:21] Energy consumed for all GPUs : 0.006008 kWh. Total GPU Power : 78.87098108957612 W\n",
      "[codecarbon INFO @ 15:11:21] Energy consumed for all CPUs : 0.001782 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:11:21] 0.008261 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:11:36] Energy consumed for RAM : 0.000496 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:11:36] Energy consumed for all GPUs : 0.006339 kWh. Total GPU Power : 79.45606731195828 W\n",
      "[codecarbon INFO @ 15:11:36] Energy consumed for all CPUs : 0.001876 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:11:36] 0.008711 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:11:51] Energy consumed for RAM : 0.000521 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:11:51] Energy consumed for all GPUs : 0.006668 kWh. Total GPU Power : 79.02375548367517 W\n",
      "[codecarbon INFO @ 15:11:51] Energy consumed for all CPUs : 0.001970 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:11:51] 0.009159 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:12:06] Energy consumed for RAM : 0.000546 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:12:06] Energy consumed for all GPUs : 0.006997 kWh. Total GPU Power : 78.88281026619161 W\n",
      "[codecarbon INFO @ 15:12:06] Energy consumed for all CPUs : 0.002064 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:12:06] 0.009606 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:12:21] Energy consumed for RAM : 0.000570 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:12:21] Energy consumed for all GPUs : 0.007326 kWh. Total GPU Power : 78.97516781506411 W\n",
      "[codecarbon INFO @ 15:12:21] Energy consumed for all CPUs : 0.002157 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:12:21] 0.010054 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:12:36] Energy consumed for RAM : 0.000595 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:12:36] Energy consumed for all GPUs : 0.007655 kWh. Total GPU Power : 79.02495077230539 W\n",
      "[codecarbon INFO @ 15:12:36] Energy consumed for all CPUs : 0.002251 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:12:36] 0.010502 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:12:36] 0.007092 g.CO2eq/s mean an estimation of 223.65487725375908 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:12:51] Energy consumed for RAM : 0.000620 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:12:51] Energy consumed for all GPUs : 0.007987 kWh. Total GPU Power : 79.39630540931489 W\n",
      "[codecarbon INFO @ 15:12:51] Energy consumed for all CPUs : 0.002345 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:12:51] 0.010951 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:13:06] Energy consumed for RAM : 0.000645 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:13:06] Energy consumed for all GPUs : 0.008316 kWh. Total GPU Power : 78.97648415096916 W\n",
      "[codecarbon INFO @ 15:13:06] Energy consumed for all CPUs : 0.002439 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:13:06] 0.011399 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:13:21] Energy consumed for RAM : 0.000670 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:13:21] Energy consumed for all GPUs : 0.008645 kWh. Total GPU Power : 79.00746636499494 W\n",
      "[codecarbon INFO @ 15:13:21] Energy consumed for all CPUs : 0.002532 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:13:21] 0.011847 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:13:36] Energy consumed for RAM : 0.000694 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:13:36] Energy consumed for all GPUs : 0.008974 kWh. Total GPU Power : 78.9409281336707 W\n",
      "[codecarbon INFO @ 15:13:36] Energy consumed for all CPUs : 0.002626 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:13:36] 0.012295 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:13:51] Energy consumed for RAM : 0.000719 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:13:51] Energy consumed for all GPUs : 0.009306 kWh. Total GPU Power : 79.51952256447905 W\n",
      "[codecarbon INFO @ 15:13:51] Energy consumed for all CPUs : 0.002720 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:13:51] 0.012745 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3517\n",
      "  Batch size = 8\n",
      "[codecarbon INFO @ 15:14:06] Energy consumed for RAM : 0.000744 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:14:06] Energy consumed for all GPUs : 0.009634 kWh. Total GPU Power : 78.90882970128406 W\n",
      "[codecarbon INFO @ 15:14:06] Energy consumed for all CPUs : 0.002814 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:14:06] 0.013192 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Biased     0.6429    0.7152    0.6772      1805\n",
      "      Biased     0.6594    0.5812    0.6178      1712\n",
      "\n",
      "    accuracy                         0.6500      3517\n",
      "   macro avg     0.6512    0.6482    0.6475      3517\n",
      "weighted avg     0.6509    0.6500    0.6483      3517\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-878\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-878\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-878\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-878\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-878\\special_tokens_map.json\n",
      "Deleting older checkpoint [D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-439] due to args.save_total_limit\n",
      "[codecarbon INFO @ 15:14:21] Energy consumed for RAM : 0.000769 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:14:21] Energy consumed for all GPUs : 0.009955 kWh. Total GPU Power : 76.90292685219985 W\n",
      "[codecarbon INFO @ 15:14:21] Energy consumed for all CPUs : 0.002908 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:14:21] 0.013631 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:14:36] Energy consumed for RAM : 0.000794 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:14:36] Energy consumed for all GPUs : 0.010284 kWh. Total GPU Power : 78.87370275119432 W\n",
      "[codecarbon INFO @ 15:14:36] Energy consumed for all CPUs : 0.003001 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:14:36] 0.014079 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:14:36] 0.007078 g.CO2eq/s mean an estimation of 223.20341045912133 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:14:51] Energy consumed for RAM : 0.000818 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:14:51] Energy consumed for all GPUs : 0.010615 kWh. Total GPU Power : 79.53152441451799 W\n",
      "[codecarbon INFO @ 15:14:51] Energy consumed for all CPUs : 0.003095 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:14:51] 0.014529 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:15:06] Energy consumed for RAM : 0.000843 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:15:06] Energy consumed for all GPUs : 0.010945 kWh. Total GPU Power : 79.03481664748371 W\n",
      "[codecarbon INFO @ 15:15:06] Energy consumed for all CPUs : 0.003189 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:15:06] 0.014977 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:15:21] Energy consumed for RAM : 0.000868 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:15:21] Energy consumed for all GPUs : 0.011274 kWh. Total GPU Power : 78.94178488804296 W\n",
      "[codecarbon INFO @ 15:15:21] Energy consumed for all CPUs : 0.003283 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:15:21] 0.015425 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:15:36] Energy consumed for RAM : 0.000893 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:15:36] Energy consumed for all GPUs : 0.011604 kWh. Total GPU Power : 79.13932010700937 W\n",
      "[codecarbon INFO @ 15:15:36] Energy consumed for all CPUs : 0.003377 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:15:36] 0.015873 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:15:51] Energy consumed for RAM : 0.000918 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:15:51] Energy consumed for all GPUs : 0.011935 kWh. Total GPU Power : 79.4307584257674 W\n",
      "[codecarbon INFO @ 15:15:51] Energy consumed for all CPUs : 0.003470 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:15:51] 0.016323 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:16:06] Energy consumed for RAM : 0.000942 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:16:06] Energy consumed for all GPUs : 0.012264 kWh. Total GPU Power : 79.00601313580286 W\n",
      "[codecarbon INFO @ 15:16:06] Energy consumed for all CPUs : 0.003564 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:16:06] 0.016771 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:16:21] Energy consumed for RAM : 0.000967 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:16:21] Energy consumed for all GPUs : 0.012594 kWh. Total GPU Power : 79.10483708748077 W\n",
      "[codecarbon INFO @ 15:16:21] Energy consumed for all CPUs : 0.003658 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:16:21] 0.017219 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:16:36] Energy consumed for RAM : 0.000992 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:16:36] Energy consumed for all GPUs : 0.012923 kWh. Total GPU Power : 79.05249377714922 W\n",
      "[codecarbon INFO @ 15:16:36] Energy consumed for all CPUs : 0.003752 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:16:36] 0.017667 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:16:36] 0.007099 g.CO2eq/s mean an estimation of 223.88870635519626 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:16:51] Energy consumed for RAM : 0.001017 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:16:51] Energy consumed for all GPUs : 0.013252 kWh. Total GPU Power : 79.01278290726351 W\n",
      "[codecarbon INFO @ 15:16:51] Energy consumed for all CPUs : 0.003846 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:16:51] 0.018115 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:17:06] Energy consumed for RAM : 0.001042 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:17:06] Energy consumed for all GPUs : 0.013584 kWh. Total GPU Power : 79.53458019519472 W\n",
      "[codecarbon INFO @ 15:17:06] Energy consumed for all CPUs : 0.003939 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:17:06] 0.018565 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:17:21] Energy consumed for RAM : 0.001066 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:17:21] Energy consumed for all GPUs : 0.013913 kWh. Total GPU Power : 78.93942378366732 W\n",
      "[codecarbon INFO @ 15:17:21] Energy consumed for all CPUs : 0.004033 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:17:21] 0.019013 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:17:36] Energy consumed for RAM : 0.001091 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:17:36] Energy consumed for all GPUs : 0.014242 kWh. Total GPU Power : 78.97289006109371 W\n",
      "[codecarbon INFO @ 15:17:36] Energy consumed for all CPUs : 0.004127 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:17:36] 0.019461 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3517\n",
      "  Batch size = 8\n",
      "[codecarbon INFO @ 15:17:51] Energy consumed for RAM : 0.001116 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:17:51] Energy consumed for all GPUs : 0.014572 kWh. Total GPU Power : 79.00768193040635 W\n",
      "[codecarbon INFO @ 15:17:51] Energy consumed for all CPUs : 0.004221 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:17:51] 0.019909 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Biased     0.7330    0.6493    0.6886      1805\n",
      "      Biased     0.6700    0.7506    0.7080      1712\n",
      "\n",
      "    accuracy                         0.6986      3517\n",
      "   macro avg     0.7015    0.6999    0.6983      3517\n",
      "weighted avg     0.7023    0.6986    0.6980      3517\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-1317\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-1317\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-1317\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-1317\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-1317\\special_tokens_map.json\n",
      "Deleting older checkpoint [D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-878] due to args.save_total_limit\n",
      "[codecarbon INFO @ 15:18:06] Energy consumed for RAM : 0.001141 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:18:06] Energy consumed for all GPUs : 0.014895 kWh. Total GPU Power : 77.63773270228565 W\n",
      "[codecarbon INFO @ 15:18:06] Energy consumed for all CPUs : 0.004315 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:18:06] 0.020351 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:18:21] Energy consumed for RAM : 0.001166 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:18:21] Energy consumed for all GPUs : 0.015224 kWh. Total GPU Power : 78.83110883087267 W\n",
      "[codecarbon INFO @ 15:18:21] Energy consumed for all CPUs : 0.004408 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:18:21] 0.020798 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:18:36] Energy consumed for RAM : 0.001190 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:18:36] Energy consumed for all GPUs : 0.015553 kWh. Total GPU Power : 78.98095969101016 W\n",
      "[codecarbon INFO @ 15:18:36] Energy consumed for all CPUs : 0.004502 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:18:36] 0.021246 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:18:36] 0.007081 g.CO2eq/s mean an estimation of 223.31229168324273 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:18:51] Energy consumed for RAM : 0.001215 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:18:51] Energy consumed for all GPUs : 0.015882 kWh. Total GPU Power : 78.99033952076384 W\n",
      "[codecarbon INFO @ 15:18:51] Energy consumed for all CPUs : 0.004596 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:18:51] 0.021694 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:19:06] Energy consumed for RAM : 0.001240 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:19:06] Energy consumed for all GPUs : 0.016213 kWh. Total GPU Power : 79.38685818699358 W\n",
      "[codecarbon INFO @ 15:19:06] Energy consumed for all CPUs : 0.004690 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:19:06] 0.022143 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:19:21] Energy consumed for RAM : 0.001265 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:19:21] Energy consumed for all GPUs : 0.016542 kWh. Total GPU Power : 78.93522651590332 W\n",
      "[codecarbon INFO @ 15:19:21] Energy consumed for all CPUs : 0.004784 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:19:21] 0.022591 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:19:36] Energy consumed for RAM : 0.001290 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:19:36] Energy consumed for all GPUs : 0.016872 kWh. Total GPU Power : 78.9713168119654 W\n",
      "[codecarbon INFO @ 15:19:36] Energy consumed for all CPUs : 0.004878 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:19:36] 0.023039 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:19:51] Energy consumed for RAM : 0.001314 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:19:51] Energy consumed for all GPUs : 0.017203 kWh. Total GPU Power : 79.47814237299785 W\n",
      "[codecarbon INFO @ 15:19:51] Energy consumed for all CPUs : 0.004971 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:19:51] 0.023489 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:20:06] Energy consumed for RAM : 0.001339 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:20:06] Energy consumed for all GPUs : 0.017533 kWh. Total GPU Power : 79.10699979179464 W\n",
      "[codecarbon INFO @ 15:20:06] Energy consumed for all CPUs : 0.005065 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:20:06] 0.023937 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:20:21] Energy consumed for RAM : 0.001364 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:20:21] Energy consumed for all GPUs : 0.017862 kWh. Total GPU Power : 79.05177474312312 W\n",
      "[codecarbon INFO @ 15:20:21] Energy consumed for all CPUs : 0.005159 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:20:21] 0.024385 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:20:36] Energy consumed for RAM : 0.001389 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:20:36] Energy consumed for all GPUs : 0.018192 kWh. Total GPU Power : 79.00379714236298 W\n",
      "[codecarbon INFO @ 15:20:36] Energy consumed for all CPUs : 0.005253 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:20:36] 0.024833 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:20:36] 0.007097 g.CO2eq/s mean an estimation of 223.82662647165284 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:20:51] Energy consumed for RAM : 0.001414 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:20:51] Energy consumed for all GPUs : 0.018521 kWh. Total GPU Power : 79.11135157966791 W\n",
      "[codecarbon INFO @ 15:20:51] Energy consumed for all CPUs : 0.005347 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:20:51] 0.025281 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:21:06] Energy consumed for RAM : 0.001438 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:21:06] Energy consumed for all GPUs : 0.018853 kWh. Total GPU Power : 79.56318672464285 W\n",
      "[codecarbon INFO @ 15:21:06] Energy consumed for all CPUs : 0.005440 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:21:06] 0.025731 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:21:21] Energy consumed for RAM : 0.001463 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:21:21] Energy consumed for all GPUs : 0.019182 kWh. Total GPU Power : 78.94764764100636 W\n",
      "[codecarbon INFO @ 15:21:21] Energy consumed for all CPUs : 0.005534 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:21:21] 0.026179 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-1756\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-1756\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-1756\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-1756\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-1756\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3517\n",
      "  Batch size = 8\n",
      "[codecarbon INFO @ 15:21:36] Energy consumed for RAM : 0.001488 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:21:36] Energy consumed for all GPUs : 0.019505 kWh. Total GPU Power : 77.58457626705957 W\n",
      "[codecarbon INFO @ 15:21:36] Energy consumed for all CPUs : 0.005628 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:21:36] 0.026621 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Biased     0.7433    0.7540    0.7486      1805\n",
      "      Biased     0.7367    0.7255    0.7310      1712\n",
      "\n",
      "    accuracy                         0.7401      3517\n",
      "   macro avg     0.7400    0.7397    0.7398      3517\n",
      "weighted avg     0.7401    0.7401    0.7401      3517\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-1756\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-1756\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-1756\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-1756\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-1756\\special_tokens_map.json\n",
      "Deleting older checkpoint [D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-1317] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\checkpoint-1756 (score: 0.5244128704071045).\n",
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\special_tokens_map.json\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\\special_tokens_map.json\n",
      "[codecarbon INFO @ 15:21:48] Energy consumed for RAM : 0.001507 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:21:48] Energy consumed for all GPUs : 0.019752 kWh. Total GPU Power : 75.33115198225036 W\n",
      "[codecarbon INFO @ 15:21:48] Energy consumed for all CPUs : 0.005702 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:21:48] 0.026961 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbon dioxide emission: 6.405677 g\n"
     ]
    }
   ],
   "source": [
    "distilbert_model_dir = train_model(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    model_path='distilbert/distilbert-base-uncased',\n",
    "    batch_size=TRAINING_CONFIG['batch_size'],\n",
    "    epoch=TRAINING_CONFIG['epochs'],\n",
    "    learning_rate=TRAINING_CONFIG['learning_rate'],\n",
    "    gradient_accumulation_steps=TRAINING_CONFIG['gradient_accumulation_steps'],\n",
    "    dataset_name='job_descriptions',\n",
    "    seed=TRAINING_CONFIG['seed'],\n",
    "    save_model=TRAINING_CONFIG['save_model'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration for BERT\n",
    "TRAINING_CONFIG = {\n",
    "    'batch_size': 8,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'epochs': 6,\n",
    "    'learning_rate': 2e-5,\n",
    "    'seed': 42,\n",
    "    'save_model': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 15:21:48] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 15:21:48] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 15:21:48] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 15:21:48] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 15:21:48] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 15:21:48] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 15:21:50] CPU Model on constant consumption mode: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 15:21:50] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 15:21:50]   Platform system: Windows-10-10.0.26100-SP0\n",
      "[codecarbon INFO @ 15:21:50]   Python version: 3.10.19\n",
      "[codecarbon INFO @ 15:21:50]   CodeCarbon version: 2.8.0\n",
      "[codecarbon INFO @ 15:21:50]   Available RAM : 15.870 GB\n",
      "[codecarbon INFO @ 15:21:50]   CPU count: 16\n",
      "[codecarbon INFO @ 15:21:50]   CPU model: Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n",
      "[codecarbon INFO @ 15:21:50]   GPU count: 1\n",
      "[codecarbon INFO @ 15:21:50]   GPU model: 1 x NVIDIA GeForce RTX 2060\n",
      "[codecarbon INFO @ 15:21:53] Saving emissions data to file D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\notebooks\\emissions.csv\n",
      "loading configuration file config.json from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Loading Model: google-bert/bert-base-uncased\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model.safetensors from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\model.safetensors\n",
      "A pretrained model of type `BertForSequenceClassification` contains parameters that have been renamed internally (a few are listed below but more are present in the model):\n",
      "* `bert.embeddings.LayerNorm.gamma` -> `bert.embeddings.LayerNorm.weight`\n",
      "* `bert.encoder.layer.0.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.0.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.1.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.1.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.10.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.10.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.11.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.11.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.2.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.2.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.3.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.3.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.4.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.4.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.5.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.5.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.6.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.6.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.7.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.7.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.8.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.8.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.9.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.9.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `cls.predictions.transform.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.embeddings.LayerNorm.beta` -> `bert.embeddings.LayerNorm.bias`\n",
      "* `bert.encoder.layer.0.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.0.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.1.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.1.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.10.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.10.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.11.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.11.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.2.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.2.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.3.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.3.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.4.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.4.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.5.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.5.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.6.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.6.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.7.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.7.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.8.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.8.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.9.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.9.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `cls.predictions.transform.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--google-bert--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e9c40280454f339380fda8f1f9233d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14065 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d811c74b2b4043a7401f010611956e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14065 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing validation data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce2cdff9b6f47cd96379beeb6c84e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3517 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10d1b9753bc458694dd539477fe75da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3517 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\hearts\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tokenization complete\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\b'h'w'l\\AppData\\Local\\Temp\\ipykernel_46708\\1808897401.py:165: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Using auto half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 14,065\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 2,634\n",
      "  Number of trainable parameters = 109,483,778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING CONFIGURATION\n",
      "============================================================\n",
      "Model: google-bert/bert-base-uncased\n",
      "Number of Epochs: 6\n",
      "Learning Rate: 2.00e-05\n",
      "Batch Size: 8\n",
      "Gradient Accumulation Steps: 4\n",
      "Effective Batch Size: 32\n",
      "Training Samples: 14,065\n",
      "Validation Samples: 3,517\n",
      "Device: cuda\n",
      "Mixed Precision (FP16): True\n",
      "============================================================\n",
      "\n",
      "   Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 15:22:08] Energy consumed for RAM : 0.000025 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:22:08] Energy consumed for all GPUs : 0.000038 kWh. Total GPU Power : 9.027937742724621 W\n",
      "[codecarbon INFO @ 15:22:08] Energy consumed for all CPUs : 0.000094 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:22:08] 0.000156 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2634' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2634/2634 43:55, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.668215</td>\n",
       "      <td>0.637267</td>\n",
       "      <td>0.608110</td>\n",
       "      <td>0.582165</td>\n",
       "      <td>0.608110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.633700</td>\n",
       "      <td>0.615355</td>\n",
       "      <td>0.652942</td>\n",
       "      <td>0.652964</td>\n",
       "      <td>0.652544</td>\n",
       "      <td>0.652964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.580032</td>\n",
       "      <td>0.691911</td>\n",
       "      <td>0.691811</td>\n",
       "      <td>0.691208</td>\n",
       "      <td>0.691811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.448300</td>\n",
       "      <td>0.460178</td>\n",
       "      <td>0.779858</td>\n",
       "      <td>0.778189</td>\n",
       "      <td>0.778426</td>\n",
       "      <td>0.778189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.377200</td>\n",
       "      <td>0.476413</td>\n",
       "      <td>0.799753</td>\n",
       "      <td>0.796837</td>\n",
       "      <td>0.794987</td>\n",
       "      <td>0.796837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.287000</td>\n",
       "      <td>0.444430</td>\n",
       "      <td>0.822470</td>\n",
       "      <td>0.819555</td>\n",
       "      <td>0.817789</td>\n",
       "      <td>0.819555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 15:22:23] Energy consumed for RAM : 0.000050 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:22:23] Energy consumed for all GPUs : 0.000366 kWh. Total GPU Power : 78.81944531296713 W\n",
      "[codecarbon INFO @ 15:22:23] Energy consumed for all CPUs : 0.000188 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:22:23] 0.000603 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:22:38] Energy consumed for RAM : 0.000074 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:22:38] Energy consumed for all GPUs : 0.000695 kWh. Total GPU Power : 78.94040813485947 W\n",
      "[codecarbon INFO @ 15:22:38] Energy consumed for all CPUs : 0.000281 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:22:38] 0.001051 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:22:53] Energy consumed for RAM : 0.000099 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:22:53] Energy consumed for all GPUs : 0.001026 kWh. Total GPU Power : 79.4316497367667 W\n",
      "[codecarbon INFO @ 15:22:53] Energy consumed for all CPUs : 0.000375 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:22:53] 0.001501 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:23:08] Energy consumed for RAM : 0.000124 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:23:08] Energy consumed for all GPUs : 0.001356 kWh. Total GPU Power : 79.0476749728537 W\n",
      "[codecarbon INFO @ 15:23:08] Energy consumed for all CPUs : 0.000469 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:23:08] 0.001949 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:23:23] Energy consumed for RAM : 0.000149 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:23:23] Energy consumed for all GPUs : 0.001687 kWh. Total GPU Power : 79.53393795456844 W\n",
      "[codecarbon INFO @ 15:23:23] Energy consumed for all CPUs : 0.000563 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:23:23] 0.002399 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:23:38] Energy consumed for RAM : 0.000174 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:23:38] Energy consumed for all GPUs : 0.002016 kWh. Total GPU Power : 78.98333297432148 W\n",
      "[codecarbon INFO @ 15:23:38] Energy consumed for all CPUs : 0.000657 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:23:38] 0.002847 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:23:53] Energy consumed for RAM : 0.000198 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:23:53] Energy consumed for all GPUs : 0.002348 kWh. Total GPU Power : 79.56526580911472 W\n",
      "[codecarbon INFO @ 15:23:53] Energy consumed for all CPUs : 0.000750 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:23:53] 0.003297 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:23:53] 0.006523 g.CO2eq/s mean an estimation of 205.70334594305993 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:24:08] Energy consumed for RAM : 0.000223 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:24:08] Energy consumed for all GPUs : 0.002678 kWh. Total GPU Power : 79.14045677087573 W\n",
      "[codecarbon INFO @ 15:24:08] Energy consumed for all CPUs : 0.000844 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:24:08] 0.003745 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:24:23] Energy consumed for RAM : 0.000248 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:24:23] Energy consumed for all GPUs : 0.003007 kWh. Total GPU Power : 79.03939047392909 W\n",
      "[codecarbon INFO @ 15:24:23] Energy consumed for all CPUs : 0.000938 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:24:23] 0.004193 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:24:38] Energy consumed for RAM : 0.000273 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:24:38] Energy consumed for all GPUs : 0.003339 kWh. Total GPU Power : 79.51664002866816 W\n",
      "[codecarbon INFO @ 15:24:38] Energy consumed for all CPUs : 0.001032 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:24:38] 0.004643 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:24:53] Energy consumed for RAM : 0.000298 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:24:53] Energy consumed for all GPUs : 0.003668 kWh. Total GPU Power : 78.9271238583929 W\n",
      "[codecarbon INFO @ 15:24:53] Energy consumed for all CPUs : 0.001126 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:24:53] 0.005091 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:25:08] Energy consumed for RAM : 0.000322 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:25:08] Energy consumed for all GPUs : 0.004000 kWh. Total GPU Power : 79.67063299822043 W\n",
      "[codecarbon INFO @ 15:25:08] Energy consumed for all CPUs : 0.001219 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:25:08] 0.005541 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:25:23] Energy consumed for RAM : 0.000347 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:25:23] Energy consumed for all GPUs : 0.004329 kWh. Total GPU Power : 78.969310971721 W\n",
      "[codecarbon INFO @ 15:25:23] Energy consumed for all CPUs : 0.001313 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:25:23] 0.005989 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:25:38] Energy consumed for RAM : 0.000372 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:25:38] Energy consumed for all GPUs : 0.004660 kWh. Total GPU Power : 79.58018427451888 W\n",
      "[codecarbon INFO @ 15:25:38] Energy consumed for all CPUs : 0.001407 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:25:38] 0.006439 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:25:53] Energy consumed for RAM : 0.000397 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:25:53] Energy consumed for all GPUs : 0.004990 kWh. Total GPU Power : 79.13469429900418 W\n",
      "[codecarbon INFO @ 15:25:53] Energy consumed for all CPUs : 0.001501 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:25:53] 0.006888 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:25:53] 0.007106 g.CO2eq/s mean an estimation of 224.09954761029664 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:26:08] Energy consumed for RAM : 0.000422 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:26:08] Energy consumed for all GPUs : 0.005322 kWh. Total GPU Power : 79.49096030280589 W\n",
      "[codecarbon INFO @ 15:26:08] Energy consumed for all CPUs : 0.001595 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:26:08] 0.007338 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:26:23] Energy consumed for RAM : 0.000446 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:26:23] Energy consumed for all GPUs : 0.005651 kWh. Total GPU Power : 78.983298690825 W\n",
      "[codecarbon INFO @ 15:26:23] Energy consumed for all CPUs : 0.001688 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:26:23] 0.007786 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:26:38] Energy consumed for RAM : 0.000471 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:26:38] Energy consumed for all GPUs : 0.005983 kWh. Total GPU Power : 79.59386832873408 W\n",
      "[codecarbon INFO @ 15:26:38] Energy consumed for all CPUs : 0.001782 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:26:38] 0.008236 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:26:53] Energy consumed for RAM : 0.000496 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:26:53] Energy consumed for all GPUs : 0.006312 kWh. Total GPU Power : 79.04728723807055 W\n",
      "[codecarbon INFO @ 15:26:53] Energy consumed for all CPUs : 0.001876 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:26:53] 0.008684 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:27:08] Energy consumed for RAM : 0.000521 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:27:08] Energy consumed for all GPUs : 0.006644 kWh. Total GPU Power : 79.5911170667301 W\n",
      "[codecarbon INFO @ 15:27:08] Energy consumed for all CPUs : 0.001970 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:27:08] 0.009135 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:27:23] Energy consumed for RAM : 0.000546 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:27:23] Energy consumed for all GPUs : 0.006973 kWh. Total GPU Power : 78.97942167042528 W\n",
      "[codecarbon INFO @ 15:27:23] Energy consumed for all CPUs : 0.002064 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:27:23] 0.009582 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:27:38] Energy consumed for RAM : 0.000570 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:27:38] Energy consumed for all GPUs : 0.007302 kWh. Total GPU Power : 78.9869944576734 W\n",
      "[codecarbon INFO @ 15:27:38] Energy consumed for all CPUs : 0.002157 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:27:38] 0.010030 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:27:53] Energy consumed for RAM : 0.000595 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:27:53] Energy consumed for all GPUs : 0.007634 kWh. Total GPU Power : 79.61996882220483 W\n",
      "[codecarbon INFO @ 15:27:53] Energy consumed for all CPUs : 0.002251 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:27:53] 0.010481 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:27:53] 0.007109 g.CO2eq/s mean an estimation of 224.17815794121614 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:28:08] Energy consumed for RAM : 0.000620 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:28:08] Energy consumed for all GPUs : 0.007963 kWh. Total GPU Power : 78.92580896622617 W\n",
      "[codecarbon INFO @ 15:28:08] Energy consumed for all CPUs : 0.002345 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:28:08] 0.010928 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:28:23] Energy consumed for RAM : 0.000645 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:28:23] Energy consumed for all GPUs : 0.008295 kWh. Total GPU Power : 79.59010489869613 W\n",
      "[codecarbon INFO @ 15:28:23] Energy consumed for all CPUs : 0.002439 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:28:23] 0.011378 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:28:38] Energy consumed for RAM : 0.000670 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:28:38] Energy consumed for all GPUs : 0.008624 kWh. Total GPU Power : 78.99131865073885 W\n",
      "[codecarbon INFO @ 15:28:38] Energy consumed for all CPUs : 0.002533 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:28:38] 0.011826 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:28:53] Energy consumed for RAM : 0.000694 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:28:53] Energy consumed for all GPUs : 0.008956 kWh. Total GPU Power : 79.64472625742246 W\n",
      "[codecarbon INFO @ 15:28:53] Energy consumed for all CPUs : 0.002626 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:28:53] 0.012277 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3517\n",
      "  Batch size = 8\n",
      "[codecarbon INFO @ 15:29:08] Energy consumed for RAM : 0.000719 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:29:08] Energy consumed for all GPUs : 0.009284 kWh. Total GPU Power : 78.73325547866713 W\n",
      "[codecarbon INFO @ 15:29:08] Energy consumed for all CPUs : 0.002720 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:29:08] 0.012724 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:29:23] Energy consumed for RAM : 0.000744 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:29:23] Energy consumed for all GPUs : 0.009613 kWh. Total GPU Power : 78.8051863303808 W\n",
      "[codecarbon INFO @ 15:29:23] Energy consumed for all CPUs : 0.002814 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:29:23] 0.013171 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Biased     0.7138    0.3745    0.4913      1805\n",
      "      Biased     0.5607    0.8417    0.6730      1712\n",
      "\n",
      "    accuracy                         0.6019      3517\n",
      "   macro avg     0.6373    0.6081    0.5822      3517\n",
      "weighted avg     0.6393    0.6019    0.5798      3517\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-439\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-439\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-439\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-439\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-439\\special_tokens_map.json\n",
      "[codecarbon INFO @ 15:29:38] Energy consumed for RAM : 0.000769 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:29:38] Energy consumed for all GPUs : 0.009931 kWh. Total GPU Power : 76.39801238221989 W\n",
      "[codecarbon INFO @ 15:29:38] Energy consumed for all CPUs : 0.002908 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:29:38] 0.013608 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:29:53] Energy consumed for RAM : 0.000794 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:29:53] Energy consumed for all GPUs : 0.010263 kWh. Total GPU Power : 79.50702174048526 W\n",
      "[codecarbon INFO @ 15:29:53] Energy consumed for all CPUs : 0.003002 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:29:53] 0.014058 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:29:53] 0.007078 g.CO2eq/s mean an estimation of 223.2119566179998 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:30:08] Energy consumed for RAM : 0.000818 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:30:08] Energy consumed for all GPUs : 0.010592 kWh. Total GPU Power : 79.00391040612418 W\n",
      "[codecarbon INFO @ 15:30:08] Energy consumed for all CPUs : 0.003095 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:30:08] 0.014506 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:30:23] Energy consumed for RAM : 0.000843 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:30:23] Energy consumed for all GPUs : 0.010923 kWh. Total GPU Power : 79.48060338201213 W\n",
      "[codecarbon INFO @ 15:30:23] Energy consumed for all CPUs : 0.003189 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:30:23] 0.014956 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:30:38] Energy consumed for RAM : 0.000868 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:30:38] Energy consumed for all GPUs : 0.011253 kWh. Total GPU Power : 79.11229471219308 W\n",
      "[codecarbon INFO @ 15:30:38] Energy consumed for all CPUs : 0.003283 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:30:38] 0.015404 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:30:53] Energy consumed for RAM : 0.000893 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:30:53] Energy consumed for all GPUs : 0.011584 kWh. Total GPU Power : 79.41494703624886 W\n",
      "[codecarbon INFO @ 15:30:53] Energy consumed for all CPUs : 0.003377 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:30:53] 0.015854 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:31:08] Energy consumed for RAM : 0.000918 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:31:08] Energy consumed for all GPUs : 0.011913 kWh. Total GPU Power : 78.9550320233048 W\n",
      "[codecarbon INFO @ 15:31:08] Energy consumed for all CPUs : 0.003471 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:31:08] 0.016301 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:31:23] Energy consumed for RAM : 0.000942 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:31:23] Energy consumed for all GPUs : 0.012243 kWh. Total GPU Power : 78.99557812752921 W\n",
      "[codecarbon INFO @ 15:31:23] Energy consumed for all CPUs : 0.003564 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:31:23] 0.016749 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:31:38] Energy consumed for RAM : 0.000967 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:31:38] Energy consumed for all GPUs : 0.012574 kWh. Total GPU Power : 79.51954029665741 W\n",
      "[codecarbon INFO @ 15:31:38] Energy consumed for all CPUs : 0.003658 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:31:38] 0.017199 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:31:53] Energy consumed for RAM : 0.000992 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:31:53] Energy consumed for all GPUs : 0.012903 kWh. Total GPU Power : 79.03243772583889 W\n",
      "[codecarbon INFO @ 15:31:53] Energy consumed for all CPUs : 0.003752 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:31:53] 0.017647 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:31:53] 0.007102 g.CO2eq/s mean an estimation of 223.97263880851162 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:32:08] Energy consumed for RAM : 0.001017 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:32:08] Energy consumed for all GPUs : 0.013235 kWh. Total GPU Power : 79.61612833963912 W\n",
      "[codecarbon INFO @ 15:32:08] Energy consumed for all CPUs : 0.003846 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:32:08] 0.018097 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:32:23] Energy consumed for RAM : 0.001042 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:32:23] Energy consumed for all GPUs : 0.013565 kWh. Total GPU Power : 79.05095176165253 W\n",
      "[codecarbon INFO @ 15:32:23] Energy consumed for all CPUs : 0.003940 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:32:23] 0.018546 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:32:38] Energy consumed for RAM : 0.001066 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:32:38] Energy consumed for all GPUs : 0.013893 kWh. Total GPU Power : 78.99044888766349 W\n",
      "[codecarbon INFO @ 15:32:38] Energy consumed for all CPUs : 0.004033 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:32:38] 0.018993 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:32:53] Energy consumed for RAM : 0.001091 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:32:53] Energy consumed for all GPUs : 0.014225 kWh. Total GPU Power : 79.49612923998265 W\n",
      "[codecarbon INFO @ 15:32:53] Energy consumed for all CPUs : 0.004127 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:32:53] 0.019443 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:33:08] Energy consumed for RAM : 0.001116 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:33:08] Energy consumed for all GPUs : 0.014554 kWh. Total GPU Power : 79.03436252871983 W\n",
      "[codecarbon INFO @ 15:33:08] Energy consumed for all CPUs : 0.004221 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:33:08] 0.019891 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:33:23] Energy consumed for RAM : 0.001141 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:33:23] Energy consumed for all GPUs : 0.014885 kWh. Total GPU Power : 79.37204321488625 W\n",
      "[codecarbon INFO @ 15:33:23] Energy consumed for all CPUs : 0.004315 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:33:23] 0.020341 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:33:38] Energy consumed for RAM : 0.001165 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:33:38] Energy consumed for all GPUs : 0.015215 kWh. Total GPU Power : 79.1328183169132 W\n",
      "[codecarbon INFO @ 15:33:38] Energy consumed for all CPUs : 0.004408 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:33:38] 0.020789 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:33:53] Energy consumed for RAM : 0.001190 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:33:53] Energy consumed for all GPUs : 0.015544 kWh. Total GPU Power : 79.01455999892039 W\n",
      "[codecarbon INFO @ 15:33:53] Energy consumed for all CPUs : 0.004502 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:33:53] 0.021237 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:33:53] 0.007104 g.CO2eq/s mean an estimation of 224.0211877181366 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:34:08] Energy consumed for RAM : 0.001215 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:34:09] Energy consumed for all GPUs : 0.015876 kWh. Total GPU Power : 79.59458394788597 W\n",
      "[codecarbon INFO @ 15:34:09] Energy consumed for all CPUs : 0.004596 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:34:09] 0.021687 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:34:24] Energy consumed for RAM : 0.001240 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:34:24] Energy consumed for all GPUs : 0.016205 kWh. Total GPU Power : 78.92715153173941 W\n",
      "[codecarbon INFO @ 15:34:24] Energy consumed for all CPUs : 0.004690 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:34:24] 0.022134 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:34:39] Energy consumed for RAM : 0.001265 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:34:39] Energy consumed for all GPUs : 0.016534 kWh. Total GPU Power : 78.8984096861263 W\n",
      "[codecarbon INFO @ 15:34:39] Energy consumed for all CPUs : 0.004784 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:34:39] 0.022582 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:34:54] Energy consumed for RAM : 0.001289 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:34:54] Energy consumed for all GPUs : 0.016865 kWh. Total GPU Power : 79.4559632012771 W\n",
      "[codecarbon INFO @ 15:34:54] Energy consumed for all CPUs : 0.004877 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:34:54] 0.023032 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:35:09] Energy consumed for RAM : 0.001314 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:35:09] Energy consumed for all GPUs : 0.017194 kWh. Total GPU Power : 78.89111379505165 W\n",
      "[codecarbon INFO @ 15:35:09] Energy consumed for all CPUs : 0.004971 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:35:09] 0.023479 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:35:24] Energy consumed for RAM : 0.001339 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:35:24] Energy consumed for all GPUs : 0.017525 kWh. Total GPU Power : 79.4802741476613 W\n",
      "[codecarbon INFO @ 15:35:24] Energy consumed for all CPUs : 0.005065 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:35:24] 0.023929 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:35:39] Energy consumed for RAM : 0.001364 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:35:39] Energy consumed for all GPUs : 0.017854 kWh. Total GPU Power : 78.99592484793257 W\n",
      "[codecarbon INFO @ 15:35:39] Energy consumed for all CPUs : 0.005159 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:35:39] 0.024377 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:35:54] Energy consumed for RAM : 0.001389 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:35:54] Energy consumed for all GPUs : 0.018183 kWh. Total GPU Power : 78.9571280110409 W\n",
      "[codecarbon INFO @ 15:35:54] Energy consumed for all CPUs : 0.005253 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:35:54] 0.024824 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:35:54] 0.007100 g.CO2eq/s mean an estimation of 223.89593651274916 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:36:09] Energy consumed for RAM : 0.001413 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:36:09] Energy consumed for all GPUs : 0.018514 kWh. Total GPU Power : 79.47411671343188 W\n",
      "[codecarbon INFO @ 15:36:09] Energy consumed for all CPUs : 0.005346 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:36:09] 0.025274 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3517\n",
      "  Batch size = 8\n",
      "[codecarbon INFO @ 15:36:24] Energy consumed for RAM : 0.001438 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:36:24] Energy consumed for all GPUs : 0.018842 kWh. Total GPU Power : 78.65461053981177 W\n",
      "[codecarbon INFO @ 15:36:24] Energy consumed for all CPUs : 0.005440 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:36:24] 0.025721 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:36:39] Energy consumed for RAM : 0.001463 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:36:39] Energy consumed for all GPUs : 0.019170 kWh. Total GPU Power : 78.67356740522182 W\n",
      "[codecarbon INFO @ 15:36:39] Energy consumed for all CPUs : 0.005534 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:36:39] 0.026167 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Biased     0.6698    0.6371    0.6530      1805\n",
      "      Biased     0.6361    0.6688    0.6521      1712\n",
      "\n",
      "    accuracy                         0.6525      3517\n",
      "   macro avg     0.6529    0.6530    0.6525      3517\n",
      "weighted avg     0.6534    0.6525    0.6526      3517\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-878\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-878\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-878\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-878\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-878\\special_tokens_map.json\n",
      "Deleting older checkpoint [D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-439] due to args.save_total_limit\n",
      "[codecarbon INFO @ 15:36:54] Energy consumed for RAM : 0.001488 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:36:54] Energy consumed for all GPUs : 0.019485 kWh. Total GPU Power : 75.55040435942468 W\n",
      "[codecarbon INFO @ 15:36:54] Energy consumed for all CPUs : 0.005628 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:36:54] 0.026600 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:37:09] Energy consumed for RAM : 0.001513 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:37:09] Energy consumed for all GPUs : 0.019816 kWh. Total GPU Power : 79.32841490630685 W\n",
      "[codecarbon INFO @ 15:37:09] Energy consumed for all CPUs : 0.005721 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:37:09] 0.027050 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:37:24] Energy consumed for RAM : 0.001537 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:37:24] Energy consumed for all GPUs : 0.020145 kWh. Total GPU Power : 78.96873089847197 W\n",
      "[codecarbon INFO @ 15:37:24] Energy consumed for all CPUs : 0.005815 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:37:24] 0.027497 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:37:39] Energy consumed for RAM : 0.001562 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:37:39] Energy consumed for all GPUs : 0.020476 kWh. Total GPU Power : 79.39551643901446 W\n",
      "[codecarbon INFO @ 15:37:39] Energy consumed for all CPUs : 0.005909 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:37:39] 0.027947 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:37:54] Energy consumed for RAM : 0.001587 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:37:54] Energy consumed for all GPUs : 0.020805 kWh. Total GPU Power : 78.96201384986202 W\n",
      "[codecarbon INFO @ 15:37:54] Energy consumed for all CPUs : 0.006003 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:37:54] 0.028395 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:37:54] 0.007065 g.CO2eq/s mean an estimation of 222.79923682855437 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:38:09] Energy consumed for RAM : 0.001612 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:38:09] Energy consumed for all GPUs : 0.021134 kWh. Total GPU Power : 78.92184013926044 W\n",
      "[codecarbon INFO @ 15:38:09] Energy consumed for all CPUs : 0.006097 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:38:09] 0.028842 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:38:24] Energy consumed for RAM : 0.001637 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:38:24] Energy consumed for all GPUs : 0.021465 kWh. Total GPU Power : 79.38697385149197 W\n",
      "[codecarbon INFO @ 15:38:24] Energy consumed for all CPUs : 0.006191 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:38:24] 0.029292 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:38:39] Energy consumed for RAM : 0.001661 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:38:39] Energy consumed for all GPUs : 0.021794 kWh. Total GPU Power : 78.84363272023293 W\n",
      "[codecarbon INFO @ 15:38:39] Energy consumed for all CPUs : 0.006284 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:38:39] 0.029739 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:38:54] Energy consumed for RAM : 0.001686 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:38:54] Energy consumed for all GPUs : 0.022125 kWh. Total GPU Power : 79.58516220498133 W\n",
      "[codecarbon INFO @ 15:38:54] Energy consumed for all CPUs : 0.006378 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:38:54] 0.030189 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:39:09] Energy consumed for RAM : 0.001711 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:39:09] Energy consumed for all GPUs : 0.022454 kWh. Total GPU Power : 78.90012996864687 W\n",
      "[codecarbon INFO @ 15:39:09] Energy consumed for all CPUs : 0.006472 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:39:09] 0.030636 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:39:24] Energy consumed for RAM : 0.001736 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:39:24] Energy consumed for all GPUs : 0.022783 kWh. Total GPU Power : 78.95112416101047 W\n",
      "[codecarbon INFO @ 15:39:24] Energy consumed for all CPUs : 0.006566 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:39:24] 0.031084 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:39:39] Energy consumed for RAM : 0.001761 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:39:39] Energy consumed for all GPUs : 0.023114 kWh. Total GPU Power : 79.48146002338301 W\n",
      "[codecarbon INFO @ 15:39:39] Energy consumed for all CPUs : 0.006659 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:39:39] 0.031534 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:39:54] Energy consumed for RAM : 0.001785 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:39:54] Energy consumed for all GPUs : 0.023443 kWh. Total GPU Power : 78.8404592283555 W\n",
      "[codecarbon INFO @ 15:39:54] Energy consumed for all CPUs : 0.006753 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:39:54] 0.031981 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:39:54] 0.007097 g.CO2eq/s mean an estimation of 223.81758507713232 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:40:09] Energy consumed for RAM : 0.001810 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:40:09] Energy consumed for all GPUs : 0.023774 kWh. Total GPU Power : 79.42562063100341 W\n",
      "[codecarbon INFO @ 15:40:09] Energy consumed for all CPUs : 0.006847 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:40:09] 0.032431 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:40:24] Energy consumed for RAM : 0.001835 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:40:24] Energy consumed for all GPUs : 0.024103 kWh. Total GPU Power : 78.98457369067926 W\n",
      "[codecarbon INFO @ 15:40:24] Energy consumed for all CPUs : 0.006941 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:40:24] 0.032878 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:40:39] Energy consumed for RAM : 0.001860 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:40:39] Energy consumed for all GPUs : 0.024432 kWh. Total GPU Power : 79.10371195018094 W\n",
      "[codecarbon INFO @ 15:40:39] Energy consumed for all CPUs : 0.007034 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:40:39] 0.033326 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:40:54] Energy consumed for RAM : 0.001885 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:40:54] Energy consumed for all GPUs : 0.024763 kWh. Total GPU Power : 79.43137423853483 W\n",
      "[codecarbon INFO @ 15:40:54] Energy consumed for all CPUs : 0.007128 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:40:54] 0.033776 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:41:09] Energy consumed for RAM : 0.001909 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:41:09] Energy consumed for all GPUs : 0.025092 kWh. Total GPU Power : 78.85795584312568 W\n",
      "[codecarbon INFO @ 15:41:09] Energy consumed for all CPUs : 0.007222 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:41:09] 0.034224 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:41:24] Energy consumed for RAM : 0.001934 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:41:24] Energy consumed for all GPUs : 0.025424 kWh. Total GPU Power : 79.54229324724767 W\n",
      "[codecarbon INFO @ 15:41:24] Energy consumed for all CPUs : 0.007316 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:41:24] 0.034674 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:41:39] Energy consumed for RAM : 0.001959 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:41:39] Energy consumed for all GPUs : 0.025753 kWh. Total GPU Power : 78.9713453657718 W\n",
      "[codecarbon INFO @ 15:41:39] Energy consumed for all CPUs : 0.007410 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:41:39] 0.035122 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:41:54] Energy consumed for RAM : 0.001984 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:41:54] Energy consumed for all GPUs : 0.026082 kWh. Total GPU Power : 79.01583925459629 W\n",
      "[codecarbon INFO @ 15:41:54] Energy consumed for all CPUs : 0.007504 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:41:54] 0.035570 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:41:54] 0.007101 g.CO2eq/s mean an estimation of 223.93391776461223 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:42:09] Energy consumed for RAM : 0.002009 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:42:09] Energy consumed for all GPUs : 0.026414 kWh. Total GPU Power : 79.49229222816992 W\n",
      "[codecarbon INFO @ 15:42:09] Energy consumed for all CPUs : 0.007597 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:42:09] 0.036020 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:42:24] Energy consumed for RAM : 0.002033 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:42:24] Energy consumed for all GPUs : 0.026743 kWh. Total GPU Power : 78.96488252421099 W\n",
      "[codecarbon INFO @ 15:42:24] Energy consumed for all CPUs : 0.007691 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:42:24] 0.036467 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:42:39] Energy consumed for RAM : 0.002058 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:42:39] Energy consumed for all GPUs : 0.027074 kWh. Total GPU Power : 79.49530801518036 W\n",
      "[codecarbon INFO @ 15:42:39] Energy consumed for all CPUs : 0.007785 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:42:39] 0.036917 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:42:54] Energy consumed for RAM : 0.002083 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:42:54] Energy consumed for all GPUs : 0.027403 kWh. Total GPU Power : 78.94058332491001 W\n",
      "[codecarbon INFO @ 15:42:54] Energy consumed for all CPUs : 0.007879 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:42:54] 0.037365 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:43:09] Energy consumed for RAM : 0.002108 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:43:09] Energy consumed for all GPUs : 0.027733 kWh. Total GPU Power : 78.99377964018687 W\n",
      "[codecarbon INFO @ 15:43:09] Energy consumed for all CPUs : 0.007972 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:43:09] 0.037813 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:43:24] Energy consumed for RAM : 0.002133 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:43:24] Energy consumed for all GPUs : 0.028064 kWh. Total GPU Power : 79.64630863043482 W\n",
      "[codecarbon INFO @ 15:43:24] Energy consumed for all CPUs : 0.008066 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:43:24] 0.038263 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3517\n",
      "  Batch size = 8\n",
      "[codecarbon INFO @ 15:43:39] Energy consumed for RAM : 0.002157 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:43:39] Energy consumed for all GPUs : 0.028393 kWh. Total GPU Power : 78.88778646209278 W\n",
      "[codecarbon INFO @ 15:43:39] Energy consumed for all CPUs : 0.008160 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:43:39] 0.038711 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:43:54] Energy consumed for RAM : 0.002182 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:43:54] Energy consumed for all GPUs : 0.028721 kWh. Total GPU Power : 78.64094748427694 W\n",
      "[codecarbon INFO @ 15:43:54] Energy consumed for all CPUs : 0.008254 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:43:54] 0.039157 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:43:54] 0.007099 g.CO2eq/s mean an estimation of 223.86338206877033 kg.CO2eq/year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Biased     0.7118    0.6693    0.6899      1805\n",
      "      Biased     0.6720    0.7144    0.6925      1712\n",
      "\n",
      "    accuracy                         0.6912      3517\n",
      "   macro avg     0.6919    0.6918    0.6912      3517\n",
      "weighted avg     0.6924    0.6912    0.6912      3517\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-1317\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-1317\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-1317\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-1317\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-1317\\special_tokens_map.json\n",
      "Deleting older checkpoint [D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-878] due to args.save_total_limit\n",
      "[codecarbon INFO @ 15:44:09] Energy consumed for RAM : 0.002207 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:44:09] Energy consumed for all GPUs : 0.029037 kWh. Total GPU Power : 75.82733661037807 W\n",
      "[codecarbon INFO @ 15:44:09] Energy consumed for all CPUs : 0.008348 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:44:09] 0.039592 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:44:24] Energy consumed for RAM : 0.002232 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:44:24] Energy consumed for all GPUs : 0.029369 kWh. Total GPU Power : 79.59174079834888 W\n",
      "[codecarbon INFO @ 15:44:24] Energy consumed for all CPUs : 0.008441 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:44:24] 0.040042 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:44:39] Energy consumed for RAM : 0.002257 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:44:39] Energy consumed for all GPUs : 0.029698 kWh. Total GPU Power : 78.97110812652943 W\n",
      "[codecarbon INFO @ 15:44:39] Energy consumed for all CPUs : 0.008535 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:44:39] 0.040489 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:44:54] Energy consumed for RAM : 0.002281 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:44:54] Energy consumed for all GPUs : 0.030027 kWh. Total GPU Power : 78.99083998016384 W\n",
      "[codecarbon INFO @ 15:44:54] Energy consumed for all CPUs : 0.008629 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:44:54] 0.040937 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:45:09] Energy consumed for RAM : 0.002306 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:45:09] Energy consumed for all GPUs : 0.030358 kWh. Total GPU Power : 79.54280425014028 W\n",
      "[codecarbon INFO @ 15:45:09] Energy consumed for all CPUs : 0.008723 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:45:09] 0.041387 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:45:24] Energy consumed for RAM : 0.002331 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:45:24] Energy consumed for all GPUs : 0.030687 kWh. Total GPU Power : 78.94664304950108 W\n",
      "[codecarbon INFO @ 15:45:24] Energy consumed for all CPUs : 0.008816 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:45:24] 0.041835 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:45:39] Energy consumed for RAM : 0.002356 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:45:39] Energy consumed for all GPUs : 0.031019 kWh. Total GPU Power : 79.46822809504913 W\n",
      "[codecarbon INFO @ 15:45:39] Energy consumed for all CPUs : 0.008910 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:45:39] 0.042285 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:45:54] Energy consumed for RAM : 0.002381 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:45:54] Energy consumed for all GPUs : 0.031348 kWh. Total GPU Power : 78.96864722816528 W\n",
      "[codecarbon INFO @ 15:45:54] Energy consumed for all CPUs : 0.009004 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:45:54] 0.042732 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:45:54] 0.007076 g.CO2eq/s mean an estimation of 223.14840496504885 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:46:09] Energy consumed for RAM : 0.002405 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:46:09] Energy consumed for all GPUs : 0.031679 kWh. Total GPU Power : 79.45563633052387 W\n",
      "[codecarbon INFO @ 15:46:09] Energy consumed for all CPUs : 0.009098 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:46:09] 0.043182 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:46:24] Energy consumed for RAM : 0.002430 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:46:24] Energy consumed for all GPUs : 0.032008 kWh. Total GPU Power : 79.03836494395408 W\n",
      "[codecarbon INFO @ 15:46:24] Energy consumed for all CPUs : 0.009192 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:46:24] 0.043630 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:46:39] Energy consumed for RAM : 0.002455 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:46:39] Energy consumed for all GPUs : 0.032337 kWh. Total GPU Power : 78.80101153032172 W\n",
      "[codecarbon INFO @ 15:46:39] Energy consumed for all CPUs : 0.009286 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:46:39] 0.044078 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:46:54] Energy consumed for RAM : 0.002480 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:46:54] Energy consumed for all GPUs : 0.032669 kWh. Total GPU Power : 79.53233841615075 W\n",
      "[codecarbon INFO @ 15:46:54] Energy consumed for all CPUs : 0.009379 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:46:54] 0.044528 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:47:09] Energy consumed for RAM : 0.002505 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:47:09] Energy consumed for all GPUs : 0.032998 kWh. Total GPU Power : 78.92601758696411 W\n",
      "[codecarbon INFO @ 15:47:09] Energy consumed for all CPUs : 0.009473 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:47:09] 0.044975 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:47:24] Energy consumed for RAM : 0.002529 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:47:24] Energy consumed for all GPUs : 0.033329 kWh. Total GPU Power : 79.45316099550679 W\n",
      "[codecarbon INFO @ 15:47:24] Energy consumed for all CPUs : 0.009567 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:47:24] 0.045425 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:47:39] Energy consumed for RAM : 0.002554 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:47:39] Energy consumed for all GPUs : 0.033658 kWh. Total GPU Power : 78.96185990370037 W\n",
      "[codecarbon INFO @ 15:47:39] Energy consumed for all CPUs : 0.009661 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:47:39] 0.045873 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:47:54] Energy consumed for RAM : 0.002579 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:47:54] Energy consumed for all GPUs : 0.033990 kWh. Total GPU Power : 79.56674016238118 W\n",
      "[codecarbon INFO @ 15:47:54] Energy consumed for all CPUs : 0.009755 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:47:54] 0.046323 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:47:54] 0.007104 g.CO2eq/s mean an estimation of 224.0336561788265 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:48:09] Energy consumed for RAM : 0.002604 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:48:09] Energy consumed for all GPUs : 0.034319 kWh. Total GPU Power : 78.93000160444345 W\n",
      "[codecarbon INFO @ 15:48:09] Energy consumed for all CPUs : 0.009848 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:48:09] 0.046771 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:48:24] Energy consumed for RAM : 0.002629 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:48:24] Energy consumed for all GPUs : 0.034650 kWh. Total GPU Power : 79.39038294923706 W\n",
      "[codecarbon INFO @ 15:48:24] Energy consumed for all CPUs : 0.009942 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:48:24] 0.047221 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:48:39] Energy consumed for RAM : 0.002653 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:48:39] Energy consumed for all GPUs : 0.034979 kWh. Total GPU Power : 79.07908962329311 W\n",
      "[codecarbon INFO @ 15:48:39] Energy consumed for all CPUs : 0.010036 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:48:39] 0.047669 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:48:54] Energy consumed for RAM : 0.002678 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:48:54] Energy consumed for all GPUs : 0.035308 kWh. Total GPU Power : 78.94318909997438 W\n",
      "[codecarbon INFO @ 15:48:54] Energy consumed for all CPUs : 0.010130 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:48:54] 0.048116 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:49:09] Energy consumed for RAM : 0.002703 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:49:09] Energy consumed for all GPUs : 0.035640 kWh. Total GPU Power : 79.51018041714131 W\n",
      "[codecarbon INFO @ 15:49:09] Energy consumed for all CPUs : 0.010224 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:49:09] 0.048566 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:49:24] Energy consumed for RAM : 0.002728 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:49:24] Energy consumed for all GPUs : 0.035969 kWh. Total GPU Power : 78.92758067637224 W\n",
      "[codecarbon INFO @ 15:49:24] Energy consumed for all CPUs : 0.010317 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:49:24] 0.049014 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:49:39] Energy consumed for RAM : 0.002753 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:49:39] Energy consumed for all GPUs : 0.036298 kWh. Total GPU Power : 78.9538483417375 W\n",
      "[codecarbon INFO @ 15:49:39] Energy consumed for all CPUs : 0.010411 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:49:39] 0.049461 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:49:54] Energy consumed for RAM : 0.002777 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:49:54] Energy consumed for all GPUs : 0.036629 kWh. Total GPU Power : 79.47798523344574 W\n",
      "[codecarbon INFO @ 15:49:54] Energy consumed for all CPUs : 0.010505 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:49:54] 0.049911 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:49:54] 0.007100 g.CO2eq/s mean an estimation of 223.89826631840103 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:50:09] Energy consumed for RAM : 0.002802 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:50:09] Energy consumed for all GPUs : 0.036958 kWh. Total GPU Power : 79.02273406707896 W\n",
      "[codecarbon INFO @ 15:50:09] Energy consumed for all CPUs : 0.010599 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:50:09] 0.050359 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:50:24] Energy consumed for RAM : 0.002827 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:50:24] Energy consumed for all GPUs : 0.037290 kWh. Total GPU Power : 79.59599930709612 W\n",
      "[codecarbon INFO @ 15:50:24] Energy consumed for all CPUs : 0.010692 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:50:24] 0.050809 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:50:39] Energy consumed for RAM : 0.002852 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:50:39] Energy consumed for all GPUs : 0.037619 kWh. Total GPU Power : 78.8040134001753 W\n",
      "[codecarbon INFO @ 15:50:39] Energy consumed for all CPUs : 0.010786 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:50:39] 0.051257 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3517\n",
      "  Batch size = 8\n",
      "[codecarbon INFO @ 15:50:54] Energy consumed for RAM : 0.002876 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:50:54] Energy consumed for all GPUs : 0.037950 kWh. Total GPU Power : 79.61566531777434 W\n",
      "[codecarbon INFO @ 15:50:54] Energy consumed for all CPUs : 0.010880 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:50:54] 0.051707 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:51:09] Energy consumed for RAM : 0.002901 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:51:09] Energy consumed for all GPUs : 0.038279 kWh. Total GPU Power : 78.82431607644405 W\n",
      "[codecarbon INFO @ 15:51:09] Energy consumed for all CPUs : 0.010974 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:51:09] 0.052154 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:51:24] Energy consumed for RAM : 0.002926 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:51:24] Energy consumed for all GPUs : 0.038607 kWh. Total GPU Power : 78.74182289393359 W\n",
      "[codecarbon INFO @ 15:51:24] Energy consumed for all CPUs : 0.011068 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:51:24] 0.052601 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Biased     0.7702    0.8116    0.7904      1805\n",
      "      Biased     0.7895    0.7447    0.7665      1712\n",
      "\n",
      "    accuracy                         0.7791      3517\n",
      "   macro avg     0.7799    0.7782    0.7784      3517\n",
      "weighted avg     0.7796    0.7791    0.7787      3517\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-1757\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-1757\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-1757\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-1757\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-1757\\special_tokens_map.json\n",
      "Deleting older checkpoint [D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-1317] due to args.save_total_limit\n",
      "[codecarbon INFO @ 15:51:39] Energy consumed for RAM : 0.002951 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:51:39] Energy consumed for all GPUs : 0.038924 kWh. Total GPU Power : 76.19648721208452 W\n",
      "[codecarbon INFO @ 15:51:39] Energy consumed for all CPUs : 0.011161 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:51:39] 0.053037 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:51:54] Energy consumed for RAM : 0.002976 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:51:54] Energy consumed for all GPUs : 0.039254 kWh. Total GPU Power : 79.0119090002599 W\n",
      "[codecarbon INFO @ 15:51:54] Energy consumed for all CPUs : 0.011255 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:51:54] 0.053484 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:51:54] 0.007072 g.CO2eq/s mean an estimation of 223.0121554171448 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:52:09] Energy consumed for RAM : 0.003000 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:52:09] Energy consumed for all GPUs : 0.039586 kWh. Total GPU Power : 79.64200092384458 W\n",
      "[codecarbon INFO @ 15:52:09] Energy consumed for all CPUs : 0.011349 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:52:09] 0.053935 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:52:24] Energy consumed for RAM : 0.003025 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:52:24] Energy consumed for all GPUs : 0.039917 kWh. Total GPU Power : 79.33796151639689 W\n",
      "[codecarbon INFO @ 15:52:24] Energy consumed for all CPUs : 0.011443 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:52:24] 0.054385 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:52:39] Energy consumed for RAM : 0.003050 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:52:39] Energy consumed for all GPUs : 0.040247 kWh. Total GPU Power : 79.16727154728102 W\n",
      "[codecarbon INFO @ 15:52:39] Energy consumed for all CPUs : 0.011537 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:52:39] 0.054833 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:52:54] Energy consumed for RAM : 0.003075 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:52:54] Energy consumed for all GPUs : 0.040576 kWh. Total GPU Power : 78.98650181635783 W\n",
      "[codecarbon INFO @ 15:52:54] Energy consumed for all CPUs : 0.011630 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:52:54] 0.055281 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:53:09] Energy consumed for RAM : 0.003100 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:53:09] Energy consumed for all GPUs : 0.040907 kWh. Total GPU Power : 79.64522937708985 W\n",
      "[codecarbon INFO @ 15:53:09] Energy consumed for all CPUs : 0.011724 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:53:09] 0.055731 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:53:24] Energy consumed for RAM : 0.003124 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:53:24] Energy consumed for all GPUs : 0.041236 kWh. Total GPU Power : 78.880824563503 W\n",
      "[codecarbon INFO @ 15:53:24] Energy consumed for all CPUs : 0.011818 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:53:24] 0.056178 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:53:39] Energy consumed for RAM : 0.003149 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:53:39] Energy consumed for all GPUs : 0.041567 kWh. Total GPU Power : 79.48929358775236 W\n",
      "[codecarbon INFO @ 15:53:39] Energy consumed for all CPUs : 0.011912 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:53:39] 0.056628 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:53:54] Energy consumed for RAM : 0.003174 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:53:54] Energy consumed for all GPUs : 0.041897 kWh. Total GPU Power : 78.9659315178514 W\n",
      "[codecarbon INFO @ 15:53:54] Energy consumed for all CPUs : 0.012006 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:53:54] 0.057077 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:53:54] 0.007107 g.CO2eq/s mean an estimation of 224.1176847277399 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:54:09] Energy consumed for RAM : 0.003199 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:54:09] Energy consumed for all GPUs : 0.042226 kWh. Total GPU Power : 79.02420574334053 W\n",
      "[codecarbon INFO @ 15:54:09] Energy consumed for all CPUs : 0.012099 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:54:09] 0.057524 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:54:24] Energy consumed for RAM : 0.003224 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:54:24] Energy consumed for all GPUs : 0.042557 kWh. Total GPU Power : 79.4875824841921 W\n",
      "[codecarbon INFO @ 15:54:24] Energy consumed for all CPUs : 0.012193 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:54:24] 0.057974 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:54:39] Energy consumed for RAM : 0.003248 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:54:39] Energy consumed for all GPUs : 0.042886 kWh. Total GPU Power : 78.9996100465605 W\n",
      "[codecarbon INFO @ 15:54:39] Energy consumed for all CPUs : 0.012287 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:54:39] 0.058422 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:54:54] Energy consumed for RAM : 0.003273 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:54:54] Energy consumed for all GPUs : 0.043218 kWh. Total GPU Power : 79.43428228718038 W\n",
      "[codecarbon INFO @ 15:54:54] Energy consumed for all CPUs : 0.012381 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:54:54] 0.058872 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:55:09] Energy consumed for RAM : 0.003298 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:55:09] Energy consumed for all GPUs : 0.043546 kWh. Total GPU Power : 78.8399937293195 W\n",
      "[codecarbon INFO @ 15:55:09] Energy consumed for all CPUs : 0.012475 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:55:09] 0.059319 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:55:24] Energy consumed for RAM : 0.003323 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:55:24] Energy consumed for all GPUs : 0.043879 kWh. Total GPU Power : 79.78520463891331 W\n",
      "[codecarbon INFO @ 15:55:24] Energy consumed for all CPUs : 0.012568 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:55:24] 0.059770 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:55:39] Energy consumed for RAM : 0.003348 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:55:39] Energy consumed for all GPUs : 0.044208 kWh. Total GPU Power : 79.02635527667002 W\n",
      "[codecarbon INFO @ 15:55:39] Energy consumed for all CPUs : 0.012662 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:55:39] 0.060218 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:55:54] Energy consumed for RAM : 0.003372 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:55:54] Energy consumed for all GPUs : 0.044537 kWh. Total GPU Power : 79.03533137939799 W\n",
      "[codecarbon INFO @ 15:55:54] Energy consumed for all CPUs : 0.012756 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:55:54] 0.060666 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:55:54] 0.007103 g.CO2eq/s mean an estimation of 224.0124171234428 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:56:09] Energy consumed for RAM : 0.003397 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:56:09] Energy consumed for all GPUs : 0.044869 kWh. Total GPU Power : 79.61011162014454 W\n",
      "[codecarbon INFO @ 15:56:09] Energy consumed for all CPUs : 0.012850 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:56:09] 0.061116 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:56:24] Energy consumed for RAM : 0.003422 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:56:24] Energy consumed for all GPUs : 0.045198 kWh. Total GPU Power : 78.95458320948288 W\n",
      "[codecarbon INFO @ 15:56:24] Energy consumed for all CPUs : 0.012943 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:56:24] 0.061564 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:56:39] Energy consumed for RAM : 0.003447 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:56:39] Energy consumed for all GPUs : 0.045528 kWh. Total GPU Power : 79.11738077012716 W\n",
      "[codecarbon INFO @ 15:56:39] Energy consumed for all CPUs : 0.013037 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:56:39] 0.062012 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:56:54] Energy consumed for RAM : 0.003472 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:56:54] Energy consumed for all GPUs : 0.045859 kWh. Total GPU Power : 79.53254816749066 W\n",
      "[codecarbon INFO @ 15:56:54] Energy consumed for all CPUs : 0.013131 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:56:54] 0.062462 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:57:09] Energy consumed for RAM : 0.003496 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:57:09] Energy consumed for all GPUs : 0.046189 kWh. Total GPU Power : 79.04587749320727 W\n",
      "[codecarbon INFO @ 15:57:09] Energy consumed for all CPUs : 0.013225 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:57:09] 0.062910 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:57:24] Energy consumed for RAM : 0.003521 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:57:24] Energy consumed for all GPUs : 0.046521 kWh. Total GPU Power : 79.64680557453852 W\n",
      "[codecarbon INFO @ 15:57:24] Energy consumed for all CPUs : 0.013319 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:57:24] 0.063361 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:57:39] Energy consumed for RAM : 0.003546 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:57:39] Energy consumed for all GPUs : 0.046850 kWh. Total GPU Power : 78.97774307350451 W\n",
      "[codecarbon INFO @ 15:57:39] Energy consumed for all CPUs : 0.013412 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:57:39] 0.063808 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:57:54] Energy consumed for RAM : 0.003571 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:57:54] Energy consumed for all GPUs : 0.047182 kWh. Total GPU Power : 79.59990650118944 W\n",
      "[codecarbon INFO @ 15:57:54] Energy consumed for all CPUs : 0.013506 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:57:54] 0.064259 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:57:54] 0.007111 g.CO2eq/s mean an estimation of 224.24026905393345 kg.CO2eq/year\n",
      "[codecarbon INFO @ 15:58:09] Energy consumed for RAM : 0.003596 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:58:09] Energy consumed for all GPUs : 0.047511 kWh. Total GPU Power : 78.96528036006816 W\n",
      "[codecarbon INFO @ 15:58:09] Energy consumed for all CPUs : 0.013600 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:58:09] 0.064707 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3517\n",
      "  Batch size = 8\n",
      "[codecarbon INFO @ 15:58:24] Energy consumed for RAM : 0.003620 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:58:24] Energy consumed for all GPUs : 0.047840 kWh. Total GPU Power : 78.94533616353905 W\n",
      "[codecarbon INFO @ 15:58:24] Energy consumed for all CPUs : 0.013694 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:58:24] 0.065154 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:58:39] Energy consumed for RAM : 0.003645 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:58:39] Energy consumed for all GPUs : 0.048169 kWh. Total GPU Power : 78.84617542538787 W\n",
      "[codecarbon INFO @ 15:58:39] Energy consumed for all CPUs : 0.013787 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:58:39] 0.065601 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Biased     0.8436    0.7380    0.7872      1805\n",
      "      Biased     0.7559    0.8557    0.8027      1712\n",
      "\n",
      "    accuracy                         0.7953      3517\n",
      "   macro avg     0.7998    0.7968    0.7950      3517\n",
      "weighted avg     0.8009    0.7953    0.7948      3517\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-2196\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-2196\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-2196\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-2196\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-2196\\special_tokens_map.json\n",
      "Deleting older checkpoint [D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-2195] due to args.save_total_limit\n",
      "[codecarbon INFO @ 15:58:54] Energy consumed for RAM : 0.003670 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:58:54] Energy consumed for all GPUs : 0.048489 kWh. Total GPU Power : 76.79546395621384 W\n",
      "[codecarbon INFO @ 15:58:54] Energy consumed for all CPUs : 0.013881 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:58:54] 0.066040 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:59:09] Energy consumed for RAM : 0.003695 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:59:09] Energy consumed for all GPUs : 0.048817 kWh. Total GPU Power : 78.79204373777394 W\n",
      "[codecarbon INFO @ 15:59:09] Energy consumed for all CPUs : 0.013975 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:59:09] 0.066487 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:59:24] Energy consumed for RAM : 0.003720 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:59:24] Energy consumed for all GPUs : 0.049148 kWh. Total GPU Power : 79.45542919943726 W\n",
      "[codecarbon INFO @ 15:59:24] Energy consumed for all CPUs : 0.014069 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:59:24] 0.066937 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:59:39] Energy consumed for RAM : 0.003744 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:59:39] Energy consumed for all GPUs : 0.049477 kWh. Total GPU Power : 78.87226454719453 W\n",
      "[codecarbon INFO @ 15:59:39] Energy consumed for all CPUs : 0.014163 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:59:39] 0.067384 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:59:54] Energy consumed for RAM : 0.003769 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 15:59:54] Energy consumed for all GPUs : 0.049806 kWh. Total GPU Power : 79.00298933720823 W\n",
      "[codecarbon INFO @ 15:59:54] Energy consumed for all CPUs : 0.014256 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 15:59:54] 0.067832 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:59:54] 0.007071 g.CO2eq/s mean an estimation of 222.9859886855327 kg.CO2eq/year\n",
      "[codecarbon INFO @ 16:00:09] Energy consumed for RAM : 0.003794 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:00:09] Energy consumed for all GPUs : 0.050138 kWh. Total GPU Power : 79.50343506370517 W\n",
      "[codecarbon INFO @ 16:00:09] Energy consumed for all CPUs : 0.014350 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:00:09] 0.068282 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:00:24] Energy consumed for RAM : 0.003819 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:00:24] Energy consumed for all GPUs : 0.050467 kWh. Total GPU Power : 78.95651861387506 W\n",
      "[codecarbon INFO @ 16:00:24] Energy consumed for all CPUs : 0.014444 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:00:24] 0.068729 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:00:39] Energy consumed for RAM : 0.003844 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:00:39] Energy consumed for all GPUs : 0.050798 kWh. Total GPU Power : 79.47245496683617 W\n",
      "[codecarbon INFO @ 16:00:39] Energy consumed for all CPUs : 0.014538 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:00:39] 0.069179 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:00:54] Energy consumed for RAM : 0.003868 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:00:54] Energy consumed for all GPUs : 0.051128 kWh. Total GPU Power : 79.06791609305269 W\n",
      "[codecarbon INFO @ 16:00:54] Energy consumed for all CPUs : 0.014632 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:00:54] 0.069628 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:01:09] Energy consumed for RAM : 0.003893 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:01:09] Energy consumed for all GPUs : 0.051457 kWh. Total GPU Power : 79.0749678589045 W\n",
      "[codecarbon INFO @ 16:01:09] Energy consumed for all CPUs : 0.014725 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:01:09] 0.070076 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:01:24] Energy consumed for RAM : 0.003918 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:01:24] Energy consumed for all GPUs : 0.051788 kWh. Total GPU Power : 79.47071648962786 W\n",
      "[codecarbon INFO @ 16:01:24] Energy consumed for all CPUs : 0.014819 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:01:24] 0.070525 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:01:39] Energy consumed for RAM : 0.003943 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:01:39] Energy consumed for all GPUs : 0.052118 kWh. Total GPU Power : 79.06784959916922 W\n",
      "[codecarbon INFO @ 16:01:39] Energy consumed for all CPUs : 0.014913 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:01:39] 0.070973 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:01:54] Energy consumed for RAM : 0.003967 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:01:54] Energy consumed for all GPUs : 0.052447 kWh. Total GPU Power : 78.94999665798674 W\n",
      "[codecarbon INFO @ 16:01:54] Energy consumed for all CPUs : 0.015007 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:01:54] 0.071421 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:01:54] 0.007103 g.CO2eq/s mean an estimation of 223.98515251888657 kg.CO2eq/year\n",
      "[codecarbon INFO @ 16:02:09] Energy consumed for RAM : 0.003992 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:02:09] Energy consumed for all GPUs : 0.052778 kWh. Total GPU Power : 79.5384240929461 W\n",
      "[codecarbon INFO @ 16:02:09] Energy consumed for all CPUs : 0.015101 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:02:09] 0.071871 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:02:24] Energy consumed for RAM : 0.004017 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:02:24] Energy consumed for all GPUs : 0.053107 kWh. Total GPU Power : 78.90485200508768 W\n",
      "[codecarbon INFO @ 16:02:24] Energy consumed for all CPUs : 0.015194 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:02:24] 0.072319 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:02:39] Energy consumed for RAM : 0.004042 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:02:39] Energy consumed for all GPUs : 0.053439 kWh. Total GPU Power : 79.49617570420583 W\n",
      "[codecarbon INFO @ 16:02:39] Energy consumed for all CPUs : 0.015288 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:02:39] 0.072769 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:02:54] Energy consumed for RAM : 0.004067 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:02:54] Energy consumed for all GPUs : 0.053768 kWh. Total GPU Power : 78.9395447767703 W\n",
      "[codecarbon INFO @ 16:02:54] Energy consumed for all CPUs : 0.015382 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:02:54] 0.073216 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:03:09] Energy consumed for RAM : 0.004092 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:03:09] Energy consumed for all GPUs : 0.054097 kWh. Total GPU Power : 78.94512926023248 W\n",
      "[codecarbon INFO @ 16:03:09] Energy consumed for all CPUs : 0.015476 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:03:09] 0.073664 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:03:24] Energy consumed for RAM : 0.004116 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:03:24] Energy consumed for all GPUs : 0.054428 kWh. Total GPU Power : 79.50559099952983 W\n",
      "[codecarbon INFO @ 16:03:24] Energy consumed for all CPUs : 0.015570 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:03:24] 0.074114 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:03:39] Energy consumed for RAM : 0.004141 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:03:39] Energy consumed for all GPUs : 0.054758 kWh. Total GPU Power : 79.02680006523289 W\n",
      "[codecarbon INFO @ 16:03:39] Energy consumed for all CPUs : 0.015663 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:03:39] 0.074562 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:03:54] Energy consumed for RAM : 0.004166 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:03:54] Energy consumed for all GPUs : 0.055089 kWh. Total GPU Power : 79.47321059370464 W\n",
      "[codecarbon INFO @ 16:03:54] Energy consumed for all CPUs : 0.015757 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:03:54] 0.075012 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:03:54] 0.007105 g.CO2eq/s mean an estimation of 224.0630716183665 kg.CO2eq/year\n",
      "[codecarbon INFO @ 16:04:09] Energy consumed for RAM : 0.004191 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:04:09] Energy consumed for all GPUs : 0.055417 kWh. Total GPU Power : 78.83323706998551 W\n",
      "[codecarbon INFO @ 16:04:09] Energy consumed for all CPUs : 0.015851 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:04:09] 0.075459 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:04:24] Energy consumed for RAM : 0.004216 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:04:24] Energy consumed for all GPUs : 0.055747 kWh. Total GPU Power : 79.05016601301999 W\n",
      "[codecarbon INFO @ 16:04:24] Energy consumed for all CPUs : 0.015945 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:04:24] 0.075907 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:04:40] Energy consumed for RAM : 0.004240 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:04:40] Energy consumed for all GPUs : 0.056078 kWh. Total GPU Power : 79.43769999730027 W\n",
      "[codecarbon INFO @ 16:04:40] Energy consumed for all CPUs : 0.016039 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:04:40] 0.076357 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:04:55] Energy consumed for RAM : 0.004265 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:04:55] Energy consumed for all GPUs : 0.056408 kWh. Total GPU Power : 79.0424192184934 W\n",
      "[codecarbon INFO @ 16:04:55] Energy consumed for all CPUs : 0.016132 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:04:55] 0.076805 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:05:10] Energy consumed for RAM : 0.004290 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:05:10] Energy consumed for all GPUs : 0.056739 kWh. Total GPU Power : 79.61555207403592 W\n",
      "[codecarbon INFO @ 16:05:10] Energy consumed for all CPUs : 0.016226 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:05:10] 0.077255 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:05:25] Energy consumed for RAM : 0.004315 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:05:25] Energy consumed for all GPUs : 0.057068 kWh. Total GPU Power : 78.90968101091767 W\n",
      "[codecarbon INFO @ 16:05:25] Energy consumed for all CPUs : 0.016320 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:05:25] 0.077703 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-2634\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-2634\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-2634\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-2634\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-2634\\special_tokens_map.json\n",
      "Deleting older checkpoint [D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-2196] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3517\n",
      "  Batch size = 8\n",
      "[codecarbon INFO @ 16:05:40] Energy consumed for RAM : 0.004340 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:05:40] Energy consumed for all GPUs : 0.057386 kWh. Total GPU Power : 76.0792595312848 W\n",
      "[codecarbon INFO @ 16:05:40] Energy consumed for all CPUs : 0.016414 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:05:40] 0.078139 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:05:55] Energy consumed for RAM : 0.004364 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:05:55] Energy consumed for all GPUs : 0.057716 kWh. Total GPU Power : 79.36452339168844 W\n",
      "[codecarbon INFO @ 16:05:55] Energy consumed for all CPUs : 0.016508 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:05:55] 0.078588 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 16:05:55] 0.007076 g.CO2eq/s mean an estimation of 223.14457044551432 kg.CO2eq/year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-Biased     0.8675    0.7618    0.8112      1805\n",
      "      Biased     0.7774    0.8773    0.8244      1712\n",
      "\n",
      "    accuracy                         0.8180      3517\n",
      "   macro avg     0.8225    0.8196    0.8178      3517\n",
      "weighted avg     0.8237    0.8180    0.8176      3517\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-2634\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-2634\\config.json\n",
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-2634\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-2634\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-2634\\special_tokens_map.json\n",
      "Deleting older checkpoint [D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-1757] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\checkpoint-2634 (score: 0.44443047046661377).\n",
      "Saving model checkpoint to D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\n",
      "Configuration saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\model.safetensors\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\special_tokens_map.json\n",
      "tokenizer config file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\tokenizer_config.json\n",
      "Special tokens file saved in D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\\special_tokens_map.json\n",
      "[codecarbon INFO @ 16:06:05] Energy consumed for RAM : 0.004382 kWh. RAM Power : 5.951219558715821 W\n",
      "[codecarbon INFO @ 16:06:05] Energy consumed for all GPUs : 0.057927 kWh. Total GPU Power : 71.84870731090284 W\n",
      "[codecarbon INFO @ 16:06:05] Energy consumed for all CPUs : 0.016574 kWh. Total CPU Power : 22.5 W\n",
      "[codecarbon INFO @ 16:06:05] 0.078882 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbon dioxide emission: 18.741605 g\n"
     ]
    }
   ],
   "source": [
    "# Train BERT\n",
    "\n",
    "bert_model_dir = train_model(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    model_path='google-bert/bert-base-uncased',\n",
    "    batch_size=TRAINING_CONFIG['batch_size'],\n",
    "    epoch=TRAINING_CONFIG['epochs'],\n",
    "    learning_rate=TRAINING_CONFIG['learning_rate'],\n",
    "    gradient_accumulation_steps=TRAINING_CONFIG['gradient_accumulation_steps'],\n",
    "    dataset_name='job_descriptions',\n",
    "    seed=TRAINING_CONFIG['seed'],\n",
    "    save_model=TRAINING_CONFIG['save_model'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ALL MODELS TRAINED SUCCESSFULLY\n",
      "============================================================\n",
      "ALBERT-V2: D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\albert_albert-base-v2\n",
      "DistilBERT: D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\distilbert_distilbert-base-uncased\n",
      "BERT: D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\google-bert_bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ALL MODELS TRAINED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ALBERT-V2: {albert_model_dir}\")\n",
    "print(f\"DistilBERT: {distilbert_model_dir}\")\n",
    "print(f\"BERT: {bert_model_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (HEARTS)",
   "language": "python",
   "name": "hearts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
