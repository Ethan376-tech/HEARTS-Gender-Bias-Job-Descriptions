{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Explainability: SHAP and LIME Analysis\n",
    "\n",
    "**Project:** HEARTS Adaptation - Gender Bias Detection  \n",
    "**Task:** Token-level explainability using SHAP and LIME\n",
    "\n",
    "This notebook implements:\n",
    "1. SHAP token-level importance calculations\n",
    "2. LIME token-level importance calculations\n",
    "3. Similarity metrics between SHAP and LIME:\n",
    "   - Cosine Similarity\n",
    "   - Pearson Correlation\n",
    "   - Jensen-Shannon Divergence\n",
    "4. Token rankings for correct and incorrect predictions\n",
    "5. Visualization of explainability results\n",
    "\n",
    "**Reference:** King, T., Wu, Z., Koshiyama, A., Kazim, E., & Treleaven, P. (2024). Hearts: A holistic framework for explainable, sustainable and robust text stereotype detection. arXiv preprint arXiv:2409.11579.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# SHAP and LIME\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lime_text\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlime_text\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LimeTextExplainer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# SHAP and LIME\n",
    "import shap\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "# Set up paths\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'notebooks':\n",
    "    project_root = current_dir.parent\n",
    "else:\n",
    "    project_root = current_dir\n",
    "\n",
    "data_dir = project_root / 'data'\n",
    "models_dir = project_root / 'models'\n",
    "results_dir = project_root / 'results'\n",
    "explainability_dir = project_root / 'explainability'\n",
    "paper_figures_dir = explainability_dir / 'paper_figures'\n",
    "generated_figures_dir = explainability_dir / 'generated_figures'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(explainability_dir, exist_ok=True)\n",
    "os.makedirs(paper_figures_dir, exist_ok=True)\n",
    "os.makedirs(generated_figures_dir, exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(f\"\\nProject root: {project_root}\")\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Models directory: {models_dir}\")\n",
    "print(f\"Explainability directory: {explainability_dir}\")\n",
    "print(f\"Paper figures directory: {paper_figures_dir}\")\n",
    "print(f\"Generated figures directory: {generated_figures_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data and Model\n",
    "\n",
    "Load the test data and the fine-tuned ALBERT-V2 model (as per the paper):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Load test data\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m test_data \u001b[38;5;241m=\u001b[39m load_test_data(data_dir)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Load ALBERT-V2 model (as per paper)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m albert_model_dir \u001b[38;5;241m=\u001b[39m models_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_descriptions\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malbert_albert-base-v2\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "def load_test_data(data_dir=None):\n",
    "    \"\"\"Load preprocessed test data\"\"\"\n",
    "    if data_dir is None:\n",
    "        data_dir = project_root / 'data'\n",
    "    \n",
    "    test_path = data_dir / 'splits' / 'test.csv'\n",
    "    \n",
    "    if not test_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Test data not found. Please run 01_Data_Loading_Preprocessing.ipynb first.\\n\"\n",
    "            f\"Expected file: {test_path}\"\n",
    "        )\n",
    "    \n",
    "    test_data = pd.read_csv(test_path)\n",
    "    print(f\"Loaded test data: {len(test_data)} examples\")\n",
    "    print(f\"\\nTest label distribution:\")\n",
    "    print(test_data['label'].value_counts().sort_index())\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "# Load model\n",
    "def load_model_for_explainability(model_dir, device='cpu'):\n",
    "    \"\"\"Load a fine-tuned model for explainability analysis\"\"\"\n",
    "    print(f\"\\nLoading model from: {model_dir}\")\n",
    "    \n",
    "    num_labels = 2  # Binary classification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_dir,\n",
    "        num_labels=num_labels,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Model loaded successfully on {device}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# Load test data\n",
    "test_data = load_test_data(data_dir)\n",
    "\n",
    "# Load ALBERT-V2 model (as per paper)\n",
    "albert_model_dir = models_dir / 'job_descriptions' / 'albert_albert-base-v2'\n",
    "if not albert_model_dir.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"ALBERT-V2 model not found. Please run 02_Model_Training.ipynb first.\\n\"\n",
    "        f\"Expected directory: {albert_model_dir}\"\n",
    "    )\n",
    "\n",
    "model, tokenizer = load_model_for_explainability(str(albert_model_dir), device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Regex Tokenizer\n",
    "\n",
    "For consistency between SHAP and LIME, we use a custom regex tokenizer as mentioned in the paper:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_regex_tokenizer(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Custom regex tokenizer for consistent tokenization between SHAP and LIME.\n",
    "    This tokenizer splits text on whitespace and punctuation while preserving tokens.\n",
    "    \"\"\"\n",
    "    # Split on whitespace and punctuation, but keep the tokens\n",
    "    tokens = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n",
    "    return tokens\n",
    "\n",
    "# Test the tokenizer\n",
    "sample_text = \"We are looking for a strong leader who can manage teams effectively.\"\n",
    "tokens = custom_regex_tokenizer(sample_text)\n",
    "print(f\"Sample text: {sample_text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction Function\n",
    "\n",
    "Wrapper function for model predictions that will be used by SHAP and LIME:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(texts: List[str], model, tokenizer, device='cpu') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predict probability of positive class (biased) for a list of texts.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    texts : List[str]\n",
    "        List of input texts\n",
    "    model : AutoModelForSequenceClassification\n",
    "        Fine-tuned model\n",
    "    tokenizer : AutoTokenizer\n",
    "        Tokenizer\n",
    "    device : str\n",
    "        Device to run inference on\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    probs : np.ndarray\n",
    "        Array of probabilities for positive class (shape: [len(texts)])\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    # Tokenize\n",
    "    encoded = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # Return probability of positive class (label 1)\n",
    "        positive_probs = probs[:, 1].cpu().numpy()\n",
    "    \n",
    "    return positive_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Prediction Function\n",
    "\n",
    "Test the prediction function after model is loaded:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the prediction function (requires model and tokenizer from Cell 3)\n",
    "if 'model' in globals() and 'tokenizer' in globals():\n",
    "    sample_texts = [\"We are looking for a strong leader.\", \"The team needs a collaborative member.\"]\n",
    "    probs = model_predict(sample_texts, model, tokenizer, device=device)\n",
    "    print(f\"Sample predictions:\")\n",
    "    for text, prob in zip(sample_texts, probs):\n",
    "        print(f\"  Text: {text}\")\n",
    "        print(f\"  Probability (biased): {prob:.4f}\")\n",
    "else:\n",
    "    print(\"Note: Model and tokenizer not loaded yet. Please run Cell 3 first to load the model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Token-Level Importance\n",
    "\n",
    "Calculate SHAP values for token-level importance. According to the paper, SHAP values are calculated using:\n",
    "\n",
    "$$\\phi_{ij} = \\sum_{S \\subseteq N_i \\setminus \\{j\\}} \\frac{|S|!(|N_i| - |S| - 1)!}{|N_i|!} [f_i(S \\cup \\{j\\}) - f_i(S)]$$\n",
    "\n",
    "where $\\phi_i = (\\phi_{i1}, \\phi_{i2}, \\ldots, \\phi_{iN})$ is the SHAP vector for instance $i$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shap_values(\n",
    "    texts: List[str],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device='cpu',\n",
    "    max_evals: int = 100,\n",
    "    batch_size: int = 10\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate SHAP values for a list of texts.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    texts : List[str]\n",
    "        List of input texts\n",
    "    model : AutoModelForSequenceClassification\n",
    "        Fine-tuned model\n",
    "    tokenizer : AutoTokenizer\n",
    "        Tokenizer\n",
    "    device : str\n",
    "        Device to run inference on\n",
    "    max_evals : int\n",
    "        Maximum number of evaluations for SHAP (default: 100)\n",
    "    batch_size : int\n",
    "        Batch size for processing\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    shap_values_list : List[np.ndarray]\n",
    "        List of SHAP value arrays, one per text\n",
    "    \"\"\"\n",
    "    # Create a wrapper function for SHAP\n",
    "    def predict_wrapper(texts_input):\n",
    "        \"\"\"Wrapper for model prediction that SHAP can use\"\"\"\n",
    "        if isinstance(texts_input, str):\n",
    "            texts_input = [texts_input]\n",
    "        return model_predict(texts_input, model, tokenizer, device)\n",
    "    \n",
    "    # Create SHAP explainer\n",
    "    # Using TextExplainer with custom tokenizer\n",
    "    explainer = shap.Explainer(predict_wrapper, tokenizer, output_names=['Non-Biased', 'Biased'])\n",
    "    \n",
    "    shap_values_list = []\n",
    "    \n",
    "    print(f\"Calculating SHAP values for {len(texts)} texts...\")\n",
    "    for i, text in enumerate(texts):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Processing text {i+1}/{len(texts)}\")\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        shap_values = explainer([text], max_evals=max_evals)\n",
    "        \n",
    "        # Extract values for positive class (biased)\n",
    "        # SHAP returns values for all classes, we want the positive class\n",
    "        if hasattr(shap_values, 'values'):\n",
    "            # For text explainer, values might be structured differently\n",
    "            values = shap_values.values[0] if len(shap_values.values.shape) > 1 else shap_values.values\n",
    "            # If multi-class, take the positive class values\n",
    "            if len(values.shape) > 1:\n",
    "                values = values[:, 1]  # Positive class\n",
    "            shap_values_list.append(values)\n",
    "        else:\n",
    "            # Fallback: use the explanation object directly\n",
    "            shap_values_list.append(shap_values.values.flatten())\n",
    "    \n",
    "    print(f\"Completed SHAP calculation for {len(texts)} texts\")\n",
    "    return shap_values_list\n",
    "\n",
    "# Test on a small sample\n",
    "print(\"Testing SHAP calculation on a small sample...\")\n",
    "sample_texts = test_data['text'].head(3).tolist()\n",
    "shap_values_sample = calculate_shap_values(sample_texts, model, tokenizer, device=device, max_evals=50)\n",
    "print(f\"\\nSHAP values calculated for {len(shap_values_sample)} texts\")\n",
    "for i, (text, shap_vals) in enumerate(zip(sample_texts, shap_values_sample)):\n",
    "    print(f\"\\nText {i+1}: {text[:50]}...\")\n",
    "    print(f\"  SHAP values shape: {shap_vals.shape}\")\n",
    "    print(f\"  Top 3 tokens by absolute SHAP value: {np.argsort(np.abs(shap_vals))[-3:][::-1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lime_values(\n",
    "    texts: List[str],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device='cpu',\n",
    "    num_features: int = 20,\n",
    "    num_samples: int = 5000\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate LIME values for a list of texts.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    texts : List[str]\n",
    "        List of input texts\n",
    "    model : AutoModelForSequenceClassification\n",
    "        Fine-tuned model\n",
    "    tokenizer : AutoTokenizer\n",
    "        Tokenizer\n",
    "    device : str\n",
    "        Device to run inference on\n",
    "    num_features : int\n",
    "        Number of top features to explain (default: 20)\n",
    "    num_samples : int\n",
    "        Number of samples for LIME (default: 5000)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    lime_values_list : List[np.ndarray]\n",
    "        List of LIME value arrays, one per text\n",
    "    \"\"\"\n",
    "    # Create LIME explainer with custom tokenizer\n",
    "    class CustomTokenizer:\n",
    "        \"\"\"Custom tokenizer wrapper for LIME\"\"\"\n",
    "        def __call__(self, text):\n",
    "            return custom_regex_tokenizer(text)\n",
    "    \n",
    "    explainer = LimeTextExplainer(class_names=['Non-Biased', 'Biased'], split_expression=CustomTokenizer())\n",
    "    \n",
    "    def predict_proba_wrapper(texts_input):\n",
    "        \"\"\"Wrapper for model prediction that LIME can use\"\"\"\n",
    "        if isinstance(texts_input, str):\n",
    "            texts_input = [texts_input]\n",
    "        probs = model_predict(texts_input, model, tokenizer, device)\n",
    "        # LIME expects probabilities for all classes\n",
    "        # Return [prob_non_biased, prob_biased]\n",
    "        return np.column_stack([1 - probs, probs])\n",
    "    \n",
    "    lime_values_list = []\n",
    "    \n",
    "    print(f\"Calculating LIME values for {len(texts)} texts...\")\n",
    "    for i, text in enumerate(texts):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Processing text {i+1}/{len(texts)}\")\n",
    "        \n",
    "        # Get LIME explanation\n",
    "        explanation = explainer.explain_instance(\n",
    "            text,\n",
    "            predict_proba_wrapper,\n",
    "            num_features=num_features,\n",
    "            num_samples=num_samples\n",
    "        )\n",
    "        \n",
    "        # Extract LIME values\n",
    "        # Get all features and their scores\n",
    "        exp_list = explanation.as_list()\n",
    "        \n",
    "        # Create a mapping from token to LIME value\n",
    "        tokens = custom_regex_tokenizer(text)\n",
    "        lime_dict = {token: 0.0 for token in tokens}\n",
    "        \n",
    "        # Map LIME explanations to tokens\n",
    "        for feature, score in exp_list:\n",
    "            # Feature might be a token or a phrase\n",
    "            feature_tokens = custom_regex_tokenizer(feature)\n",
    "            if len(feature_tokens) == 1:\n",
    "                # Single token\n",
    "                if feature_tokens[0] in lime_dict:\n",
    "                    lime_dict[feature_tokens[0]] = score\n",
    "            else:\n",
    "                # Multi-token feature - distribute score\n",
    "                for token in feature_tokens:\n",
    "                    if token in lime_dict:\n",
    "                        lime_dict[token] += score / len(feature_tokens)\n",
    "        \n",
    "        # Convert to array matching token order\n",
    "        lime_values = np.array([lime_dict.get(token, 0.0) for token in tokens])\n",
    "        lime_values_list.append(lime_values)\n",
    "    \n",
    "    print(f\"Completed LIME calculation for {len(texts)} texts\")\n",
    "    return lime_values_list\n",
    "\n",
    "# Test on a small sample\n",
    "print(\"Testing LIME calculation on a small sample...\")\n",
    "lime_values_sample = calculate_lime_values(sample_texts, model, tokenizer, device=device, num_samples=1000)\n",
    "print(f\"\\nLIME values calculated for {len(lime_values_sample)} texts\")\n",
    "for i, (text, lime_vals) in enumerate(zip(sample_texts, lime_values_sample)):\n",
    "    print(f\"\\nText {i+1}: {text[:50]}...\")\n",
    "    print(f\"  LIME values shape: {lime_vals.shape}\")\n",
    "    print(f\"  Top 3 tokens by absolute LIME value: {np.argsort(np.abs(lime_vals))[-3:][::-1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "\n",
    "def pearson_correlation(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Calculate Pearson correlation between two vectors\"\"\"\n",
    "    if len(vec1) != len(vec2) or len(vec1) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    corr, _ = pearsonr(vec1, vec2)\n",
    "    return corr if not np.isnan(corr) else 0.0\n",
    "\n",
    "\n",
    "def jensen_shannon_divergence(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Jensen-Shannon Divergence between two vectors.\n",
    "    \n",
    "    As per the paper:\n",
    "    P_j = (φ_{ij} + |Min(φ_i)|) / Σ(φ_{ij} + |Min(φ_i)|)\n",
    "    Q_j = (β_{ij} + |Min(β_i)|) / Σ(β_{ij} + |Min(β_i)|)\n",
    "    JSD = sqrt(0.5 * Σ P_j log(P_j / (P_j/2 + Q_j/2)) + 0.5 * Σ Q_j log(Q_j / (P_j/2 + Q_j/2)))\n",
    "    \"\"\"\n",
    "    # Normalize to make them probability distributions\n",
    "    vec1_min = np.min(vec1)\n",
    "    vec1_shifted = vec1 + abs(vec1_min)\n",
    "    vec1_sum = np.sum(vec1_shifted)\n",
    "    if vec1_sum == 0:\n",
    "        P = np.ones_like(vec1) / len(vec1)\n",
    "    else:\n",
    "        P = vec1_shifted / vec1_sum\n",
    "    \n",
    "    vec2_min = np.min(vec2)\n",
    "    vec2_shifted = vec2 + abs(vec2_min)\n",
    "    vec2_sum = np.sum(vec2_shifted)\n",
    "    if vec2_sum == 0:\n",
    "        Q = np.ones_like(vec2) / len(vec2)\n",
    "    else:\n",
    "        Q = vec2_shifted / vec2_sum\n",
    "    \n",
    "    # Calculate JSD\n",
    "    M = (P + Q) / 2\n",
    "    \n",
    "    # Avoid log(0)\n",
    "    epsilon = 1e-10\n",
    "    P = np.clip(P, epsilon, 1)\n",
    "    Q = np.clip(Q, epsilon, 1)\n",
    "    M = np.clip(M, epsilon, 1)\n",
    "    \n",
    "    kl_pm = np.sum(P * np.log(P / M))\n",
    "    kl_qm = np.sum(Q * np.log(Q / M))\n",
    "    \n",
    "    jsd = np.sqrt(0.5 * kl_pm + 0.5 * kl_qm)\n",
    "    \n",
    "    return jsd\n",
    "\n",
    "\n",
    "def calculate_similarity_metrics(\n",
    "    shap_values: np.ndarray,\n",
    "    lime_values: np.ndarray\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate all similarity metrics between SHAP and LIME vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    shap_values : np.ndarray\n",
    "        SHAP values for a text instance\n",
    "    lime_values : np.ndarray\n",
    "        LIME values for the same text instance\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    metrics : Dict[str, float]\n",
    "        Dictionary containing cosine similarity, Pearson correlation, and JSD\n",
    "    \"\"\"\n",
    "    # Ensure same length (pad or truncate if necessary)\n",
    "    min_len = min(len(shap_values), len(lime_values))\n",
    "    shap_aligned = shap_values[:min_len]\n",
    "    lime_aligned = lime_values[:min_len]\n",
    "    \n",
    "    metrics = {\n",
    "        'cosine_similarity': cosine_similarity(shap_aligned, lime_aligned),\n",
    "        'pearson_correlation': pearson_correlation(shap_aligned, lime_aligned),\n",
    "        'jensen_shannon_divergence': jensen_shannon_divergence(shap_aligned, lime_aligned)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Test similarity metrics\n",
    "print(\"Testing similarity metrics...\")\n",
    "if len(shap_values_sample) > 0 and len(lime_values_sample) > 0:\n",
    "    test_metrics = calculate_similarity_metrics(shap_values_sample[0], lime_values_sample[0])\n",
    "    print(f\"\\nSimilarity metrics for first sample:\")\n",
    "    for metric_name, value in test_metrics.items():\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_explainability_analysis(\n",
    "    test_data: pd.DataFrame,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device='cpu',\n",
    "    sample_size: Optional[int] = None,\n",
    "    max_evals_shap: int = 100,\n",
    "    num_samples_lime: int = 5000\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run complete explainability analysis on test data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    test_data : pd.DataFrame\n",
    "        Test data with 'text' and 'label' columns\n",
    "    model : AutoModelForSequenceClassification\n",
    "        Fine-tuned model\n",
    "    tokenizer : AutoTokenizer\n",
    "        Tokenizer\n",
    "    device : str\n",
    "        Device to run inference on\n",
    "    sample_size : Optional[int]\n",
    "        Number of samples to analyze (None for all)\n",
    "    max_evals_shap : int\n",
    "        Maximum evaluations for SHAP\n",
    "    num_samples_lime : int\n",
    "        Number of samples for LIME\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results_df : pd.DataFrame\n",
    "        DataFrame with SHAP values, LIME values, predictions, and similarity metrics\n",
    "    \"\"\"\n",
    "    # Sample data if specified\n",
    "    if sample_size is not None and sample_size < len(test_data):\n",
    "        test_data = test_data.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "        print(f\"Sampled {sample_size} examples for analysis\")\n",
    "    \n",
    "    texts = test_data['text'].tolist()\n",
    "    true_labels = test_data['label'].values\n",
    "    \n",
    "    # Get predictions\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Step 1: Getting model predictions...\")\n",
    "    print(\"=\"*60)\n",
    "    predictions = model_predict(texts, model, tokenizer, device)\n",
    "    predicted_labels = (predictions > 0.5).astype(int)\n",
    "    \n",
    "    # Identify correct and incorrect predictions\n",
    "    correct_mask = (predicted_labels == true_labels)\n",
    "    incorrect_mask = ~correct_mask\n",
    "    \n",
    "    print(f\"\\nPredictions summary:\")\n",
    "    print(f\"  Total examples: {len(texts)}\")\n",
    "    print(f\"  Correct predictions: {correct_mask.sum()}\")\n",
    "    print(f\"  Incorrect predictions: {incorrect_mask.sum()}\")\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Step 2: Calculating SHAP values...\")\n",
    "    print(\"=\"*60)\n",
    "    shap_values_list = calculate_shap_values(\n",
    "        texts, model, tokenizer, device, max_evals=max_evals_shap\n",
    "    )\n",
    "    \n",
    "    # Calculate LIME values\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Step 3: Calculating LIME values...\")\n",
    "    print(\"=\"*60)\n",
    "    lime_values_list = calculate_lime_values(\n",
    "        texts, model, tokenizer, device, num_samples=num_samples_lime\n",
    "    )\n",
    "    \n",
    "    # Calculate similarity metrics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Step 4: Calculating similarity metrics...\")\n",
    "    print(\"=\"*60)\n",
    "    similarity_metrics_list = []\n",
    "    for i, (shap_vals, lime_vals) in enumerate(zip(shap_values_list, lime_values_list)):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  Processing {i+1}/{len(texts)}\")\n",
    "        metrics = calculate_similarity_metrics(shap_vals, lime_vals)\n",
    "        similarity_metrics_list.append(metrics)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'text': texts,\n",
    "        'true_label': true_labels,\n",
    "        'predicted_label': predicted_labels,\n",
    "        'predicted_probability': predictions,\n",
    "        'correct_prediction': correct_mask,\n",
    "        'shap_values': shap_values_list,\n",
    "        'lime_values': lime_values_list,\n",
    "        'cosine_similarity': [m['cosine_similarity'] for m in similarity_metrics_list],\n",
    "        'pearson_correlation': [m['pearson_correlation'] for m in similarity_metrics_list],\n",
    "        'jensen_shannon_divergence': [m['jensen_shannon_divergence'] for m in similarity_metrics_list]\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Analysis Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nSimilarity metrics summary:\")\n",
    "    print(f\"  Cosine Similarity - Mean: {results_df['cosine_similarity'].mean():.4f}, Std: {results_df['cosine_similarity'].std():.4f}\")\n",
    "    print(f\"  Pearson Correlation - Mean: {results_df['pearson_correlation'].mean():.4f}, Std: {results_df['pearson_correlation'].std():.4f}\")\n",
    "    print(f\"  Jensen-Shannon Divergence - Mean: {results_df['jensen_shannon_divergence'].mean():.4f}, Std: {results_df['jensen_shannon_divergence'].std():.4f}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run analysis on a sample (use 1005 as mentioned in paper, or smaller for testing)\n",
    "print(\"Starting explainability analysis...\")\n",
    "print(\"Note: This may take a while. Using a small sample for initial testing.\")\n",
    "results_df = run_explainability_analysis(\n",
    "    test_data,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device=device,\n",
    "    sample_size=50,  # Start with 50 for testing, increase to 1005 for full analysis\n",
    "    max_evals_shap=50,  # Reduced for faster testing\n",
    "    num_samples_lime=1000  # Reduced for faster testing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Rankings for Correct and Incorrect Predictions\n",
    "\n",
    "Generate token rankings based on SHAP and LIME values for correct and incorrect predictions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_rankings(\n",
    "    results_df: pd.DataFrame,\n",
    "    top_k: int = 10\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Get token rankings for correct and incorrect predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_df : pd.DataFrame\n",
    "        Results from explainability analysis\n",
    "    top_k : int\n",
    "        Number of top tokens to extract\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    rankings : Dict[str, pd.DataFrame]\n",
    "        Dictionary with rankings for correct/incorrect predictions\n",
    "    \"\"\"\n",
    "    rankings = {}\n",
    "    \n",
    "    # Separate correct and incorrect predictions\n",
    "    correct_df = results_df[results_df['correct_prediction'] == True].copy()\n",
    "    incorrect_df = results_df[results_df['correct_prediction'] == False].copy()\n",
    "    \n",
    "    def extract_top_tokens(row, method='shap', top_k=top_k):\n",
    "        \"\"\"Extract top K tokens by importance\"\"\"\n",
    "        text = row['text']\n",
    "        tokens = custom_regex_tokenizer(text)\n",
    "        \n",
    "        if method == 'shap':\n",
    "            values = row['shap_values']\n",
    "        else:  # lime\n",
    "            values = row['lime_values']\n",
    "        \n",
    "        # Ensure same length\n",
    "        min_len = min(len(tokens), len(values))\n",
    "        tokens = tokens[:min_len]\n",
    "        values = values[:min_len]\n",
    "        \n",
    "        # Get top K by absolute value\n",
    "        top_indices = np.argsort(np.abs(values))[-top_k:][::-1]\n",
    "        top_tokens = [tokens[i] for i in top_indices]\n",
    "        top_values = [values[i] for i in top_indices]\n",
    "        \n",
    "        return {\n",
    "            'tokens': top_tokens,\n",
    "            'values': top_values,\n",
    "            'text': text\n",
    "        }\n",
    "    \n",
    "    # Extract top tokens for correct predictions\n",
    "    if len(correct_df) > 0:\n",
    "        correct_shap = correct_df.apply(\n",
    "            lambda row: extract_top_tokens(row, 'shap', top_k), axis=1\n",
    "        )\n",
    "        correct_lime = correct_df.apply(\n",
    "            lambda row: extract_top_tokens(row, 'lime', top_k), axis=1\n",
    "        )\n",
    "        \n",
    "        rankings['correct_shap'] = pd.DataFrame({\n",
    "            'text': correct_df['text'].values,\n",
    "            'true_label': correct_df['true_label'].values,\n",
    "            'predicted_label': correct_df['predicted_label'].values,\n",
    "            'top_tokens': [x['tokens'] for x in correct_shap],\n",
    "            'top_values': [x['values'] for x in correct_shap]\n",
    "        })\n",
    "        \n",
    "        rankings['correct_lime'] = pd.DataFrame({\n",
    "            'text': correct_df['text'].values,\n",
    "            'true_label': correct_df['true_label'].values,\n",
    "            'predicted_label': correct_df['predicted_label'].values,\n",
    "            'top_tokens': [x['tokens'] for x in correct_lime],\n",
    "            'top_values': [x['values'] for x in correct_lime]\n",
    "        })\n",
    "    \n",
    "    # Extract top tokens for incorrect predictions\n",
    "    if len(incorrect_df) > 0:\n",
    "        incorrect_shap = incorrect_df.apply(\n",
    "            lambda row: extract_top_tokens(row, 'shap', top_k), axis=1\n",
    "        )\n",
    "        incorrect_lime = incorrect_df.apply(\n",
    "            lambda row: extract_top_tokens(row, 'lime', top_k), axis=1\n",
    "        )\n",
    "        \n",
    "        rankings['incorrect_shap'] = pd.DataFrame({\n",
    "            'text': incorrect_df['text'].values,\n",
    "            'true_label': incorrect_df['true_label'].values,\n",
    "            'predicted_label': incorrect_df['predicted_label'].values,\n",
    "            'top_tokens': [x['tokens'] for x in incorrect_shap],\n",
    "            'top_values': [x['values'] for x in incorrect_shap]\n",
    "        })\n",
    "        \n",
    "        rankings['incorrect_lime'] = pd.DataFrame({\n",
    "            'text': incorrect_df['text'].values,\n",
    "            'true_label': incorrect_df['true_label'].values,\n",
    "            'predicted_label': incorrect_df['predicted_label'].values,\n",
    "            'top_tokens': [x['tokens'] for x in incorrect_lime],\n",
    "            'top_values': [x['values'] for x in incorrect_lime]\n",
    "        })\n",
    "    \n",
    "    return rankings\n",
    "\n",
    "# Get token rankings\n",
    "print(\"Extracting token rankings...\")\n",
    "rankings = get_token_rankings(results_df, top_k=10)\n",
    "\n",
    "print(f\"\\nRankings extracted:\")\n",
    "for key in rankings.keys():\n",
    "    print(f\"  {key}: {len(rankings[key])} examples\")\n",
    "\n",
    "# Display sample rankings\n",
    "if 'correct_shap' in rankings and len(rankings['correct_shap']) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Sample: Top tokens for a correct prediction (SHAP)\")\n",
    "    print(\"=\"*60)\n",
    "    sample_idx = 0\n",
    "    sample = rankings['correct_shap'].iloc[sample_idx]\n",
    "    print(f\"Text: {sample['text'][:100]}...\")\n",
    "    print(f\"True label: {sample['true_label']}, Predicted: {sample['predicted_label']}\")\n",
    "    print(f\"Top tokens: {sample['top_tokens']}\")\n",
    "    print(f\"Top values: {[f'{v:.4f}' for v in sample['top_values']]}\")\n",
    "\n",
    "if 'incorrect_shap' in rankings and len(rankings['incorrect_shap']) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Sample: Top tokens for an incorrect prediction (SHAP)\")\n",
    "    print(\"=\"*60)\n",
    "    sample_idx = 0\n",
    "    sample = rankings['incorrect_shap'].iloc[sample_idx]\n",
    "    print(f\"Text: {sample['text'][:100]}...\")\n",
    "    print(f\"True label: {sample['true_label']}, Predicted: {sample['predicted_label']}\")\n",
    "    print(f\"Top tokens: {sample['top_tokens']}\")\n",
    "    print(f\"Top values: {[f'{v:.4f}' for v in sample['top_values']]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity_metrics(results_df: pd.DataFrame, save_path: Optional[Path] = None):\n",
    "    \"\"\"Plot similarity metrics distribution\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    metrics = ['cosine_similarity', 'pearson_correlation', 'jensen_shannon_divergence']\n",
    "    titles = ['Cosine Similarity', 'Pearson Correlation', 'Jensen-Shannon Divergence']\n",
    "    \n",
    "    for ax, metric, title in zip(axes, metrics, titles):\n",
    "        ax.hist(results_df[metric], bins=30, edgecolor='black', alpha=0.7)\n",
    "        ax.axvline(results_df[metric].mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {results_df[metric].mean():.4f}')\n",
    "        ax.set_xlabel(title)\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title(f'{title} Distribution')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved figure to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_correct_vs_incorrect_similarity(results_df: pd.DataFrame, save_path: Optional[Path] = None):\n",
    "    \"\"\"Plot similarity metrics comparison between correct and incorrect predictions\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    correct_df = results_df[results_df['correct_prediction'] == True]\n",
    "    incorrect_df = results_df[results_df['correct_prediction'] == False]\n",
    "    \n",
    "    metrics = ['cosine_similarity', 'pearson_correlation', 'jensen_shannon_divergence']\n",
    "    titles = ['Cosine Similarity', 'Pearson Correlation', 'Jensen-Shannon Divergence']\n",
    "    \n",
    "    for ax, metric, title in zip(axes, metrics, titles):\n",
    "        data_to_plot = [correct_df[metric].dropna(), incorrect_df[metric].dropna()]\n",
    "        ax.boxplot(data_to_plot, labels=['Correct', 'Incorrect'])\n",
    "        ax.set_ylabel(title)\n",
    "        ax.set_title(f'{title}: Correct vs Incorrect')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved figure to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_token_importance_example(\n",
    "    text: str,\n",
    "    shap_values: np.ndarray,\n",
    "    lime_values: np.ndarray,\n",
    "    top_k: int = 10,\n",
    "    save_path: Optional[Path] = None\n",
    "):\n",
    "    \"\"\"Plot token importance comparison for a single example\"\"\"\n",
    "    tokens = custom_regex_tokenizer(text)\n",
    "    \n",
    "    # Align values\n",
    "    min_len = min(len(tokens), len(shap_values), len(lime_values))\n",
    "    tokens = tokens[:min_len]\n",
    "    shap_aligned = shap_values[:min_len]\n",
    "    lime_aligned = lime_values[:min_len]\n",
    "    \n",
    "    # Get top K tokens\n",
    "    top_indices = np.argsort(np.abs(shap_aligned))[-top_k:][::-1]\n",
    "    \n",
    "    top_tokens = [tokens[i] for i in top_indices]\n",
    "    top_shap = [shap_aligned[i] for i in top_indices]\n",
    "    top_lime = [lime_aligned[i] for i in top_indices]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # SHAP plot\n",
    "    colors_shap = ['red' if v < 0 else 'green' for v in top_shap]\n",
    "    ax1.barh(range(len(top_tokens)), top_shap, color=colors_shap, alpha=0.7)\n",
    "    ax1.set_yticks(range(len(top_tokens)))\n",
    "    ax1.set_yticklabels(top_tokens)\n",
    "    ax1.set_xlabel('SHAP Value')\n",
    "    ax1.set_title(f'Top {top_k} Tokens by SHAP Importance')\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "    ax1.axvline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    # LIME plot\n",
    "    colors_lime = ['red' if v < 0 else 'green' for v in top_lime]\n",
    "    ax2.barh(range(len(top_tokens)), top_lime, color=colors_lime, alpha=0.7)\n",
    "    ax2.set_yticks(range(len(top_tokens)))\n",
    "    ax2.set_yticklabels(top_tokens)\n",
    "    ax2.set_xlabel('LIME Value')\n",
    "    ax2.set_title(f'Top {top_k} Tokens by LIME Importance')\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    ax2.axvline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved figure to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "print(\"Creating visualizations...\")\n",
    "\n",
    "# 1. Similarity metrics distribution\n",
    "plot_similarity_metrics(\n",
    "    results_df,\n",
    "    save_path=generated_figures_dir / 'similarity_metrics_distribution.png'\n",
    ")\n",
    "\n",
    "# 2. Correct vs Incorrect comparison\n",
    "plot_correct_vs_incorrect_similarity(\n",
    "    results_df,\n",
    "    save_path=generated_figures_dir / 'similarity_correct_vs_incorrect.png'\n",
    ")\n",
    "\n",
    "# 3. Example token importance\n",
    "if len(results_df) > 0:\n",
    "    sample_row = results_df.iloc[0]\n",
    "    plot_token_importance_example(\n",
    "        sample_row['text'],\n",
    "        sample_row['shap_values'],\n",
    "        sample_row['lime_values'],\n",
    "        top_k=10,\n",
    "        save_path=generated_figures_dir / 'token_importance_example.png'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Save the explainability analysis results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_output_dir = results_dir / 'job_descriptions' / 'explainability'\n",
    "os.makedirs(results_output_dir, exist_ok=True)\n",
    "\n",
    "# Save main results (convert arrays to lists for JSON serialization)\n",
    "results_to_save = results_df.copy()\n",
    "results_to_save['shap_values'] = results_to_save['shap_values'].apply(lambda x: x.tolist())\n",
    "results_to_save['lime_values'] = results_to_save['lime_values'].apply(lambda x: x.tolist())\n",
    "\n",
    "# Save as CSV (without array columns for readability)\n",
    "results_csv = results_df.copy()\n",
    "results_csv['shap_values'] = results_csv['shap_values'].apply(lambda x: str(x.tolist()))\n",
    "results_csv['lime_values'] = results_csv['lime_values'].apply(lambda x: str(x.tolist()))\n",
    "results_csv.to_csv(results_output_dir / 'explainability_results.csv', index=False)\n",
    "\n",
    "# Save full results with arrays as pickle\n",
    "with open(results_output_dir / 'explainability_results_full.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'results_df': results_df,\n",
    "        'rankings': rankings,\n",
    "        'similarity_summary': {\n",
    "            'cosine_similarity': {\n",
    "                'mean': float(results_df['cosine_similarity'].mean()),\n",
    "                'std': float(results_df['cosine_similarity'].std())\n",
    "            },\n",
    "            'pearson_correlation': {\n",
    "                'mean': float(results_df['pearson_correlation'].mean()),\n",
    "                'std': float(results_df['pearson_correlation'].std())\n",
    "            },\n",
    "            'jensen_shannon_divergence': {\n",
    "                'mean': float(results_df['jensen_shannon_divergence'].mean()),\n",
    "                'std': float(results_df['jensen_shannon_divergence'].std())\n",
    "            }\n",
    "        }\n",
    "    }, f)\n",
    "\n",
    "# Save rankings\n",
    "for key, ranking_df in rankings.items():\n",
    "    ranking_df.to_csv(results_output_dir / f'token_rankings_{key}.csv', index=False)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_output_dir}\")\n",
    "print(f\"  - explainability_results.csv\")\n",
    "print(f\"  - explainability_results_full.pkl\")\n",
    "for key in rankings.keys():\n",
    "    print(f\"  - token_rankings_{key}.csv\")\n",
    "\n",
    "print(f\"\\nFigures saved to: {generated_figures_dir}\")\n",
    "print(f\"  - similarity_metrics_distribution.png\")\n",
    "print(f\"  - similarity_correct_vs_incorrect.png\")\n",
    "print(f\"  - token_importance_example.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
