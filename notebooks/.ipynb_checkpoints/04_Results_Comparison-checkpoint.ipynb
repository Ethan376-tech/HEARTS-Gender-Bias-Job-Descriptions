{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results Comparison: HEARTS Adaptation vs. Original\n",
        "\n",
        "**Project:** HEARTS Adaptation - Gender Bias Detection  \n",
        "**Purpose:** Compare results from adapted models with original HEARTS replication\n",
        "\n",
        "This notebook handles:\n",
        "1. Loading evaluation results from both projects\n",
        "2. Comparing metrics across models\n",
        "3. Visualizing performance differences\n",
        "4. Generating comparison report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set up paths\n",
        "project_root = os.path.dirname(os.path.dirname(os.path.abspath('')))\n",
        "results_dir = os.path.join(project_root, 'results')\n",
        "hearts_original_dir = r'D:\\Coursework\\Project Replication\\HEARTS-Text-Stereotype-Detection-main'\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Results directory: {results_dir}\")\n",
        "print(f\"Original HEARTS directory: {hearts_original_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Results\n",
        "\n",
        "Load evaluation results from both the adapted project and original HEARTS replication:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_classification_reports(results_dir, model_names=None):\n",
        "    \"\"\"\n",
        "    Load classification reports for all models\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    results_dir : str\n",
        "        Directory containing results\n",
        "    model_names : list or None\n",
        "        List of model names to load (if None, auto-detect)\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    results_dict : dict\n",
        "        Dictionary mapping model names to classification reports\n",
        "    \"\"\"\n",
        "    results_dict = {}\n",
        "    \n",
        "    if model_names is None:\n",
        "        # Auto-detect model names from directory structure\n",
        "        dataset_dir = os.path.join(results_dir, 'job_descriptions')\n",
        "        if os.path.exists(dataset_dir):\n",
        "            csv_files = [f for f in os.listdir(dataset_dir) if f.startswith('classification_report_') and f.endswith('.csv')]\n",
        "            model_names = [f.replace('classification_report_', '').replace('.csv', '') for f in csv_files]\n",
        "    \n",
        "    if model_names:\n",
        "        for model_name in model_names:\n",
        "            report_path = os.path.join(results_dir, 'job_descriptions', f'classification_report_{model_name}.csv')\n",
        "            if os.path.exists(report_path):\n",
        "                df = pd.read_csv(report_path, index_col=0)\n",
        "                results_dict[model_name] = df\n",
        "                print(f\"Loaded results for: {model_name}\")\n",
        "            else:\n",
        "                print(f\"Warning: Results not found for {model_name} at {report_path}\")\n",
        "    \n",
        "    return results_dict\n",
        "\n",
        "\n",
        "def extract_metrics_from_report(report_df):\n",
        "    \"\"\"\n",
        "    Extract key metrics from classification report\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    metrics : dict\n",
        "        Dictionary with precision, recall, f1, balanced_accuracy\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "    \n",
        "    # Get macro averages\n",
        "    if 'macro avg' in report_df.index:\n",
        "        macro_avg = report_df.loc['macro avg']\n",
        "        metrics['precision'] = macro_avg.get('precision', np.nan)\n",
        "        metrics['recall'] = macro_avg.get('recall', np.nan)\n",
        "        metrics['f1'] = macro_avg.get('f1-score', np.nan)\n",
        "    \n",
        "    # Get accuracy\n",
        "    if 'accuracy' in report_df.index:\n",
        "        metrics['accuracy'] = report_df.loc['accuracy', 'f1-score']\n",
        "    \n",
        "    # Try to get balanced accuracy if available\n",
        "    if 'balanced_accuracy' in report_df.columns:\n",
        "        metrics['balanced_accuracy'] = report_df.loc['macro avg', 'balanced_accuracy']\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "\n",
        "# Load adapted project results\n",
        "print(\"Loading adapted project results...\")\n",
        "adapted_results = load_classification_reports(results_dir)\n",
        "\n",
        "# Load original HEARTS results (if available)\n",
        "print(\"\\nLoading original HEARTS results...\")\n",
        "# Note: Update path based on where original HEARTS results are stored\n",
        "hearts_results_path = os.path.join(hearts_original_dir, 'Model Training and Evaluation', 'result_output_albertv2')\n",
        "if os.path.exists(hearts_results_path):\n",
        "    # Try to find classification reports\n",
        "    hearts_results = {}\n",
        "    # This would need to be adapted based on actual HEARTS result structure\n",
        "    print(\"Original HEARTS results directory found\")\n",
        "else:\n",
        "    print(\"Original HEARTS results directory not found - update path if needed\")\n",
        "    hearts_results = {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare Metrics\n",
        "\n",
        "Create a comparison table of metrics across models:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_comparison_table(adapted_results, hearts_results=None):\n",
        "    \"\"\"\n",
        "    Create a comparison table of metrics\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    adapted_results : dict\n",
        "        Results from adapted project\n",
        "    hearts_results : dict\n",
        "        Results from original HEARTS (optional)\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    comparison_df : pd.DataFrame\n",
        "        Comparison table\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    \n",
        "    # Add adapted project results\n",
        "    for model_name, report_df in adapted_results.items():\n",
        "        metrics = extract_metrics_from_report(report_df)\n",
        "        row = {\n",
        "            'Project': 'Adapted (Gender Bias)',\n",
        "            'Model': model_name,\n",
        "            'Task': 'Gender Bias Detection',\n",
        "            **metrics\n",
        "        }\n",
        "        rows.append(row)\n",
        "    \n",
        "    # Add original HEARTS results (if available)\n",
        "    if hearts_results:\n",
        "        for model_name, report_df in hearts_results.items():\n",
        "            metrics = extract_metrics_from_report(report_df)\n",
        "            row = {\n",
        "                'Project': 'Original HEARTS',\n",
        "                'Model': model_name,\n",
        "                'Task': 'Stereotype Detection',\n",
        "                **metrics\n",
        "            }\n",
        "            rows.append(row)\n",
        "    \n",
        "    # Create DataFrame\n",
        "    comparison_df = pd.DataFrame(rows)\n",
        "    \n",
        "    return comparison_df\n",
        "\n",
        "\n",
        "# Create comparison table\n",
        "if adapted_results:\n",
        "    comparison_df = create_comparison_table(adapted_results, hearts_results)\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"RESULTS COMPARISON\")\n",
        "    print(\"=\" * 80)\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "    # Save comparison\n",
        "    comparison_path = os.path.join(results_dir, 'results_comparison.csv')\n",
        "    comparison_df.to_csv(comparison_path, index=False)\n",
        "    print(f\"\\nComparison saved to: {comparison_path}\")\n",
        "else:\n",
        "    print(\"No results found. Please run evaluation notebooks first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_metrics_comparison(comparison_df, save_path=None):\n",
        "    \"\"\"\n",
        "    Create visualization comparing metrics across models\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    comparison_df : pd.DataFrame\n",
        "        Comparison table\n",
        "    save_path : str or None\n",
        "        Path to save figure\n",
        "    \"\"\"\n",
        "    if comparison_df.empty:\n",
        "        print(\"No data to plot\")\n",
        "        return\n",
        "    \n",
        "    # Set up the plotting style\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "    sns.set_palette(\"husl\")\n",
        "    \n",
        "    # Prepare data for plotting\n",
        "    metrics_to_plot = ['precision', 'recall', 'f1']\n",
        "    available_metrics = [m for m in metrics_to_plot if m in comparison_df.columns]\n",
        "    \n",
        "    if not available_metrics:\n",
        "        print(\"No metrics available for plotting\")\n",
        "        return\n",
        "    \n",
        "    # Create figure with subplots\n",
        "    n_metrics = len(available_metrics)\n",
        "    fig, axes = plt.subplots(1, n_metrics, figsize=(6 * n_metrics, 6))\n",
        "    \n",
        "    if n_metrics == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for idx, metric in enumerate(available_metrics):\n",
        "        ax = axes[idx]\n",
        "        \n",
        "        # Create bar plot\n",
        "        x_pos = np.arange(len(comparison_df))\n",
        "        bars = ax.bar(x_pos, comparison_df[metric], alpha=0.7, edgecolor='black')\n",
        "        \n",
        "        # Customize plot\n",
        "        ax.set_xlabel('Model', fontsize=12)\n",
        "        ax.set_ylabel(metric.capitalize(), fontsize=12)\n",
        "        ax.set_title(f'{metric.capitalize()} Comparison', fontsize=14, fontweight='bold')\n",
        "        ax.set_xticks(x_pos)\n",
        "        ax.set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
        "        ax.set_ylim([0, 1])\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{height:.3f}',\n",
        "                   ha='center', va='bottom', fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"Figure saved to: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Create visualizations\n",
        "if 'comparison_df' in locals() and not comparison_df.empty:\n",
        "    plot_path = os.path.join(results_dir, 'metrics_comparison.png')\n",
        "    plot_metrics_comparison(comparison_df, save_path=plot_path)\n",
        "else:\n",
        "    print(\"No comparison data available. Please run the comparison cell above first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Report\n",
        "\n",
        "Generate a summary report comparing the adapted project with original HEARTS:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_summary_report(comparison_df, output_path=None):\n",
        "    \"\"\"\n",
        "    Generate a text summary report\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    comparison_df : pd.DataFrame\n",
        "        Comparison table\n",
        "    output_path : str or None\n",
        "        Path to save report\n",
        "    \"\"\"\n",
        "    if comparison_df.empty:\n",
        "        return \"No results available for summary.\"\n",
        "    \n",
        "    report_lines = []\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(\"HEARTS ADAPTATION - RESULTS SUMMARY\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(\"Project: Gender Bias Detection in Job Descriptions\")\n",
        "    report_lines.append(\"SDG Alignment: SDG 5 (Gender Equality) & SDG 8 (Decent Work)\")\n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(\"MODEL PERFORMANCE\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(\"\")\n",
        "    \n",
        "    # Add adapted project results\n",
        "    adapted_df = comparison_df[comparison_df['Project'] == 'Adapted (Gender Bias)']\n",
        "    if not adapted_df.empty:\n",
        "        report_lines.append(\"Adapted Project Results:\")\n",
        "        report_lines.append(\"-\" * 80)\n",
        "        for _, row in adapted_df.iterrows():\n",
        "            report_lines.append(f\"\\nModel: {row['Model']}\")\n",
        "            if 'precision' in row:\n",
        "                report_lines.append(f\"  Precision: {row['precision']:.4f}\")\n",
        "            if 'recall' in row:\n",
        "                report_lines.append(f\"  Recall: {row['recall']:.4f}\")\n",
        "            if 'f1' in row:\n",
        "                report_lines.append(f\"  F1-Score: {row['f1']:.4f}\")\n",
        "            if 'balanced_accuracy' in row and pd.notna(row['balanced_accuracy']):\n",
        "                report_lines.append(f\"  Balanced Accuracy: {row['balanced_accuracy']:.4f}\")\n",
        "    \n",
        "    # Add original HEARTS results if available\n",
        "    hearts_df = comparison_df[comparison_df['Project'] == 'Original HEARTS']\n",
        "    if not hearts_df.empty:\n",
        "        report_lines.append(\"\\n\" + \"=\" * 80)\n",
        "        report_lines.append(\"Original HEARTS Results (for reference):\")\n",
        "        report_lines.append(\"-\" * 80)\n",
        "        for _, row in hearts_df.iterrows():\n",
        "            report_lines.append(f\"\\nModel: {row['Model']}\")\n",
        "            if 'precision' in row:\n",
        "                report_lines.append(f\"  Precision: {row['precision']:.4f}\")\n",
        "            if 'recall' in row:\n",
        "                report_lines.append(f\"  Recall: {row['recall']:.4f}\")\n",
        "            if 'f1' in row:\n",
        "                report_lines.append(f\"  F1-Score: {row['f1']:.4f}\")\n",
        "    \n",
        "    report_lines.append(\"\\n\" + \"=\" * 80)\n",
        "    report_lines.append(\"END OF REPORT\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    \n",
        "    report_text = \"\\n\".join(report_lines)\n",
        "    \n",
        "    if output_path:\n",
        "        with open(output_path, 'w') as f:\n",
        "            f.write(report_text)\n",
        "        print(f\"Summary report saved to: {output_path}\")\n",
        "    \n",
        "    return report_text\n",
        "\n",
        "\n",
        "# Generate summary report\n",
        "if 'comparison_df' in locals() and not comparison_df.empty:\n",
        "    report_path = os.path.join(results_dir, 'summary_report.txt')\n",
        "    summary = generate_summary_report(comparison_df, output_path=report_path)\n",
        "    print(summary)\n",
        "else:\n",
        "    print(\"No comparison data available. Please run the comparison cell above first.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
