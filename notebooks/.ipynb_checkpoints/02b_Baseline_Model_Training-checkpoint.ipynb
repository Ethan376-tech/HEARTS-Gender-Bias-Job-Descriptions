{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models Training\n",
    "\n",
    "**Project:** HEARTS Adaptation - Gender Bias Detection  \n",
    "**SDG Alignment:** SDG 5 (Gender Equality) & SDG 8 (Decent Work and Economic Growth)  \n",
    "**Task:** Binary classification (Biased vs. Non-Biased job descriptions)\n",
    "\n",
    "This notebook trains three baseline models for comparison with Alvert-v2, DistilBERT and BERT:\n",
    "1. **LR - TF-IDF**: Logistic Regression with TF-IDF features\n",
    "2. **DistilRoBERTa-Bias**: Fine-tuned DistilRoBERTa model for gender bias detection\n",
    "3. **LR - Embeddings**: Logistic Regression using embeddings from pre-trained transformer model\n",
    "\n",
    "**Note:** Evaluation of these models is performed in `03_Model_Evaluation.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\n",
      "Data directory: D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\data\n",
      "Models directory: D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\n",
      "Results directory: D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\results\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, precision_recall_fscore_support, \n",
    "    balanced_accuracy_score, confusion_matrix, roc_curve, auc,\n",
    "    precision_recall_curve, roc_auc_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up paths\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'notebooks':\n",
    "    project_root = current_dir.parent\n",
    "else:\n",
    "    project_root = current_dir\n",
    "\n",
    "data_dir = project_root / 'data'\n",
    "models_dir = project_root / 'models'\n",
    "results_dir = project_root / 'results'\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Models directory: {models_dir}\")\n",
    "print(f\"Results directory: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loaded train split: 14065 examples\n",
      "   Loaded val split: 3517 examples\n",
      "   Loaded test data: 4396 examples\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "def load_preprocessed_data(data_dir=None):\n",
    "    \"\"\"Load preprocessed train, val, and test data\"\"\"\n",
    "    if data_dir is None:\n",
    "        data_dir = project_root / 'data'\n",
    "    \n",
    "    # Load from splits directory\n",
    "    if isinstance(data_dir, Path):\n",
    "        splits_dir = data_dir / 'splits'\n",
    "        train_path = splits_dir / 'train.csv'\n",
    "        val_path = splits_dir / 'val.csv'\n",
    "        test_path = splits_dir / 'test.csv'\n",
    "    else:\n",
    "        splits_dir = os.path.join(data_dir, 'splits')\n",
    "        train_path = os.path.join(splits_dir, 'train.csv')\n",
    "        val_path = os.path.join(splits_dir, 'val.csv')\n",
    "        test_path = os.path.join(splits_dir, 'test.csv')\n",
    "    \n",
    "    # Check if files exist\n",
    "    missing_files = []\n",
    "    if not os.path.exists(str(train_path)):\n",
    "        missing_files.append(str(train_path))\n",
    "    if not os.path.exists(str(val_path)):\n",
    "        missing_files.append(str(val_path))\n",
    "    if not os.path.exists(str(test_path)):\n",
    "        missing_files.append(str(test_path))\n",
    "    \n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Preprocessed data not found. Please run 01_Data_Loading_Preprocessing.ipynb first.\\n\"\n",
    "            f\"Missing files: {missing_files}\"\n",
    "        )\n",
    "    \n",
    "    # Load data\n",
    "    train_data = pd.read_csv(str(train_path))\n",
    "    val_data = pd.read_csv(str(val_path))\n",
    "    test_data = pd.read_csv(str(test_path))\n",
    "    \n",
    "    print(f\"   Loaded train split: {len(train_data)} examples\")\n",
    "    print(f\"   Loaded val split: {len(val_data)} examples\")\n",
    "    print(f\"   Loaded test data: {len(test_data)} examples\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Load data\n",
    "train_data, val_data, test_data = load_preprocessed_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Baseline Model 1: LR - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASELINE MODEL CONFIGURATION\n",
      "============================================================\n",
      "max_features: 10000\n",
      "ngram_range: (1, 2)\n",
      "max_iter: 1000\n",
      "random_state: 42\n",
      "C: 1.0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "BASELINE_CONFIG = {\n",
    "    'max_features': 10000,  # Limit TF-IDF features to manage memory\n",
    "    'ngram_range': (1, 2),  # Unigrams and bigrams\n",
    "    'max_iter': 1000,  # Maximum iterations for logistic regression\n",
    "    'random_state': 42,\n",
    "    'C': 1.0  # Regularization strength\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE MODEL CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in BASELINE_CONFIG.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING BASELINE MODEL\n",
      "============================================================\n",
      "Combined training data: 17582 examples\n",
      "Training samples: 17,582\n",
      "Test samples: 4,396\n",
      "\n",
      "Fitting TF-IDF vectorizer...\n",
      "TF-IDF matrix shape: (17582, 10000)\n",
      "Test TF-IDF matrix shape: (4396, 10000)\n",
      "\n",
      "Training Logistic Regression model...\n",
      "   Model training completed!\n"
     ]
    }
   ],
   "source": [
    "# Combine train and val for training (baseline models typically use all available training data)\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING BASELINE MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Combine train and validation sets for training\n",
    "train_val_data = pd.concat([train_data, val_data], ignore_index=True)\n",
    "print(f\"Combined training data: {len(train_val_data)} examples\")\n",
    "\n",
    "# Extract text and labels\n",
    "X_train = train_val_data['text'].values\n",
    "y_train = train_val_data['label'].values\n",
    "\n",
    "X_test = test_data['text'].values\n",
    "y_test = test_data['label'].values\n",
    "\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Test samples: {len(X_test):,}\")\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "print(\"\\nFitting TF-IDF vectorizer...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=BASELINE_CONFIG['max_features'],\n",
    "    ngram_range=BASELINE_CONFIG['ngram_range'],\n",
    "    stop_words='english',  # Remove common English stop words\n",
    "    lowercase=True,\n",
    "    min_df=2,  # Minimum document frequency\n",
    "    max_df=0.95  # Maximum document frequency (remove very common words)\n",
    ")\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "print(f\"TF-IDF matrix shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# Transform test data\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "print(f\"Test TF-IDF matrix shape: {X_test_tfidf.shape}\")\n",
    "\n",
    "# Train Logistic Regression model\n",
    "print(\"\\nTraining Logistic Regression model...\")\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=BASELINE_CONFIG['max_iter'],\n",
    "    random_state=BASELINE_CONFIG['random_state'],\n",
    "    C=BASELINE_CONFIG['C'],\n",
    "    solver='liblinear',  # Good for small datasets\n",
    "    class_weight='balanced'  # Handle class imbalance\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "print(\"   Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Model saved to: D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\baseline_lr_tfidf\n",
      "   - lr_model.pkl\n",
      "   - tfidf_vectorizer.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save model and vectorizer\n",
    "baseline_model_dir = models_dir / 'job_descriptions' / 'baseline_lr_tfidf'\n",
    "os.makedirs(baseline_model_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "with open(baseline_model_dir / 'lr_model.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "\n",
    "# Save vectorizer\n",
    "with open(baseline_model_dir / 'tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "print(f\"   Model saved to: {baseline_model_dir}\")\n",
    "print(f\"   - lr_model.pkl\")\n",
    "print(f\"   - tfidf_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Baseline Model 2: DistilRoBERTa-Bias\n",
    "\n",
    "This section trains a DistilRoBERTa model fine-tuned for gender bias detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DISTILROBERTA-BIAS CONFIGURATION\n",
      "============================================================\n",
      "model_path: distilbert/distilroberta-base\n",
      "batch_size: 8\n",
      "epochs: 3\n",
      "learning_rate: 2e-05\n",
      "gradient_accumulation_steps: 4\n",
      "seed: 42\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Import transformers libraries for DistilRoBERTa\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, balanced_accuracy_score\n",
    "\n",
    "# Training configuration for DistilRoBERTa\n",
    "DISTILROBERTA_CONFIG = {\n",
    "    'model_path': 'distilbert/distilroberta-base',\n",
    "    'batch_size': 8,\n",
    "    'epochs': 3,\n",
    "    'learning_rate': 2e-5,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DISTILROBERTA-BIAS CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in DISTILROBERTA_CONFIG.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined training data: 17582 examples\n",
      "\n",
      "Loading DistilRoBERTa model: distilbert/distilroberta-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ab92afb6c343129b57e43cb4f8a625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\hearts\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\b'h'w'l\\.cache\\huggingface\\hub\\models--distilbert--distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbbbac520e8f44de973e4e6051c755f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilbert/distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb542aeea29d49fda29b783bf570d050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec64da1ef684afe8090a4829e0b46e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1be827e5b9542a386c248bbc259b6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec943a820a0a483cb31115d20be403b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5917089c69c4ff3a29cd41603c9f41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692d1d99fc524475be6be9f29966b8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67df8b473cd4b26be0139360da2b069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3517 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0a9e77b633434c858115d86a96fe87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3517 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized training samples: 17582\n",
      "Tokenized validation samples: 3517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\hearts\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\b'h'w'l\\AppData\\Local\\Temp\\ipykernel_47584\\3276077721.py:87: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING DISTILROBERTA-BIAS\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1647' max='1647' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1647/1647 14:15, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.668500</td>\n",
       "      <td>0.654100</td>\n",
       "      <td>0.644301</td>\n",
       "      <td>0.608854</td>\n",
       "      <td>0.578996</td>\n",
       "      <td>0.608854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.580200</td>\n",
       "      <td>0.538206</td>\n",
       "      <td>0.722587</td>\n",
       "      <td>0.722619</td>\n",
       "      <td>0.722601</td>\n",
       "      <td>0.722619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving DistilRoBERTa model...\n",
      "âœ… DistilRoBERTa model saved to: D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\baseline_distilroberta_bias\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "np.random.seed(DISTILROBERTA_CONFIG['seed'])\n",
    "torch.manual_seed(DISTILROBERTA_CONFIG['seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(DISTILROBERTA_CONFIG['seed'])\n",
    "\n",
    "# Combine train and val for training\n",
    "train_val_data = pd.concat([train_data, val_data], ignore_index=True)\n",
    "print(f\"Combined training data: {len(train_val_data)} examples\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(f\"\\nLoading DistilRoBERTa model: {DISTILROBERTA_CONFIG['model_path']}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    DISTILROBERTA_CONFIG['model_path'],\n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DISTILROBERTA_CONFIG['model_path'])\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"\\nTokenizing datasets...\")\n",
    "tokenized_train = Dataset.from_pandas(train_val_data).map(\n",
    "    tokenize_function,\n",
    "    batched=True\n",
    ").map(lambda examples: {'labels': examples['label']})\n",
    "\n",
    "tokenized_val = Dataset.from_pandas(val_data).map(\n",
    "    tokenize_function,\n",
    "    batched=True\n",
    ").map(lambda examples: {'labels': examples['label']})\n",
    "\n",
    "print(f\"Tokenized training samples: {len(tokenized_train)}\")\n",
    "print(f\"Tokenized validation samples: {len(tokenized_val)}\")\n",
    "\n",
    "# Metrics computation\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='macro'\n",
    "    )\n",
    "    balanced_acc = balanced_accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"balanced_accuracy\": balanced_acc\n",
    "    }\n",
    "\n",
    "# Set up output directory\n",
    "distilroberta_model_dir = models_dir / 'job_descriptions' / 'baseline_distilroberta_bias'\n",
    "os.makedirs(distilroberta_model_dir, exist_ok=True)\n",
    "output_dir_str = str(distilroberta_model_dir)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir_str,\n",
    "    num_train_epochs=DISTILROBERTA_CONFIG['epochs'],\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=DISTILROBERTA_CONFIG['learning_rate'],\n",
    "    per_device_train_batch_size=DISTILROBERTA_CONFIG['batch_size'],\n",
    "    per_device_eval_batch_size=DISTILROBERTA_CONFIG['batch_size'],\n",
    "    gradient_accumulation_steps=DISTILROBERTA_CONFIG['gradient_accumulation_steps'],\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=1,\n",
    "    logging_dir=os.path.join(output_dir_str, 'logs'),\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    "    seed=DISTILROBERTA_CONFIG['seed'],\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    lr_scheduler_type=\"constant\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING DISTILROBERTA-BIAS\")\n",
    "print(\"=\" * 60)\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "print(\"\\nSaving DistilRoBERTa model...\")\n",
    "trainer.save_model(output_dir_str)\n",
    "tokenizer.save_pretrained(output_dir_str)\n",
    "print(f\"âœ… DistilRoBERTa model saved to: {distilroberta_model_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Baseline Model 3: LR - Embeddings\n",
    "\n",
    "This section trains a Logistic Regression model using embeddings from a pre-trained transformer model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LR - EMBEDDINGS CONFIGURATION\n",
      "============================================================\n",
      "embedding_model: distilbert/distilroberta-base\n",
      "batch_size: 32\n",
      "max_iter: 1000\n",
      "random_state: 42\n",
      "C: 1.0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configuration for LR - Embeddings\n",
    "LR_EMBEDDINGS_CONFIG = {\n",
    "    'embedding_model': 'distilbert/distilroberta-base',  # Model to extract embeddings from\n",
    "    'batch_size': 32,  # Batch size for embedding extraction\n",
    "    'max_iter': 1000,  # Maximum iterations for logistic regression\n",
    "    'random_state': 42,\n",
    "    'C': 1.0  # Regularization strength\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LR - EMBEDDINGS CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in LR_EMBEDDINGS_CONFIG.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined training data: 17582 examples\n",
      "\n",
      "Loading embedding model: distilbert/distilroberta-base\n",
      "Using device: cuda\n",
      "\n",
      "Extracting embeddings for training data...\n",
      "Training embeddings shape: (17582, 768)\n",
      "\n",
      "Extracting embeddings for test data...\n",
      "Test embeddings shape: (4396, 768)\n",
      "\n",
      "Training Logistic Regression on embeddings...\n",
      "   Model training completed!\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "np.random.seed(LR_EMBEDDINGS_CONFIG['random_state'])\n",
    "torch.manual_seed(LR_EMBEDDINGS_CONFIG['random_state'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(LR_EMBEDDINGS_CONFIG['random_state'])\n",
    "\n",
    "# Combine train and val for training\n",
    "train_val_data = pd.concat([train_data, val_data], ignore_index=True)\n",
    "print(f\"Combined training data: {len(train_val_data)} examples\")\n",
    "\n",
    "# Load model for embeddings (without classification head)\n",
    "from transformers import AutoModel\n",
    "print(f\"\\nLoading embedding model: {LR_EMBEDDINGS_CONFIG['embedding_model']}\")\n",
    "embedding_model = AutoModel.from_pretrained(LR_EMBEDDINGS_CONFIG['embedding_model'])\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(LR_EMBEDDINGS_CONFIG['embedding_model'])\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "embedding_model.to(device)\n",
    "embedding_model.eval()\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to extract embeddings\n",
    "def extract_embeddings(texts, model, tokenizer, device, batch_size=32):\n",
    "    \"\"\"Extract embeddings from texts using the model\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "        \n",
    "        # Get embeddings (use [CLS] token embedding)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            # Use mean pooling of all token embeddings\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        \n",
    "        embeddings.append(batch_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Extract embeddings for training data\n",
    "print(\"\\nExtracting embeddings for training data...\")\n",
    "X_train_texts = train_val_data['text'].values.tolist()\n",
    "X_train_embeddings = extract_embeddings(\n",
    "    X_train_texts, \n",
    "    embedding_model, \n",
    "    embedding_tokenizer, \n",
    "    device,\n",
    "    batch_size=LR_EMBEDDINGS_CONFIG['batch_size']\n",
    ")\n",
    "y_train = train_val_data['label'].values\n",
    "\n",
    "print(f\"Training embeddings shape: {X_train_embeddings.shape}\")\n",
    "\n",
    "# Extract embeddings for test data\n",
    "print(\"\\nExtracting embeddings for test data...\")\n",
    "X_test_texts = test_data['text'].values.tolist()\n",
    "X_test_embeddings = extract_embeddings(\n",
    "    X_test_texts,\n",
    "    embedding_model,\n",
    "    embedding_tokenizer,\n",
    "    device,\n",
    "    batch_size=LR_EMBEDDINGS_CONFIG['batch_size']\n",
    ")\n",
    "y_test = test_data['label'].values\n",
    "\n",
    "print(f\"Test embeddings shape: {X_test_embeddings.shape}\")\n",
    "\n",
    "# Train Logistic Regression on embeddings\n",
    "print(\"\\nTraining Logistic Regression on embeddings...\")\n",
    "lr_embeddings_model = LogisticRegression(\n",
    "    max_iter=LR_EMBEDDINGS_CONFIG['max_iter'],\n",
    "    random_state=LR_EMBEDDINGS_CONFIG['random_state'],\n",
    "    C=LR_EMBEDDINGS_CONFIG['C'],\n",
    "    solver='liblinear',\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "lr_embeddings_model.fit(X_train_embeddings, y_train)\n",
    "print(\"   Model training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save LR - Embeddings Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LR - Embeddings model saved to: D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\models\\job_descriptions\\baseline_lr_embeddings\n",
      "   - lr_model.pkl\n",
      "   - embedding_model_name.txt\n",
      "\n",
      "Note: Evaluation will be performed in 03_Model_Evaluation.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Save LR - Embeddings model\n",
    "baseline_lr_embeddings_dir = models_dir / 'job_descriptions' / 'baseline_lr_embeddings'\n",
    "os.makedirs(baseline_lr_embeddings_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "with open(baseline_lr_embeddings_dir / 'lr_model.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_embeddings_model, f)\n",
    "\n",
    "# Save embedding model name (for evaluation)\n",
    "with open(baseline_lr_embeddings_dir / 'embedding_model_name.txt', 'w') as f:\n",
    "    f.write(LR_EMBEDDINGS_CONFIG['embedding_model'])\n",
    "\n",
    "print(f\"   LR - Embeddings model saved to: {baseline_lr_embeddings_dir}\")\n",
    "print(f\"   - lr_model.pkl\")\n",
    "print(f\"   - embedding_model_name.txt\")\n",
    "print(f\"\\nNote: Evaluation will be performed in 03_Model_Evaluation.ipynb\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (HEARTS)",
   "language": "python",
   "name": "hearts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
