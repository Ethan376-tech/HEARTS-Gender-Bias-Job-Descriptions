{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation for Gender Bias Detection in Job Descriptions\n",
    "\n",
    "**Project:** HEARTS Adaptation - Gender Bias Detection  \n",
    "**SDG Alignment:** SDG 5 (Gender Equality) & SDG 8 (Decent Work and Economic Growth)  \n",
    "**Models:** ALBERT-V2, DistilBERT, BERT  \n",
    "**Task:** Binary classification (Biased vs. Non-Biased job descriptions)\n",
    "\n",
    "This notebook handles:\n",
    "1. Dataset downloading\n",
    "2. Dataset labeling\n",
    "3. Model training with validation (80/20 train/val split)\n",
    "4. Saving trained models to `models/job_descriptions/`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Introduction\n",
    "\n",
    "### Original Dataset\n",
    "\n",
    "**Dataset Name:** U.S. Technology Jobs on Dice.com\n",
    "\n",
    "**Source:** [Kaggle Dataset](https://www.kaggle.com/datasets/PromptCloudHQ/us-technology-jobs-on-dicecom/data)\n",
    "\n",
    "**Sample Size:** 21,978 job descriptions\n",
    "\n",
    "**SDG Alignment:** This research aligns with **SDG 5 (Gender Equality)** and **SDG 8 (Decent Work and Economic Growth)**, contributing to the  Sustainable Development Goals through the detection and mitigation of gender bias in employment opportunities.\n",
    "\n",
    "### Dataset Appropriateness and Ethical Sourcing\n",
    "\n",
    "**High Appropriateness for Research Objective:**\n",
    "This dataset is **appropriate** for gender bias detection research in job descriptions for several reasons:\n",
    "\n",
    "1. **Real-World Relevance**: The dataset contains actual job postings from Dice.com, a major technology job board, providing **authentic examples** of how companies communicate job opportunities in the technology sector‚Äîan industry with well-documented gender disparities.\n",
    "\n",
    "2. **Representative Sample**: With 21,978 job descriptions, the dataset provides a **substantial and representative sample** of technology job postings, enabling robust statistical analysis and model training.\n",
    "\n",
    "3. **Public Domain Data**: The job descriptions are **publicly available** information posted on a public job board, making them appropriate for research use without privacy concerns.\n",
    "\n",
    "4. **Direct Application**: Technology sector job descriptions are particularly relevant for gender bias research, as this field has historically shown significant gender imbalances, making the detection and mitigation of biased language a critical concern.\n",
    "\n",
    "**Ethical Sourcing and Data Handling:**\n",
    "\n",
    "1. **Publicly Available Data**: All job descriptions were obtained from publicly accessible job postings on Dice.com, ensuring **ethical data collection** practices.\n",
    "\n",
    "2. **No Personal Information**: The dataset focuses exclusively on **job description text** and does not contain personal information about applicants, employees, or individuals, ensuring **privacy protection**.\n",
    "\n",
    "3. **Anonymized Company Information**: While company names are included, the analysis focuses on language patterns rather than identifying specific companies, maintaining **ethical research practices**.\n",
    "\n",
    "4. **Research Purpose - SDG Alignment**: The data is used solely for **academic research** aimed at identifying and understanding gender bias in job descriptions, with the goal of promoting more inclusive hiring practices. This research directly contributes to:\n",
    "   - **SDG 5 (Gender Equality)**: By identifying and addressing gender-biased language in job descriptions, we contribute to eliminating discrimination and promoting equal opportunities for all genders in the workplace.\n",
    "   - **SDG 8 (Decent Work and Economic Growth)**: By promoting fair and inclusive hiring practices, we support the creation of equitable employment opportunities that contribute to sustainable economic growth and decent work for all.\n",
    "\n",
    "5. **Transparent Methodology**: The labeling methodology is based on **peer-reviewed research** (Gaucher et al., 2011), ensuring scientific rigor and reproducibility.\n",
    "\n",
    "6. **No Harmful Applications**: The research is designed to **detect and mitigate bias**, not to perpetuate discrimination, ensuring ethical use of the data.\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "The original dataset contains **12 columns**:\n",
    "- `index`\n",
    "- `advertiserurl`\n",
    "- `company`\n",
    "- `employmenttype_jobstatus`\n",
    "- `jobdescription` ‚≠ê **(Only this column is used for our analysis)**\n",
    "- `jobid`\n",
    "- `joblocation_address`\n",
    "- `jobtitle`\n",
    "- `postdate`\n",
    "- `shift`\n",
    "- `site_name`\n",
    "- `skills`\n",
    "- `uniq_id`\n",
    "\n",
    "**Note:** For this gender bias detection project, we only utilize the `jobdescription` column. All other columns are ignored during processing to focus exclusively on the language content of job descriptions.\n",
    "\n",
    "### Dataset Download\n",
    "\n",
    "‚ö†Ô∏è **Important:** The dataset is downloaded **manually** from Kaggle, not programmatically. \n",
    "\n",
    "To replicate this project:\n",
    "1. Visit the [Kaggle dataset page](https://www.kaggle.com/datasets/PromptCloudHQ/us-technology-jobs-on-dicecom/data)\n",
    "2. Download the dataset file (`dice_com-job_us_sample.csv`)\n",
    "3. Place it in the `data/raw/` directory\n",
    "4. Ensure the file is named `dice_com-job_us_sample.csv`\n",
    "\n",
    "### Gender Bias Labeling Method\n",
    "\n",
    "We label job descriptions as **biased (1)** or **non-biased (0)** based on the methodology from **Gaucher et al. (2011)**, a peer-reviewed study published in the *Journal of Personality and Social Psychology*:\n",
    "\n",
    "**Reference:** Gaucher, D., Friesen, J., & Kay, A. C. (2011). Evidence that gendered wording in job advertisements exists and sustains gender inequality. *Journal of Personality and Social Psychology*, 101(1), 109‚Äì128. https://doi.org/10.1037/a0022530\n",
    "\n",
    "**Scientific Foundation:**\n",
    "The word lists used in this methodology were **empirically validated** through controlled experiments, demonstrating that gendered wording in job advertisements affects applicant pool composition and perpetuates gender inequality. This provides a **scientifically grounded** approach to identifying potentially biased language.\n",
    "\n",
    "**SDG Contribution:**\n",
    "By systematically identifying gender-biased language in job descriptions, this methodology supports:\n",
    "- **SDG 5 (Gender Equality)**: Enabling organizations to recognize and eliminate gender-biased language that may discourage qualified candidates from applying, thereby promoting equal access to employment opportunities.\n",
    "- **SDG 8 (Decent Work and Economic Growth)**: Facilitating fair hiring practices that ensure all qualified individuals have equal opportunities to access decent work, contributing to inclusive economic growth.\n",
    "\n",
    "**Labeling Rules:**\n",
    "1. **Word Lists:** We use predefined lists of **25 masculine-coded** and **25 feminine-coded word stems** derived from established psychological research on gender stereotypes and language (e.g., \"aggress\", \"competit\", \"decis\" for masculine; \"collabor\", \"support\", \"empath\" for feminine).\n",
    "\n",
    "2. **Counting:** For each job description, we count occurrences of masculine and feminine word stems using stem matching, which captures word variations (e.g., \"aggressive\", \"aggressively\", \"aggression\" all match the stem \"aggress\").\n",
    "\n",
    "3. **Labeling Criteria:**\n",
    "   - **BIASED (1):** The job description contains:\n",
    "     - At least 2 gendered words (masculine + feminine combined), AND\n",
    "     - A gender imbalance ratio of ‚â•2:1 (e.g., 4 masculine words vs. 1 feminine word, or vice versa)\n",
    "   - **NON-BIASED (0):** Otherwise (fewer than 2 gendered words, or balanced gender representation)\n",
    "\n",
    "4. **Text Preprocessing:** Before labeling, we clean the text by:\n",
    "   - Removing HTML tags\n",
    "   - Removing URLs\n",
    "   - Normalizing whitespace\n",
    "   - Converting to lowercase for word matching\n",
    "\n",
    "**Validation:**\n",
    "The reliability of this automated labeling methodology has been validated through manual review of 200 samples, achieving an **85.5% agreement rate** between automated and manual labels, demonstrating the methodology's reliability and scientific validity.\n",
    "\n",
    "This automated labeling approach allows us to systematically identify potentially gender-biased language in job descriptions at scale, providing a **reproducible, objective, and scientifically grounded** method for large-scale gender bias analysis that directly supports progress toward **SDG 5 (Gender Equality)** and **SDG 8 (Decent Work and Economic Growth)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\n",
      "Raw data directory: D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\data\\raw\n",
      "Processed data directory: D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set up paths\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'notebooks':\n",
    "    project_root = current_dir.parent\n",
    "else:\n",
    "    project_root = current_dir\n",
    "\n",
    "data_dir = project_root / 'data'\n",
    "raw_data_dir = data_dir / 'raw'\n",
    "processed_data_dir = data_dir / 'processed'\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(raw_data_dir, exist_ok=True)\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Raw data directory: {raw_data_dir}\")\n",
    "print(f\"Processed data directory: {processed_data_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the word list according to Gaucher et al. (2011)\n",
    "Gaucher, D., Friesen, J., & Kay, A. C. (2011). Evidence that gendered wording in job advertisements exists and sustains gender inequality. Journal of Personality and Social Psychology, 101(1), 109‚Äì128. https://doi.org/10.1037/a0022530"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masculine-coded words: 48\n",
      "Feminine-coded words: 49\n",
      "\n",
      "Sample masculine words: ['active', 'adventurous', 'aggress', 'ambitio', 'analy']\n",
      "Sample feminine words: ['agree', 'affectionate', 'child', 'cheer', 'collab']\n"
     ]
    }
   ],
   "source": [
    "# Using stems for matching (e.g., \"compet\" matches \"competitive\", \"competition\")\n",
    "MASCULINE_WORDS = [\n",
    "    'active', 'adventurous', 'aggress', 'ambitio', 'analy', 'assert', 'athlet', 'autonom',\n",
    "    'battle', 'boast', 'challeng', 'champion', 'compet', 'confident', 'courag', 'decid',\n",
    "    'decision', 'decisive', 'defend', 'determin', 'domina', 'dominant', 'driven', 'fearless',\n",
    "    'fight', 'force', 'greedy', 'headstrong', 'hierarch', 'hostil', 'impulsive', 'independen',\n",
    "    'individual', 'intellect', 'lead', 'logic', 'objective', 'opinion', 'outspoken', 'persist',\n",
    "    'principle', 'reckless', 'self-confiden', 'self-relian', 'self-sufficien', 'stubborn',\n",
    "    'superior', 'unreasonab'\n",
    "]\n",
    "\n",
    "# Gaucher et al. (2011) Feminine-Coded Words (50 words)\n",
    "FEMININE_WORDS = [\n",
    "    'agree', 'affectionate', 'child', 'cheer', 'collab', 'commit', 'communal', 'compassion',\n",
    "    'connect', 'considerate', 'cooperat', 'co-operat', 'depend', 'emotiona', 'empath', 'feel',\n",
    "    'flatterable', 'gentle', 'honest', 'interpersonal', 'interdependen', 'interpersona',\n",
    "    'inter-personal', 'inter-dependen', 'kind', 'kinship', 'loyal', 'modesty', 'nag', 'nurtur',\n",
    "    'pleasant', 'polite', 'quiet', 'respon', 'sensitiv', 'submissive', 'support', 'sympath',\n",
    "    'tender', 'together', 'trust', 'understand', 'warm', 'whin', 'enthusias', 'inclusive',\n",
    "    'yield', 'share', 'sharin'\n",
    "]\n",
    "\n",
    "print(f\"Masculine-coded words: {len(MASCULINE_WORDS)}\")\n",
    "print(f\"Feminine-coded words: {len(FEMININE_WORDS)}\")\n",
    "print(f\"\\nSample masculine words: {MASCULINE_WORDS[:5]}\")\n",
    "print(f\"Sample feminine words: {FEMININE_WORDS[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning Function\n",
    "\n",
    "Clean job description text by removing HTML tags, URLs, and extra whitespace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean job description text\n",
    "    - Remove HTML tags\n",
    "    - Remove URLs\n",
    "    - Remove extra whitespace\n",
    "    - Convert to lowercase for word matching\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender Bias Labeling Function\n",
    "\n",
    "**Labeling Rules (Gaucher et al. 2011 methodology):**\n",
    "- Count masculine and feminine word stems in text\n",
    "- **BIASED (1)**: Contains ‚â•2 gendered words AND ‚â•2:1 imbalance ratio\n",
    "- **NON-BIASED (0)**: Otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing labeling function:\n",
      "======================================================================\n",
      "\n",
      "Text: We are seeking an aggressive, competitive leader who is deci...\n",
      "  Masculine words: 5, Feminine words: 0\n",
      "  Label: 1 (BIASED)\n",
      "\n",
      "Text: Looking for a collaborative team player who is supportive an...\n",
      "  Masculine words: 0, Feminine words: 3\n",
      "  Label: 1 (BIASED)\n",
      "\n",
      "Text: Need someone who is analytical and strategic, but also colla...\n",
      "  Masculine words: 1, Feminine words: 1\n",
      "  Label: 0 (NON-BIASED)\n",
      "\n",
      "Text: Seeking a confident, assertive individual with strong leader...\n",
      "  Masculine words: 4, Feminine words: 0\n",
      "  Label: 1 (BIASED)\n"
     ]
    }
   ],
   "source": [
    "def count_gendered_words(text, masculine_words, feminine_words):\n",
    "    \"\"\"\n",
    "    Count masculine and feminine word stems in text using stem matching\n",
    "    \n",
    "    Returns:\n",
    "        masc_count: Number of masculine word matches\n",
    "        fem_count: Number of feminine word matches\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    masc_count = 0\n",
    "    fem_count = 0\n",
    "    \n",
    "    # Count masculine words (stem matching)\n",
    "    for word_stem in masculine_words:\n",
    "        # Match word stem as whole word or part of word\n",
    "        pattern = r'\\b' + re.escape(word_stem) + r'\\w*'\n",
    "        matches = len(re.findall(pattern, text_lower))\n",
    "        masc_count += matches\n",
    "    \n",
    "    # Count feminine words (stem matching)\n",
    "    for word_stem in feminine_words:\n",
    "        pattern = r'\\b' + re.escape(word_stem) + r'\\w*'\n",
    "        matches = len(re.findall(pattern, text_lower))\n",
    "        fem_count += matches\n",
    "    \n",
    "    return masc_count, fem_count\n",
    "\n",
    "\n",
    "def label_gender_bias(text, masculine_words, feminine_words, threshold=2, ratio_threshold=2.0):\n",
    "    \"\"\"\n",
    "    Label job description as biased (1) or non-biased (0)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Job description text\n",
    "    masculine_words : list\n",
    "        List of masculine word stems\n",
    "    feminine_words : list\n",
    "        List of feminine word stems\n",
    "    threshold : int\n",
    "        Minimum number of gendered words required (default: 2)\n",
    "    ratio_threshold : float\n",
    "        Minimum imbalance ratio required (default: 2.0, meaning 2:1)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    label : int\n",
    "        1 if biased, 0 if non-biased\n",
    "    masc_count : int\n",
    "        Number of masculine words found\n",
    "    fem_count : int\n",
    "        Number of feminine words found\n",
    "    \"\"\"\n",
    "    # Clean text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    if len(cleaned_text) < 50:  # Filter very short texts\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    # Count gendered words\n",
    "    masc_count, fem_count = count_gendered_words(cleaned_text, masculine_words, feminine_words)\n",
    "    total_gendered = masc_count + fem_count\n",
    "    \n",
    "    # Check threshold: need at least 2 gendered words\n",
    "    if total_gendered < threshold:\n",
    "        return 0, masc_count, fem_count\n",
    "    \n",
    "    # Check imbalance ratio: need ‚â•2:1 ratio\n",
    "    if masc_count > 0 and fem_count > 0:\n",
    "        ratio = max(masc_count / fem_count, fem_count / masc_count)\n",
    "        if ratio >= ratio_threshold:\n",
    "            return 1, masc_count, fem_count\n",
    "        else:\n",
    "            return 0, masc_count, fem_count\n",
    "    elif masc_count >= threshold:\n",
    "        # Only masculine words, and meets threshold\n",
    "        return 1, masc_count, fem_count\n",
    "    elif fem_count >= threshold:\n",
    "        # Only feminine words, and meets threshold\n",
    "        return 1, masc_count, fem_count\n",
    "    else:\n",
    "        return 0, masc_count, fem_count\n",
    "\n",
    "\n",
    "# Test the labeling function\n",
    "test_texts = [\n",
    "    \"We are seeking an aggressive, competitive leader who is decisive and driven.\",\n",
    "    \"Looking for a collaborative team player who is supportive and empathetic.\",\n",
    "    \"Need someone who is analytical and strategic, but also collaborative.\",\n",
    "    \"Seeking a confident, assertive individual with strong leadership skills.\"\n",
    "]\n",
    "\n",
    "print(\"Testing labeling function:\")\n",
    "print(\"=\" * 70)\n",
    "for text in test_texts:\n",
    "    label, masc, fem = label_gender_bias(text, MASCULINE_WORDS, FEMININE_WORDS)\n",
    "    print(f\"\\nText: {text[:60]}...\")\n",
    "    print(f\"  Masculine words: {masc}, Feminine words: {fem}\")\n",
    "    print(f\"  Label: {label} ({'BIASED' if label == 1 else 'NON-BIASED'})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "\n",
    "**Instructions:**\n",
    "1. Download dataset from Kaggle (search \"dice.com job descriptions\" or \"tech job postings\")\n",
    "2. Place CSV file in `data/raw/` directory\n",
    "3. Update the filename below to match your downloaded file\n",
    "\n",
    "**Dataset Structure:**\n",
    "- The dataset has **multiple columns** (e.g., job_title, company, location, etc.)\n",
    "- The code will automatically detect and use the **jobdescription** column\n",
    "- Only the job description text will be used for gender bias labeling\n",
    "- Other columns will be ignored during processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSV file: dice_com-job_us_sample.csv\n",
      "\n",
      "‚úÖ Loading dataset from: D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\data\\raw\\dice_com-job_us_sample.csv\n",
      "‚úÖ Dataset loaded successfully!\n",
      "   Shape: (22000, 13)\n",
      "   Columns: ['index', 'advertiserurl', 'company', 'employmenttype_jobstatus', 'jobdescription', 'jobid', 'joblocation_address', 'jobtitle', 'postdate', 'shift', 'site_name', 'skills', 'uniq_id']\n",
      "\n",
      "   First few rows:\n",
      "   index                                      advertiserurl  \\\n",
      "0      0  https://www.dice.com/jobs/detail/AUTOMATION-TE...   \n",
      "1      1  https://www.dice.com/jobs/detail/Information-S...   \n",
      "2      2  https://www.dice.com/jobs/detail/Business-Solu...   \n",
      "\n",
      "                             company  \\\n",
      "0  Digital Intelligence Systems, LLC   \n",
      "1  University of Chicago/IT Services   \n",
      "2               Galaxy Systems, Inc.   \n",
      "\n",
      "                            employmenttype_jobstatus  \\\n",
      "0  C2H Corp-To-Corp, C2H Independent, C2H W2, 3 M...   \n",
      "1                                          Full Time   \n",
      "2                                          Full Time   \n",
      "\n",
      "                                      jobdescription               jobid  \\\n",
      "0  Looking for Selenium engineers...must have sol...  Dice Id : 10110693   \n",
      "1  The University of Chicago has a rapidly growin...  Dice Id : 10114469   \n",
      "2  GalaxE.SolutionsEvery day, our solutions affec...  Dice Id : CXGALXYS   \n",
      "\n",
      "  joblocation_address                       jobtitle     postdate  \\\n",
      "0         Atlanta, GA       AUTOMATION TEST ENGINEER   1 hour ago   \n",
      "1         Chicago, IL  Information Security Engineer   1 week ago   \n",
      "2      Schaumburg, IL   Business Solutions Architect  2 weeks ago   \n",
      "\n",
      "                                             shift site_name  \\\n",
      "0  Telecommuting not available|Travel not required       NaN   \n",
      "1  Telecommuting not available|Travel not required       NaN   \n",
      "2  Telecommuting not available|Travel not required       NaN   \n",
      "\n",
      "                                              skills  \\\n",
      "0                                          SEE BELOW   \n",
      "1  linux/unix, network monitoring, incident respo...   \n",
      "2  Enterprise Solutions Architecture, business in...   \n",
      "\n",
      "                            uniq_id  \n",
      "0  418ff92580b270ef4e7c14f0ddfc36b4  \n",
      "1  8aec88cba08d53da65ab99cf20f6f9d9  \n",
      "2  46baa1f69ac07779274bcd90b85d9a72  \n",
      "\n",
      "‚úÖ Using text column: 'jobdescription'\n",
      "   Column will be used for gender bias labeling\n"
     ]
    }
   ],
   "source": [
    "# Dataset filename (update this to match your downloaded file)\n",
    "dataset_filename = 'job_descriptions.csv'  # Change this to your actual filename\n",
    "\n",
    "# Alternative: Try to find any CSV file in raw_data_dir\n",
    "csv_files = list(raw_data_dir.glob('*.csv'))\n",
    "if csv_files:\n",
    "    dataset_filename = csv_files[0].name\n",
    "    print(f\"Found CSV file: {dataset_filename}\")\n",
    "else:\n",
    "    print(f\"    No CSV files found in {raw_data_dir}\")\n",
    "    print(f\"   Please download a dataset and place it in: {raw_data_dir}\")\n",
    "    print(f\"   Recommended: Dice.com tech jobs dataset from Kaggle\")\n",
    "\n",
    "dataset_path = raw_data_dir / dataset_filename\n",
    "\n",
    "if dataset_path.exists():\n",
    "    print(f\"\\n   Loading dataset from: {dataset_path}\")\n",
    "    \n",
    "    # Try to load the dataset\n",
    "    try:\n",
    "        df_raw = pd.read_csv(dataset_path)\n",
    "        print(f\"   Dataset loaded successfully!\")\n",
    "        print(f\"   Shape: {df_raw.shape}\")\n",
    "        print(f\"   Columns: {df_raw.columns.tolist()}\")\n",
    "        print(f\"\\n   First few rows:\")\n",
    "        print(df_raw.head(3))\n",
    "        \n",
    "        # Identify text column - prioritize \"jobdescription\" and variations\n",
    "        # Check for exact matches first (case-insensitive)\n",
    "        text_col = None\n",
    "        possible_text_cols = [\n",
    "            'jobdescription',  # Most common format\n",
    "            'job_description', \n",
    "            'jobDescription',\n",
    "            'JobDescription',\n",
    "            'description', \n",
    "            'job_desc',\n",
    "            'jobdesc',\n",
    "            'text', \n",
    "            'content', \n",
    "            'summary'\n",
    "        ]\n",
    "        \n",
    "        # First, try exact matches (case-insensitive)\n",
    "        df_columns_lower = [col.lower() for col in df_raw.columns]\n",
    "        for preferred_col in possible_text_cols:\n",
    "            if preferred_col.lower() in df_columns_lower:\n",
    "                # Find the actual column name (preserving case)\n",
    "                text_col = df_raw.columns[df_columns_lower.index(preferred_col.lower())]\n",
    "                break\n",
    "        \n",
    "        # If not found, try partial matches\n",
    "        if text_col is None:\n",
    "            for col in df_raw.columns:\n",
    "                col_lower = col.lower()\n",
    "                if 'job' in col_lower and ('desc' in col_lower or 'description' in col_lower):\n",
    "                    text_col = col\n",
    "                    break\n",
    "        \n",
    "        # If still not found, use first text-like column\n",
    "        if text_col is None:\n",
    "            for col in df_raw.columns:\n",
    "                if df_raw[col].dtype == 'object':\n",
    "                    text_col = col\n",
    "                    break\n",
    "        \n",
    "        if text_col:\n",
    "            print(f\"\\n   Using text column: '{text_col}'\")\n",
    "            print(f\"   Column will be used for gender bias labeling\")\n",
    "        else:\n",
    "            print(f\"\\n    Could not identify text column. Please specify manually.\")\n",
    "            print(f\"   Available columns: {df_raw.columns.tolist()}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading dataset: {e}\")\n",
    "        df_raw = None\n",
    "        text_col = None\n",
    "else:\n",
    "    print(f\"‚ùå Dataset not found at: {dataset_path}\")\n",
    "    print(f\"\\n   To download a dataset:\")\n",
    "    print(f\"   1. Go to kaggle.com/datasets\")\n",
    "    print(f\"   2. Search for 'dice.com job descriptions' or 'tech job postings'\")\n",
    "    print(f\"   3. Download and extract to: {raw_data_dir}\")\n",
    "    df_raw = None\n",
    "    text_col = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and Label Dataset\n",
    "\n",
    "Apply gender bias labeling to all job descriptions and save the labeled dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROCESSING AND LABELING DATASET\n",
      "======================================================================\n",
      "\n",
      "üìä Dataset Statistics:\n",
      "   Initial samples: 22,000\n",
      "   After filtering: 21,978\n",
      "   Removed: 22 samples\n",
      "\n",
      "üè∑Ô∏è  Applying gender bias labels...\n",
      "   Processing 21,978 samples...\n",
      "   Processed 1,000 / 21,978 samples...\n",
      "   Processed 2,000 / 21,978 samples...\n",
      "   Processed 3,000 / 21,978 samples...\n",
      "   Processed 4,000 / 21,978 samples...\n",
      "   Processed 5,000 / 21,978 samples...\n",
      "   Processed 6,000 / 21,978 samples...\n",
      "   Processed 7,000 / 21,978 samples...\n",
      "   Processed 8,000 / 21,978 samples...\n",
      "   Processed 9,000 / 21,978 samples...\n",
      "   Processed 10,000 / 21,978 samples...\n",
      "   Processed 11,000 / 21,978 samples...\n",
      "   Processed 12,000 / 21,978 samples...\n",
      "   Processed 13,000 / 21,978 samples...\n",
      "   Processed 14,000 / 21,978 samples...\n",
      "   Processed 15,000 / 21,978 samples...\n",
      "   Processed 16,000 / 21,978 samples...\n",
      "   Processed 17,000 / 21,978 samples...\n",
      "   Processed 18,000 / 21,978 samples...\n",
      "   Processed 19,000 / 21,978 samples...\n",
      "   Processed 20,000 / 21,978 samples...\n",
      "   Processed 21,000 / 21,978 samples...\n",
      "   Processed 22,000 / 21,978 samples...\n",
      "\n",
      "‚úÖ Labeling complete!\n",
      "\n",
      "üìä Label Distribution:\n",
      "   Non-Biased (0): 11,281 samples (51.3%)\n",
      "   Biased (1): 10,697 samples (48.7%)\n",
      "\n",
      "üìä Word Count Statistics:\n",
      "   Mean masculine words: 3.97\n",
      "   Mean feminine words: 4.55\n",
      "   Max masculine words: 65\n",
      "   Max feminine words: 80\n",
      "\n",
      "üíæ Saved labeled dataset to: D:\\Coursework\\Project Replication\\HEARTS-Gender-Bias-Job-Descriptions\\data\\processed\\job_descriptions_labeled.csv\n",
      "   Total samples: 21,978\n",
      "   Format: text, label (0=Non-Biased, 1=Biased)\n",
      "\n",
      "üìù Example Biased Samples (label=1):\n",
      "\n",
      "   Sample 6:\n",
      "   Text: Network Engineer Job Description A Network Engineer is responsible for analyzing, designing, install...\n",
      "   Masc: 2, Fem: 8\n",
      "\n",
      "   Sample 7:\n",
      "   Text: Bluebeam is looking for talented sr. web developers¬†with a passion for building robust, performant, ...\n",
      "   Masc: 0, Fem: 6\n",
      "\n",
      "   Sample 8:\n",
      "   Text: This is a fulltime position for a Javascript developer for a financial software, data, and media com...\n",
      "   Masc: 0, Fem: 4\n",
      "\n",
      "üìù Example Non-Biased Samples (label=0):\n",
      "\n",
      "   Sample 0:\n",
      "   Text: Looking for Selenium engineers...must have solid java coding skills¬†I have several openings some 3 m...\n",
      "   Masc: 6, Fem: 8\n",
      "\n",
      "   Sample 1:\n",
      "   Text: The University of Chicago has a rapidly growing security program with many opportunities for a motiv...\n",
      "   Masc: 3, Fem: 3\n",
      "\n",
      "   Sample 2:\n",
      "   Text: GalaxE.SolutionsEvery day, our solutions affect people throughout the world. From Fortune 100 compan...\n",
      "   Masc: 5, Fem: 4\n"
     ]
    }
   ],
   "source": [
    "if 'df_raw' in locals() and 'text_col' in locals() and df_raw is not None and text_col is not None:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PROCESSING AND LABELING DATASET\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Extract text column\n",
    "    df_processed = pd.DataFrame()\n",
    "    df_processed['text'] = df_raw[text_col].copy()\n",
    "    \n",
    "    # Remove rows with missing or very short text\n",
    "    initial_count = len(df_processed)\n",
    "    df_processed = df_processed[df_processed['text'].notna()].copy()\n",
    "    df_processed = df_processed[df_processed['text'].astype(str).str.len() >= 50].copy()\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Statistics:\")\n",
    "    print(f\"   Initial samples: {initial_count:,}\")\n",
    "    print(f\"   After filtering: {len(df_processed):,}\")\n",
    "    print(f\"   Removed: {initial_count - len(df_processed):,} samples\")\n",
    "    \n",
    "    # Apply labeling\n",
    "    print(f\"\\nüè∑Ô∏è  Applying gender bias labels...\")\n",
    "    print(f\"   Processing {len(df_processed):,} samples...\")\n",
    "    \n",
    "    results = []\n",
    "    for idx, row in df_processed.iterrows():\n",
    "        text = row['text']\n",
    "        label, masc_count, fem_count = label_gender_bias(\n",
    "            text, \n",
    "            MASCULINE_WORDS, \n",
    "            FEMININE_WORDS,\n",
    "            threshold=2,\n",
    "            ratio_threshold=2.0\n",
    "        )\n",
    "        results.append({\n",
    "            'text': text,\n",
    "            'label': label,\n",
    "            'masc_count': masc_count,\n",
    "            'fem_count': fem_count\n",
    "        })\n",
    "        \n",
    "        if (idx + 1) % 1000 == 0:\n",
    "            print(f\"   Processed {idx + 1:,} / {len(df_processed):,} samples...\")\n",
    "    \n",
    "    df_labeled = pd.DataFrame(results)\n",
    "    \n",
    "    # Label distribution\n",
    "    label_dist = df_labeled['label'].value_counts()\n",
    "    print(f\"\\n‚úÖ Labeling complete!\")\n",
    "    print(f\"\\nüìä Label Distribution:\")\n",
    "    print(f\"   Non-Biased (0): {label_dist.get(0, 0):,} samples ({label_dist.get(0, 0)/len(df_labeled)*100:.1f}%)\")\n",
    "    print(f\"   Biased (1): {label_dist.get(1, 0):,} samples ({label_dist.get(1, 0)/len(df_labeled)*100:.1f}%)\")\n",
    "    \n",
    "    # Word count statistics\n",
    "    print(f\"\\nüìä Word Count Statistics:\")\n",
    "    print(f\"   Mean masculine words: {df_labeled['masc_count'].mean():.2f}\")\n",
    "    print(f\"   Mean feminine words: {df_labeled['fem_count'].mean():.2f}\")\n",
    "    print(f\"   Max masculine words: {df_labeled['masc_count'].max()}\")\n",
    "    print(f\"   Max feminine words: {df_labeled['fem_count'].max()}\")\n",
    "    \n",
    "    # Save labeled dataset\n",
    "    output_path = processed_data_dir / 'job_descriptions_labeled.csv'\n",
    "    df_labeled[['text', 'label']].to_csv(output_path, index=False, encoding='utf-8')\n",
    "    print(f\"\\nüíæ Saved labeled dataset to: {output_path}\")\n",
    "    print(f\"   Total samples: {len(df_labeled):,}\")\n",
    "    print(f\"   Format: text, label (0=Non-Biased, 1=Biased)\")\n",
    "    \n",
    "    # Show examples\n",
    "    print(f\"\\nüìù Example Biased Samples (label=1):\")\n",
    "    biased_samples = df_labeled[df_labeled['label'] == 1].head(3)\n",
    "    for idx, row in biased_samples.iterrows():\n",
    "        print(f\"\\n   Sample {idx}:\")\n",
    "        print(f\"   Text: {row['text'][:100]}...\")\n",
    "        print(f\"   Masc: {row['masc_count']}, Fem: {row['fem_count']}\")\n",
    "    \n",
    "    print(f\"\\nüìù Example Non-Biased Samples (label=0):\")\n",
    "    nonbiased_samples = df_labeled[df_labeled['label'] == 0].head(3)\n",
    "    for idx, row in nonbiased_samples.iterrows():\n",
    "        print(f\"\\n   Sample {idx}:\")\n",
    "        print(f\"   Text: {row['text'][:100]}...\")\n",
    "        print(f\"   Masc: {row['masc_count']}, Fem: {row['fem_count']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Cannot process dataset. Please ensure dataset is loaded first (run Cell 9).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Next Steps:**\n",
    "1. Run `01_Data_Loading_Preprocessing.ipynb` to load and validate the labeled dataset\n",
    "2. Create manual validation sample (200 samples)\n",
    "3. Complete train/val/test splits\n",
    "\n",
    "**Files Created:**\n",
    "- `data/processed/job_descriptions_labeled.csv` - Labeled dataset ready for preprocessing\n",
    "\n",
    "**Note:** The processed dataset contains:\n",
    "- `text`: Job description text (from jobdescription column)\n",
    "- `label`: Gender bias label (0=Non-Biased, 1=Biased)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (HEARTS)",
   "language": "python",
   "name": "hearts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
